GeneratedusingtheofficialAMSLATEXtemplatev6.1two-columnlayout. Thisworkhasbeensubmittedfor
publication. Copyrightinthisworkmaybetransferredwithoutfurthernotice,andthisversionmaynolongerbe
accessible.
AMachineLearningTutorialforOperationalMeteorology,PartI:TraditionalMachine
Learning
RandyJ.Chasea,b,c,DavidR.Harrisonb,d,e,AmandaBurkeb,c,GaryM.Lackmannf andAmyMcGoverna,b,c
aSchoolofComputerScience,UniversityofOklahoma,NormanOKUSA
bSchoolofMeteorology,UniversityofOklahoma,NormanOKUSA
cNSFAIInstituteforResearchonTrustworthyAIinWeather,Climate,andCoastalOceanography,UniversityofOklahoma,NormanOK
USA
dCooperativeInstituteforSevereandHigh-ImpactWeatherResearchandOperations,UniversityofOklahoma,NormanOKUSA
eNOAA/NWS/StormPredictionCenter,Norman,Oklahoma
fDepartmentofMarine,Earth,andAtmosphericSciences,NorthCarolinaStateUniversity,Raleigh,NorthCarolina
ABSTRACT: Recently,theuseofmachinelearninginmeteorologyhasincreasedgreatly. Whilemanymachinelearningmethodsarenot
new,universityclassesonmachinelearningarelargelyunavailabletometeorologystudentsandarenotrequiredtobecomeameteorologist.
Thelackofformalinstructionhascontributedtoperceptionthatmachinelearningmethodsareâ€™blackboxesâ€™andthusend-usersarehesitant
toapplythemachinelearningmethodsintheireverydayworkflow. Toreducetheopaquenessofmachinelearningmethodsandlower
hesitancytowardsmachinelearninginmeteorology,thispaperprovidesasurveyofsomeofthemostcommonmachinelearningmethods.
Afamiliarmeteorologicalexampleisusedtocontextualizethemachinelearningmethodswhilealsodiscussingmachinelearningtopics
usingplainlanguage. Thefollowingmachinelearningmethodsaredemonstrated: linearregression;logisticregression;decisiontrees;
randomforest;gradientboosteddecisiontrees;naÃ¯veBayes;andsupportvectormachines. Beyonddiscussingthedifferentmethods,the
paperalsocontainsdiscussionsonthegeneralmachinelearningprocessaswellasbestpracticestoenablereaderstoapplymachinelearning
totheirowndatasets. Furthermore,allcode(intheformofJupyternotebooksandGoogleColaboratorynotebooks)usedtomakethe
examplesinthepaperisprovidedinanefforttocatalysetheuseofmachinelearninginmeteorology.
1. Introduction Inpractice,MLmodelsareoftenviewedasablackbox
whichcouldalsobecontributingtouserhesitancy. These
Thementionanduseofmachinelearning(ML)within
mystifiedfeelingstowardsMLmethodscanleadtoanin-
meteorologicaljournalarticlesisaccelerating(Fig. 1;e.g.,
Burkeetal.2020;Hilletal.2020;Lagerquistetal.2020; herent distrust with ML methods, despite their potential.
Lietal.2020;Lokenetal.2020;MaoandSorteberg2020; Furthermore,theseeminglyopaquenatureofMLmethods
MuÃ±oz-Esparza et al. 2020; Wang et al. 2020; Bonavita prevents ML forecasts from meeting one of the three re-
et al. 2021; Cui et al. 2021; Flora et al. 2021; Hill and quirementsofagoodforecastoutlinedbyMurphy(1993):
Schumacher 2021; Schumacher et al. 2021; Yang et al.
consistency. Inshort,Murphy(1993)explainsthatinorder
2021; Zhang et al. 2021). With a growing number of
foraforecasttobegood, theforecastmust(1)beconsis-
publishedmeteorologicalstudiesusingMLmethods,itis
tentwiththeuserâ€™spriorknowledge,(2)havegoodquality
increasinglyimportantformeteorologiststobewell-versed
(i.e., accuracy)and(3)bevaluable(i.e., providebenefit).
in ML. However, the availability of meteorology specific
resources about ML terms and methods is scarce. Thus, PlentyoftechnicalpapersdemonstratehowMLforecasts
thisseriesofpapers(totalof2)aimtoreducethescarcity can meet requirements 2 and 3, but as noted above if the
ofmeteorologyspecificMLresources. MLmethodsareconfusingandenigmatic,thenitisdiffi-
While many ML methods are generally not new (i.e., cultforMLforecaststobeconsistentwithameteorologists
published before 2002), there is a concern from ML de-
priorknowledge. Thisseriesofpaperswillserveasaref-
velopers that end users (i.e., non-ML specialists) may be
erenceformeteorologistsinordertomaketheblackboxof
hesitant or are concerned about trusting ML. However,
MLmoretransparentandenhanceusertrustinML.
early work in this space suggests that non-technical ex-
Thispaper isorganized asfollows. Section2 provides
planations may be an important part of how end users
perceive the trustworthiness of ML guidance (e.g., Cains anintroductiontoallMLmethodsdiscussedinthispaper
et al. 2022). Thus, an additional goal of these papers is and will define common ML terms. Section 3 discusses
to enhance trustworthiness of ML methods through plain the general ML methods in context of a simple meteoro-
languagediscussionsandmeteorologicalexamples. logicalexample,whilealsodescribingtheend-to-endML
pipeline. Then,Section4summarizesthispaperandalso
discussesthetopicsofthenextpaperintheseries.
Correspondingauthor:RandyJ.Chase,randychase@ou.edu
1
2202
nuJ
7
]hp-oa.scisyhp[
2v29470.4022:viXra
2
Fig.1.SearchresultsfortheMeteorologyandAtmosphericSciencecategorywhensearchingabstractsformachinelearningmethodsandsevere
weather. Machinelearningkeywordssearchedwere: linearregression,logisticregression,decisiontrees,randomforest,gradientboostedtrees,
supportvectormachines,k-means,k-nearest,empiricalorthogonalfunctions,principalcomponentanalysis,selforganizingmaps,neuralnetworks,
convolutionalneuralnetworksandunets.Severeweatherkeywordssearchedwere:tornadoes,hail,hurricanesandtropicalcyclones.(a)Countsof
publicationsperyearforallpapersintheMeteorologyandAtmosphericSciencecategory(blackline;reducedbyoneorderofmagnitude),machine
learningtopics(blueline)andsevereweathertopics(redline). (b)Sameas(a),butwiththetwosubtopicsnormalizedbythetotalnumberof
MeteorologyandAtmosphericSciencepapers.(c)Numberofneuralnetworkpapers(includingconvolutionalandunets)publishedinMeteorology
andAtmosphericsciences.AlldataarederivedfromClarivateWebofScience.
2. MachineLearningMethodsandCommonTerms SupervisedMLmethodscanbefurtherbrokenintotwo
sub-categories: regressionandclassification. Regression
This section will describe a handful of the most com-
mon ML methods. Before that, it is helpful to define tasks are ML methods that output a continuous range of
some terminology used within ML. First, we define ML values, like the forecast of tomorrowâ€™s high temperature
as any empirical1 method where parameters are fit (i.e., (e.g., 75.0â—¦F). Meanwhile classification tasks are charac-
learned) on a training dataset in order to optimize (e.g., teristicofMLmethodsthatclassifydata(e.g.,willitrainor
minimizeormaximize)apredefinedloss(i.e.,cost)func-
snowtomorrow). Reposingtomorrowâ€™shightemperature
tion. Within this general framework, ML has two cate-
forecast as a classification task would be: "Will tomor-
gories: supervisedandunsupervisedlearning. Supervised
row be warmer than today?". This paper will cover both
learningareMLmethodsthataretrainedwithprescribed
inputfeaturesandoutputlabels. Forexample, predicting regression and classification methods. In fact, many ML
tomorrowâ€™shightemperatureataspecificlocationwhere methodscanbeusedforbothtasks.
wehavemeasurements(i.e.,labels). Meanwhile,unsuper- AllMLmethodsdescribedherewillhaveonethingin
visedmethodsdonothaveapredefinedoutputlabel(e.g.,
common: theMLmethodquantitativelyusesthetraining
self-organizing maps; Nowotarski and Jensen 2013). An
datatooptimizeasetofweights(i.e.,thresholds)thaten-
exampleofanunsupervisedMLtaskwouldbeclustering
abletheprediction. Theseweightsaredeterminedeitherby
all500mbgeopotentialheightmapstolookforunspecified
patternsintheweather. Thispaperfocusesonsupervised minimizingtheerroroftheMLpredictionormaximizing
learning. a probability of a class label. The two different methods
Theinputfeaturesforsupervisedlearning,alsoreferred coincidewiththeregressionandclassificationrespectively.
to as input data, predictors or variables, can be written
Alternative names for error that readers might encounter
mathematicallyasthevector(matrix) ð‘‹. Thedesiredout-
intheliteraturearelossorcost.
put of the ML model is usually called the target, predic-
NowthatsomeofthecommonMLtermshasbeendis-
tand or label, and is mathematically written as the scalar
(vector) ð‘¦. Drawing on the meteorological example of cussed, the following subsections will describe the ML
predictingtomorrowâ€™shightemperature,theinputfeature methods. Itwillstartwiththesimplestmethods(e.g.,lin-
would be tomorrowâ€™s forecasted temperature from a nu- earregression)andmovetomorecomplexmethods(e.g.,
mericalweathermodel(e.g.,GFS)andthelabelwouldbe supportvectormachines)asthesectionsproceed. Please
tomorrowâ€™sobservedtemperature.
notethatthefollowingsubsectionsaimtoprovideanintro-
ductionandtheintuitionbehindeachmethod. Anexample
ofthemethodsbeingappliedandhelpfulapplicationdis-
1By empirical we mean any method that uses data as opposed to
physics cussioncanbefoundinSection3.
3
whereð‘¦ ð‘— isatruedatapoint,ð‘¦Ë†ð‘— isthepredicteddatapoint
and ð‘ is the total number of data points in the training
dataset. A graphical example of a linear regression and
its residuals is shown in Fig. 2. Linear regression using
residual summed squared error can work very well and
is a fast learning algorithm, so we suggest it as a base-
line method before choosing more complicated methods.
Residual!=(ð‘¦+" âˆ’ð‘¦!) The exact minimization method is beyond the scope of
thispaper, butknowthattheminimizationusestheslope
+ ð‘ (i.e., derivative) of the loss function to determine how to
ð‘¤ ð‘¥ adjust the trainable weights. If this sounds familiar, that
=
ð‘¦! is because it is the same minimization technique learned
inmostfirstyearcollegecalculusclassesandisasimilar
technique to what is used in data assimilation for numer-
ical weather prediction (c.f., Chapter 5 and Section 10.5
in Kalnay 2002; Lackmann 2011). The concept of using
thederivativetofindtheminimumisrepeatedthroughout
Fig. 2. A visual example of linear regression with a single input
mostMLmethodsgiventhereisoftenaminimization(or
predictor.Thex-axisisasyntheticinputfeature,they-axisisasynthetic
maximization)objective.
outputlabel.Thesolidblacklineistheregressionfit,andthereddashed
linesaretheresiduals. Occasionally datasets can contain irrelevant or noisy
predictors which can cause instabilities in the learning.
a. LinearRegression One approach to address this is to use a modified ver-
sionoflinearregressionknownasridgeregression(Hoerl
An important concept in ML is when choosing to use and Kennard 1970), which minimizes both the summed
MLforatask,oneshouldstartwiththesimplerMLmodels squared error (like before) and the sum of the squared
first. Occamâ€™srazor2tellsustopreferthesimplestsolution weights called an ð¿ penalty. Mathematically, the new
2
that can solve the task or represent the data. While this lossfunctioncanbedescribedas
doesnâ€™t always mean the simplest ML model available, it
doesmeanthatsimplermodelsshouldbetriedbeforemore âˆ‘ï¸ ð‘ âˆ‘ï¸ ð·
complicatedones(Holte1993). Thus,thefirstMLmethod ð‘…ð‘†ð‘† ð‘Ÿð‘–ð‘‘ð‘”ð‘’= (ð‘¦ ð‘—âˆ’ð‘¦Ë†ð‘—)2+ðœ† ð‘¤2 ð‘– (3)
discussedislinearregressionwhichhasalonghistoryin ð‘—=1 ð‘–=0
meteorology (e.g. Malone 1955) and forms the heart of
Here,ðœ†(whichisâ‰¥0)isauser-definedparameterthatcon-
themodeloutputstatisticsproduct(i.e.,MOS;Glahnand
trolstheweightofthepenalty. Likewise,anothermodified
Lowry 1972) that many meteorologists are familiar with.
versionoflinearregressionislassoregression(Tibshirani
Linearregressionispopularbecauseitisasimplemethod
1996) which minimizes the sum of the absolute value of
thatisalsocomputationallyefficient. Atitssimplestform,
theweights. Thispenaltytolearningisalsotermedanð¿
linearregressionapproximatesthevalueyouwouldliketo 1
penalty. Thelassolossfunctionmathematicallyis
predict (ð‘¦Ë†) by fitting weight terms (ð‘¤ ð‘–) in the following
equation, ð‘ ð·
ð‘– âˆ‘ï¸ =ð· ð‘…ð‘†ð‘† ð‘™ð‘Žð‘ ð‘ ð‘œ= âˆ‘ï¸ (ð‘¦ ð‘—âˆ’ð‘¦Ë†ð‘—)2+ðœ† âˆ‘ï¸ |ð‘¤ ð‘–| (4)
ð‘¦Ë†= ð‘¤ ð‘– ð‘¥ ð‘– . (1) ð‘—=1 ð‘–=0
ð‘–=0
Thefirstpredictor(ð‘¥ )isalways1sothatð‘¤ isabiasterm, Bothlassoandridgeencouragethelearnedweightstobe
0 0
allowingthefunctiontomovefromtheoriginasneeded. small but in different ways. The two penalties are often
ð· isthenumberoffeaturesforthetask. combinedtocreatetheelastic-netpenalty(ZouandHastie
As noted before, with ML, the objective is to find ð‘¤ ð‘– 2005)
such that a user-specified loss function (i.e., error func-
ð‘ ð·
tion) is minimized. The most common loss function for âˆ‘ï¸ âˆ‘ï¸
traditionallinearregressionistheresidualsummedsquared
ð‘…ð‘†ð‘† ð‘’ð‘™ð‘Žð‘ ð‘¡ð‘–ð‘= (ð‘¦ ð‘—âˆ’ð‘¦Ë†ð‘—)2+ðœ† (ð›¼ð‘¤2
ð‘–
+(1âˆ’ð›¼)|ð‘¤ ð‘–|).
ð‘—=1 ð‘–=0
error(RSS):
(5)
ð‘
ð‘…ð‘†ð‘†= âˆ‘ï¸ (ð‘¦ ð‘—âˆ’ð‘¦Ë†ð‘—)2 (2) Ingeneral,theadditionofcomponentstothelossfunction,
likedescribedinEq. 3-5,isknownasregularizationandis
ð‘—=1
foundinotherMLmethods. Somerecentexamplesofpa-
persusinglinearregressionincludesubseasonalprediction
of tropical cyclone parameters (Lee et al. 2020), relating
2https://en.wikipedia.org/wiki/Occam%27s_razor
4
ð‘ƒ 32Â°Fsnow)
ð‘ƒ 32Â°Frain)
Fig.3. Agraphicaldepictionofthesigmoidfunction(Eq. 6). The Fig.4. Visualizingtheprobabilityofainputfeaturegiventheclass
x-axisisthepredictedlabelvalue,whilethey-axisisthenowscaled label. Thisexampleiscreatedfromfiveminuteweatherstationobser-
value. vationsfromnearMarquette,Michigan(yearsincluded: 2005-2020).
Precipitationphasewasdeterminedbythepresentweathersensor. The
histogramisthenormalizednumberofobservationsinthattemperature
mesocyclonecharacteristicstotornadointensity(Sessaand bin,whilethesmoothcurvesarethenormaldistributionfittothedata.
Trapp2020)andshorttermforecastingoftropicalcyclone Redareraininginstances,bluearesnowinginstances.
intensity(Huetal.2020).
in more information on the mathematical techniques of
b. LogisticRegression
minimizationtheycanfindmoreinformationinChapter5
Asacomplementtolinearregression,thefirstclassifica- ofKalnay(2002).
tionmethoddiscussedhereislogisticregression. Logistic Logisticregressionhasbeenusedforalongtimewithin
regressionisanextensionfromlinearregressioninthatit meteorology. One of the earliest papers using logistic
uses the same functional form of Eq. 1. The differences regressionshowedskillinpredictingtheprobabilityofhail
lieinhowtheweightsforEq. 1aredeterminedandami- greaterthan1.9cm(Billetetal.1997),whilemorerecent
noradjustmenttotheoutputofEq. 1. Morespecifically, papershaveusedlogisticregressiontoidentifystormmode
logisticregressionappliesthesigmoidfunction(Fig. 3)to (Jergensenetal.2020), subseasonalpredictionofsurface
theoutputofEq. 1definedas: temperature(Vigaudetal.2019)andpredictthetransition
of tropical cyclones to extratropical cyclones (Bieli et al.
1
ð‘†(ð‘¦Ë†)= (6) 2020).
1+ð‘’âˆ’ð‘¦Ë†
Largepositivevaluesintothesigmoidresultsinavalueof c. NaÃ¯veBayes
1whilelargenegativevaluesresultinavalueof0. Effec-
An additional method to do classification is known as
tively,thesigmoidscalestheoutputofEq. 1toarangeof
naÃ¯veBayes(Kuncheva2006),whichisnamedforitsuse
0to1,whichthencanbeinterpretedlikeaprobability. For
ofBayesâ€™stheoremandcanbewrittenasthefollowing:
thesimplestcaseofclassificationinvolvingjusttwoclasses
(e.g.,rainorsnow),theoutputofthesigmoidcanbeinter- ð‘ƒ(ð‘¦)ð‘ƒ(ð‘¥|ð‘¦)
pretedasaprobabilityofeitherclass(e.g.,rainorsnow). ð‘ƒ(ð‘¦|ð‘¥)= . (8)
ð‘ƒ(ð‘¥)
The output probability then allows for the classification
tobeformulatedastheð‘¤ ð‘– thatmaximizestheprobability In words, Eq. 8 is looking for the probability of some
ofadesiredclass. Mathematically, theclassificationloss labelð‘¦(e.g.,snow),givenasetofinputfeaturesð‘¥(ð‘ƒ(ð‘¦|ð‘¥);
functionforlogisticregressioncanbedescribedas e.g.,temperature). Thisprobabilitycanbecalculatedfrom
knowing the probability of the label ð‘¦ occurring in the
ð‘–=ð·
loss=
âˆ‘ï¸
âˆ’ð‘¦ ð‘–log(ð‘†(ð‘¦Ë†))+(1âˆ’ð‘¦ ð‘–)log(1âˆ’ð‘†(ð‘¦Ë†)). (7)
dataset(ð‘ƒ(ð‘¦);e.g.,howfrequentitsnows)timestheprob-
ability of the input features given it belongs to the class
ð‘–=0
ð‘¦ (ð‘ƒ(ð‘¥|ð‘¦);e.g.,howfrequentlyisit32oFwhenitâ€™ssnow-
Likebeforeforlinearregression, theexpressioninEq. 7 ing),dividedbytheprobabilityoftheinputfeatures(ð‘ƒ(ð‘¥)).
isminimizedusingderivatives. Ifthereaderisinterested The naÃ¯ve part of the naÃ¯ve Bayes algorithm comes from
5
assumingthatallinputfeaturesð‘¥,areindependentofone
anotherandthetermð‘ƒ(ð‘¥|ð‘¦)canbemodeledbyanassumed
distribution(e.g.,normaldistribution)withparametersde-
terminedfromthetrainingdata. Whiletheseassumptions
are often not true, the naÃ¯ve Bayes classifier can be skill-
ful in practice. A few simplification steps results in the
following
ð‘
âˆ‘ï¸
ð‘¦Ë†=argmax(log(ð‘ƒ(ð‘¦))+ log(ð‘ƒ(ð‘¥ ð‘–|ð‘¦))). (9)
ð‘–=0
Againinwords,thepredictedclass(ð‘¦Ë†)fromnaÃ¯veBayesis
theclassificationlabel(ð‘¦)suchthatthesumofthelogof
theprobabilityofthatclassification(ð‘ƒ(ð‘¦))andthesumof
logofalltheprobabilitiesofthespecificinputsgiventhe
classification(ð‘ƒ(ð‘¥ ð‘–|ð‘¦))ismaximized. Inordertohelpvi-
Fig.5. Avisualrepresentationofthetwofunctionsthatcanbeusedin
sualizethequantityð‘ƒ(ð‘¥ ð‘–|ð‘¦),agraphicalexampleisshown
decisiontreesforclassification,Entropy(blue)andGiniimpurity(red).
in Fig. 4. This example uses surface weather measure-
mentsfromastationnearMarquette,Michiganwheredata
were compiled when it was raining and snowing. Fig. 4 d. TreesandForests
shows distribution of air temperature (i.e., an input fea-
Decisiontreesarebasedonadecisionmakingmethod
ture)giventhetwoclasses(i.e.,rainvssnow). Inorderto
thathumanshavebeenusingforyears: flowcharts,where
getð‘ƒ(ð‘¥ ð‘–|ð‘¦),weneedtoassumeanunderlyingdistribution
the quantitative decision points within the flow chart are
function. The common assumed distribution with naÃ¯ve
learnedautomaticallyfromthedata. Earlyuseofdecision
Bayesisthenormaldistribution
treesinmeteorology(e.g.,Chisholmetal.1968)actually
pre-dated the formal description of the decision tree al-
ð‘“(ð‘¥;ðœ‡,ðœŽ)= âˆš 1 ð‘’âˆ’ 2 1(ð‘¥ ðœŽ âˆ’ðœ‡) (10) gorithm (Breiman 1984; Quinlan 1993; Breiman 2001).
ðœŽ 2ðœ‹
Since then, tree-based methods have grown in popularity
whereðœ‡isthemeanandðœŽisthestandarddeviationofthe and have been demonstrated to predict a variety of com-
training data. While the normal distribution assumption plexmeteorologicalphenomena. Topicsinclude: aviation
applications(e.g.,Williamsetal.2008a,b;Williams2014;
for the temperature distribution in Fig. 4 is questionable
MuÃ±oz-Esparza et al. 2020); severe weather (e.g., Gagne
due to thermodynamic constraints that lock the tempera-
tureat32oF(i.e.,latentcool/heating),naÃ¯vebayescanstill etal.2009,2013;McGovernetal.2014;Mecikalskietal.
2015;Lagerquistetal.2017;Gagneetal.2017;Czernecki
have skill. Initially, it might not seem like any sort of
etal.2019;Burkeetal.2020;Hilletal.2020;Lokenetal.
weights/biasesarebeingfitlikethepreviouslymentioned
2020; Gensini et al. 2021; Flora et al. 2021; Loken et al.
methods (e.g., logistic regression), but ðœ‡ and ðœŽ are be-
2022); solarpower(e.g.,McGovernetal.2015); precipi-
ing learned fron the training data. If performance from
tation (e.g., Elmore and Grams 2016; Herman and Schu-
thenormaldistributionispoor,otherdistributionscanbe
macher2018b,a;Taillardatetal.2019;Lokenetal.2020;
assumed,likeamultinomialoraBernoullidistribution.
Wangetal.2020;MaoandSorteberg2020;Lietal.2020;
ApopularuseofnaÃ¯veBayesclassificationinthemete-
HillandSchumacher2021;Schumacheretal.2021);satel-
orologicalliteraturehasbeentheimplementationofProb- liteandradarretrievals(e.g.,KÃ¼hnleinetal.2014;Conrick
Severe(e.g.,Cintineoetal.2014,2018,2020)whichuses etal.2020;Yangetal.2021;Zhangetal.2021)andclimate
variousseverestormparametersandobservationstoclas- relatedtopics(e.g.,Cuietal.2021).
sify the likelihood of any storm becoming severe in the Tostart,wewilldescribedecisiontreesincontextofa
next60minutes. AdditionalexamplesofnaÃ¯veBayesclas- classificationproblem. Thedecisiontreecreatessplitsin
sifiersinmeteorologyhavebeenusedforidentifyingtrop- thedata(i.e.,decisions)thatarechosensuchthateitherthe
icalcyclonesecondaryeyewallformationfrommicrowave Gini Impurity value or the Entropy value decreases after
imagery(KossinandSitkowski2009),identifyinganoma- thesplit. GiniImpurityisdefinedas
louspropagationinradardata(Peteretal.2013)andpre-
ð‘–=ð‘˜
cipitationtype(e.g.,Convective/Stratiform)retrievalsfrom Gini= âˆ‘ï¸ ð‘ ð‘–(1âˆ’ð‘ ð‘–) (11)
geostationarysatellites(Gramsetal.2016).
ð‘–=0
6
where ð‘ ð‘– is the probability of class i (i.e., the number of insteadoftrainingmultipletreesonrandomsubsets(i.e.,
data points labeled class i divided by the total number of random forest), each tree in the ensemble is successively
datapoints). WhileEntropyisdefinedas trainedontheremainingerrorfromtheprevioustrees. To
put it another way, rather than minimizing the total error
âˆ‘ï¸
ð‘–=ð‘˜
onrandomtrees,thereducederrorfromthefirstdecision
Entropy= ð‘ ð‘–log
2
(ð‘ ð‘–). (12)
treeisnowminimizedonthesecondtree,andthereduced
ð‘–=0
error from trees one and two is then minimized on the
third tree and so on. In order to come up with a single
Both functions effectively measure how similar the data
predictionoutoftheensembleoftrees,thepredictionscan
point labels are in each one of the groupings of the tree
be combined through a voting procedure (i.e., count up
after some split in the data. Envision the flow chart as
thepredictedclassesofeachtree)orbytakingtheaverage
a tree. The decision is where the tree branches into two
probabilistic output from each tree. Random forests can
directions,resultingintwoseparateleaves. Thegoalofa
useeithermethod,whilegradientboostedtreesarelimited
decisiontreeistochoosethebranchthatresultsinaleaf
tothevotingprocedure.
havingaminimumofGiniorEntropy. Inotherwords,the
Whilethediscussionherehasbeencenteredonclassi-
data split would ideally result in two sub-groups of data
fication for the tree-based methods, they can be used for
where all the labels are the same within each sub-group.
regressionaswell. Themainalterationtothedecisiontree
Fig. 5 shows both the Gini impurity and entropy for a
methodtoconverttoaregression-basedproblemisthesub-
two class problem. Consider the example of classifying
stitutionofthelossfunction(i.e.,Eq. 11-12). Forexample
winterprecipitationasrainorsnow. Fromsomeexample
a common loss function for random forest for regression
surface temperature dataset the likely decision threshold
wouldbenear32â—¦F,whichwouldresultinthesubsequent andgradientboostedregressionisthesamelossfunction
aslinearregressiondescribedintheprevioussection(e.g.,
twogroupingsofdatapointlabels(i.e.,snow/rain)having
Eq. 2),theresidualsummedsquarederror.
a dominant class label (i.e., fraction of class k is near 0
or1)andthushavingaminimumofEntropyorGini(i.e.,
near0). Theactualoutputofthistreecouldbeeitherthe e. SupportVectorMachines
majorityclasslabel,ortheratioofthemajorclass(i.e.,a
A support vector machine (commonly referred to as
probabilisticoutput).
SVM; Vapnik 1963) is an ML method similar to linear
While it is helpful to consider a decision tree with a
and logistic regression. The idea is that a support vector
single decision, also known as a tree with a depth of 1,
machineusesalinearboundarytodoitspredictions,which
thepredictionpowerofasingledecisionislimited. Astep
hasasimilarmathematicalformbutwrittendifferentlyto
towardmorecomplexityistoincludeincreasingdepth(i.e.,
accounttovectornotation. Theequationis
moredecisions/branches). Tocontinuewiththerain/snow
examplefromthepreviousparagraph,wecouldincludea ð‘¦Ë†=w ð‘‡ x+ð‘ (13)
seconddecisionbasedonmeasuredwetbulbtemperature.
Atreewithdepthtwowilllikelyhavebetterperformance, wherewisavectorofweights,xisavectorofinputfea-
butthepredictionpowerisstillsomewhatlimited. tures, b is a bias term and ð‘¦Ë† is the regression prediction.
Anadditionalsteptoincreasethecomplexityofdecision Inthecaseofclassification,onlysignoftherightsideof
trees, beyond including more predictors, is a commonly Eq. 13 is used. This linear boundary can be generalized
usedmethodinmeteorology: ensembles. Whileitmight beyondtwo-dimensionalproblems(i.e.,twoinputfeatures)
notbeclearhere,decisiontreesbecomeover-fit(i.e.,work tothree-dimensionswherethedecisionboundaryiscalled
really well for training data, but perform poorly on new aplane, oranyhigherorderspacewheretheboundaryis
data) as the depth of the tree increases. An alternative called a hyperplane. The main difference between linear
approachistouseanensembleoftrees(i.e.,aforest). Us- methods discussed in Sections 2a-2b and support vector
inganensembleoftreesformsthebasisoftwoadditional machinesisthatsupportvectormachinesincludemargins
tree based methods: random forests (Breiman 2001) and to the linear boundary. Formally, the margin is the area
gradientboosteddecisiontrees(Friedman2001). between the linear boundary and the closest training dat-
Randomforestsareacollectionofdecisiontreesthatare apoint for each class label (e.g., closest rain data point
trainedonrandomsubsetsofdataandrandomsubsetsof andclosestsnowdatapoint). Thisisshownschematically
input variables from the initial training dataset. In other withasyntheticdatasetinFig. 6a. Whilethisisanideal
words,themathematicsareexactlythesameforeachtree, case,usuallyclassesoverlap(Fig. 6b),butsupportvector
thedecisionsstillaimtominimizetheloss(e.g.,Entropy), machines can still handle splitting the classes. The opti-
but each tree is given a different random subset of data mizationtaskforsupportvectormachinesisstatedasthe
sampledfromtheoriginaldatasetwithreplacement. Gra- following: Find wð‘‡ such that the margin is maximized.
dientboosteddecisiontreesareanensembleoftreesthat Inotherwords,supportvectormachinesaimtomaximize
7
Input Feature 1
2
erutaeF
tupnI
a) Class 2
b)
ð’˜
ð’˜ð‘»ð’™+ð‘=0
Class 1
Fig.6.Supportvectormachineclassificationexamples.(a)ideal(synthetic)datawherethexandyaxisarebothinputfeatures,whilethecolor
designateswhatclasseachpointbelongsto.Thedecisionboundarylearnedbythesupportvectormachineisthesolidblackline,whilethemargin
isshownbythedashedlines.(b)arealworldexampleusingNAM18ZforecastsofUandVwindandtippingbucketmeasurementsofprecipitation.
Blueplusmarkersareraininginstancesandtheredminussignsarenon-raininginstances.Blacklinesarethedecisionboundaryandmargins.
the distance between the two closest observations on ei- GOES-16 and NEXRAD. An example storm event and
ther side of the hyperplane. Mathematically, the margin the5measuredvariables: Redchannelvisiblereflectance
distanceisdescribedas (0.64ðœ‡m;Channel2),midtroposphericwatervaporbright-
nesstemperature(6.9ðœ‡m;Channel9),cleaninfraredwin-
1
margin= . (14) dow brightness temperature (10.7 ðœ‡m; Channel 13), Ver-
wð‘‡w
ticallyIntegratedLiquid(VIL;fromNEXRAD)andGeo-
Like before, the maximization is handled by numerical stationary Lightning Mapper (GLM) measured lightning
techniques to optimize the problem but the resulting so- flashesarefoundinFig. 7. InadditiontodiscussingML
lution will be the hyperplane with the largest separation incontextoftheSEVIRdataset,thissectionwillfollowthe
between the classes. A powerful attribute of the support generalstepstousingMLandcontainhelpfuldiscussions
vector machine method is that it can be extended to ad- ofthebestpracticesaswellasthemostcommonpitfalls.
ditional mathematical formulations for the boundary, for
exampleaquadraticfunction. Thusthepersonusingsup-
port vector machines can decide which function would a. ProblemStatements
work best for their data. Recent applications of support
TheSEVIRdatawillbeappliedtotwotasks: (1)Does
vectormachinesinmeteorologyincludetheclassification
this image contain a thunderstorm? and (2) How many
ofstormmode(Jergensenetal.2020),hindcastsoftropi-
lightningflashesareinthisimage? Tobeexplicit,weas-
calcyclones(Neetuetal.2020)andevaluatingerrorswith
sumetheGLMobservationsareunavailableandweneed
quantitative precipitation retrievals in the United States
to use the other measurements (e.g., infrared brightness
(Kurdzoetal.2020).
temperature) as features to estimate if there are lightning
flashes (i.e., classification), and how many of them are
3. MachineLearningApplicationandDiscussion
there(i.e.,regression). Whilebothofthesetasksmightbe
ThissectionwilldiscusstheuseofallMLmethodswith consideredredundantsincewehaveGLM,thegoalofthis
afamiliaruse-case: thunderstorms. Specifically,thissec- paper is to provide discussion on how to use ML as well
tionwillshowtwoMLapplicationsderivedfrompopular asdiscussionontheMLmethodsthemselves. Thatbeing
meteorologicaldatasets: radarandsatellite. Theparticu- said, a potential useful application of the trained models
lar data used are from the Storm EVent ImageRy dataset hereinwouldbetousethemonsatellitesensorsthatdonot
(SEVIR;Veilletteetal.2020),whichcontainsover10,000 havelightningmeasurements. Forexample,allgenerations
storm events from between 2017 and 2019. Each event ofGOESpriortoGOES-16didnothavealightningsensor
spans four hours and includes measurements from both co-located with the main sensor. Thus, we could poten-
8
Fig.7. AnexamplestormimagefromtheStormEVentImageRydataset. Thiseventisfrom06August2018. (a)thevisiblereflectance(b)
themid-troposphericwatervaporbrightnesstemperature(c)thecleaninfraredbrightnesstemperatures(d)theverticallyintegratedliquidretrieved
fromNEXRADand(e)griddedGLMnumberofflashes.(a)alsohasannotatedlocationsofrepresentativepercentilesthatwereengineeredfeatures
usedfortheMLmodels.
tially use the ML models trained here to estimate GLM model will not know how to differentiate between spu-
measurementspriortoGOES-16(i.e.,November2016). rious data and high quality data. A common anecdote
when using ML models is garbage in, garbage out. The
SEVIRdatasethasalreadygonethroughrigorousquality
control,butthisisoftennotthecasewithrawmeteorolog-
b. Data
ical datasets. Two examples of quality issues that would
ThefirststepofanyMLprojectistoobtaindata. Here, likely be found in satellite and radar datasets are satellite
thedataarefroma publicarchivehostedontheAmazon artifacts(e.g.,GOES-17heatpipe;McCorkeletal.2019)
WebService. ForinformationofhowtoobtaintheSEVIR andradargroundclutter(e.g.,Hubbertetal.2009). Clean-
data as well as the code associated with this manuscript ing and manipulating the dataset to get it ready for ML
seetheDataAvailabilityStatement. Onemajorquestionat oftentakesaresearcher50%-80%oftheirtime3. Thus,
thisjunctureis,"Howmuchdataisneededtodomachine donotbediscouragedifcleaningoneâ€™sdatasetsistaking
learning?". While there does not exist a generic number alargeamountoftimebecauseahigh-qualitydatasetwill
thatcanapplytoalldatasets,theideaistoobtainenough bebestforhavingasuccessfulMLmodel.
data such that oneâ€™s training data are diverse. A diverse Subsequenttocleaningthedata, thenextstepistoen-
datasetisdesiredbecauseanybiasfoundwithinthetrain- gineer the inputs (i.e., features) and outputs (i.e., labels).
ingdatawouldbeencodedintheMLmethod(McGovern Oneavenuetocreatefeaturesistouseeverysinglepixelin
etal.2021). Forexample, ifaMLmodelwastrainedon theimageasapredictor. Whilethiscouldwork,giventhe
only images where thunderstorms were present, then the numberofpixelsintheSEVIRimages(589,824totalpix-
ML model would likely not know what a non-lightning elsforonevisibleimage)itiscomputationallyimpractical
producingstormwouldlooklikeandbebiased. Diversity totrainaMLmodelwithallpixels. Thus,wearelooking
intheSEVIRdatasetiscreatedbyincludingrandomim- forasetofstatisticsthancanbeextractedfromeachimage.
ages(i.e.,nostorms)fromallaroundtheUnitedStates(c.f. Forthegenerationoffeatures,domainknowledgeiscriti-
Figure2inVeilletteetal.2020).
After obtaining the data, it is vital to remove as much
3https://www.nytimes.com/2014/08/18/technology/
spurious data as possible before training because the ML
for-big-data-scientists-hurdle-to-insights-is-janitor-work.
html
9
calbecausechoosingmeteorologicallyrelevantquantities varyingofthehyperparameterstheMLpractitionerisin-
will ultimately determine the ML models skill. For the advertently tuning a ML model to the validation dataset.
ML tasks presented in Section 3a, information about the One will often choose specific hyperparameters in such
stormcharacteristics(e.g.,strength)intheimagewouldbe a way to achieve the best performance on the validation
beneficialfeatures. Forexample, amoreintensestormis dataset. Thus, to provide a truly unbiased assessment of
oftenassociatedwithmorelightning. Proxiesforestimat- thetrainedMLmodelskillforunseendata,thetestdataset
ingstormstrengthwouldbe: themagnitudeofreflectance issetasideandnotuseduntilaftertrainingallMLmodels.
in the visible channel; how cold brightness temperatures Itiscommonpracticeoutsideofmeteorology(i.e.,data
inthewatervaporandcleaninfraredchannelare;andhow science) to randomly split the total dataset into the three
muchverticallyintegratedwaterthereis. Thus,tocharac- subsets. However,itisimportanttostriveforindependence
terizethesestatistics,weextractthefollowingpercentiles of the various subsets. A data point in the training set
fromeachimageandvariable: 0,1,10,25,50,75,90,99,100. should not be highly correlated to a data point in the test
To create the labels the number of lightning flashes in set. In meteorology this level of independence is often
theimagearesummed. ForProblemStatement1,animage challenginggiventhefrequentspatialandtemporalauto-
isclassifiedascontainingathunderstormiftheimagehas correlations in meteorologic data. Consider the SEVIR
at least one flash in the last five minutes. For Problem dataset. Each storm event has four hours of data broken
Statement2,thesumofalllightningflashesinthepastfive intofiveminutetimesteps. Foronestormevent, thereis
minuteswithintheimageareusedfortheregressiontarget. alargecorrelationbetweenadjacentfiveminutesamples.
Nowthatthedatahavebeenqualitycontrolledandour Thus, randomly splitting the data would likely provide a
features and labels have been extracted, the next step is biased assessment of the true skill of the ML model. In
tosplitthatdatasetintothreeindependentsub-categories ordertoreducethenumberofcorrelateddatapointsacross
named the training, validation and testing sets. The rea- subsets, time is often used to split the dataset. For our
sonforthesethreesub-categoriesisbecauseoftherelative example,wechoosetosplittheSEVIRdataupbytraining
ease at which ML methods can "memorize" the training on01Jan2017-01Jun2019andspliteveryotherweekin
data. ThisoccursbecauseMLmodelscancontainnumer- the rest of 2019 into the validating and testing sets. This
ous(e.g.,hundreds,thousands,orevenmillions)learnable equates to a 72%, 13% and 15% split for the training,
parameters,thustheMLmodelcanlearntoperformwell validation and test sets respectively. In the event that the
onthetrainingdatabutnotgeneralizetoothernon-training total dataset is small and splitting the data into smaller
data, which is called over-fitting. In order to assess how subsetscreateslessrobuststatistics,aresamplingmethod
over-fitaMLmodelis,itisimportanttoevaluateatrained knownask-foldcross-validation(e.g.,Bischletal.2012;
MLmodelondataoutsideofitstrainingdata(i.e.,valida- Goodfellowetal.2016)canbeused. TheSEVIRdataset
tionandtestingsets). wassufficientlylargethatwechosenottodok-foldcross-
The training dataset is the largest subset of the total validation, but a meteorological example using it can be
amountofdata. Thereasonthetrainingsetisthelargestis foundinShieldandHouston(2022).
becausetheaforementioneddesiredoutcomeofmostML
modelsistogeneralizeonwidevarietyofexamples. Typ- c. TrainingandEvaluation
ically,theamountoftrainingdataisbetween70-85%of
1) Classification
thetotalamountofdataavailable. Thevalidationdataset,
regularly 5-15% of the total dataset, is a subset of data AsstatedinSection3.a,task(1)istoclassifyifanimage
usedtoassessifaMLmodelisover-fitandisalsousedfor contains a thunderstorm. Thus, the classification meth-
evaluating best model configurations (e.g., the depth of a odsavailabletodothistaskare: logisticregression,naÃ¯ve
decisiontree). Thesemodelconfigurationsarealsoknown Bayes,decisiontrees,randomforest,gradientboostedtrees
as hyperparameters. Machine learning models have nu- andsupportvectormachines. Inordertofindanoptimal
merousconfigurationsandpermutationsthatcanbevaried ML model, it is often best to try all methods available.
and could impact the skill of any one trained ML model. While this might seem like a considerable amount of ad-
Thus,commonpracticeistosystematicallyvarytheavail- ditional effort, the ML package used in this tutorial (i.e.,
ablehyperparameterchoices,alsocalledagridsearch,and scikit learn4) uses the same syntax for all methods (e.g.,
thenevaluatethedifferenttrainedmodelsbasedontheval- method.fit(ð‘‹,ð‘¦), method.predict(ð‘‹ ð‘£ð‘Žð‘™)),. Thus, fitting all
idationdataset. Hyperparamterswillbediscussedinmore availablemethodsdoesnotrequiresubstantiallymoreef-
detaillater. Thetestdatasetisthelastgroupingthatisset fortfromtheMLpractitionerandwilllikelyresultinfind-
ingabestperformingmodel.
asidetotheveryendoftheMLprocess. Thetestdataset
Tostartoff,allmethodsareinitiallytrainedusingtheir
is often of similar size to the validation dataset, but the
defaulthyperparametersinscikit-learnandjustoneinput
key difference is that the test dataset is used after all hy-
perparametervariationshavebeenconcluded. Thereason
feature,theminimuminfraredbrightnesstemperature(ð‘‡ ð‘).
forthislastdatasetisbecausewhendoingthesystematic
4https://scikit-learn.org/stable/
10
oftheclassification(e.g.,thisimageis95%likelytohave
lightning in it). When calculating the accuracy before,
we assumed a threshold of 50% to designate what the
ML prediction was. In order to get the ROC curve, the
thresholdprobabilityisinsteadvariedfrom0%to100%.
TheresultingROCcurvesforalloftheMLmethodsexcept
supportvectormachinesareshowninFig. 9a. Weseethat
for this simple one feature model, all methods are still
very similar and have AUCs near 0.9 (Fig. 9a), which is
generallyconsideredgoodperformance5.
An additional method for evaluating the performance
of classification method is called a performance diagram
(Figure 9b; Roebber 2009). The performance diagram is
alsocalculatedfromthecontingencytable,usingthePOD
againforthey-axis,butthistimethex-axisisthesuccess
ratio(SR)whichisdefinedas
Fig.8. Thenormalizeddistributionsofminimumbrightnesstem- TruePositive
perature(ð‘‡ ð‘)fromthecleaninfraredchannelforthunderstormimages SR=
TruePositive+FalsePositive
. (17)
(blue;T-storm)andnon-thunderstormimages(red;NoT-storm).
Fromthisdiagram,severalthingscanbegleanedaboutthe
models performance. In general, the top right corner is
Wechoosetouseð‘‡ ð‘becausemeteorologicallyitisaproxy
where â€™bestâ€™ performing models are found. This area is
for the depth of the storms in the domain, which is cor-
characterizedbymodelsthatcapturenearlyallevents(i.e.,
related to lightning formation (Yoshida et al. 2009). To
thunderstorms), whilenotpredictingalotoffalsealarms
assess the predictive power of this variable, the distribu-
(i.e., false positives). This corner is also associated with
tions of ð‘‡ ð‘ for thunderstorms and no thunderstorms are highvaluesofcriticalsuccessindex(CSI;filledcontours
showninFig. 8. Asexpected,ð‘‡ ð‘ forthunderstormsshow Fig. 9b),definedas
morefrequentlowertemperaturesthannon-thunderstorm
images. Training all methods usingð‘‡ ð‘ achieves an accu- CSI= TruePositive .
racyof80%onthevalidationdataset. Whileaccuracyisa TruePositive+FalsePositive+FalseNegative
commonandeasytounderstandmetric,itisbesttoalways (18)
usemorethanonemetricwhenevaluatingMLmethods. whichisametricthatshowsamodelâ€™sperformancewithout
Anothercommonperformancemetricforclassification considering the true negatives. Not considering the true
tasksistheAreaUndertheCurve(AUC).Morespecifically negativesisimportantbecausetruenegativescandominate
the common area metric is associated with the Receiver ML tasks in meteorology given the often rare nature of
OperatingCharacteristicscurve(ROC).TheROCcurveis events with large impacts (e.g., floods, hail, tornadoes).
calculatedfromtherelationshipbetweentheProbabilityof The last set of lines on this diagram are the frequency
FalseDetection(POFD)andtheProbabilityofDetection biascontours(dashedgreylinesFig. 9b). Thesecontours
(POD). Both POFD and POD parameters are calculated indicateifamodelisover-forecastingorunder-forecasting.
from determining parameters within a contingency table ForthesimpleMLmodelstrained,eventhoughmostof
which are the true positives (both the ML prediction and them have a similar accuracy and AUC, the performance
label say thunderstorm), false positives (ML prediction diagram suggests their performance is indeed different.
predicts thunderstorm, label has no thunderstorm), false Consider the tree based methods (green box; Fig. 9b).
negatives(MLpredictionisnothunderstorm,labelshows TheyarealleffectivelyatthesamelocationwithaPODof
there is a thunderstorm) and true negatives (ML says no about 0.9 and a SR of about 0.75, which is a region that
thunderstorm,labelsaysnothunderstorm). ThePOFDand hasafrequencybiasofalmost1.5. Meanwhilethelogistic
PODaredefinedby regression,supportvectormachinesandnaÃ¯veBayesmeth-
odsaremuchclosertothefrequencybiaslineof1,while
POFD= FalsePositive . (15) having a similar CSI as the tree based methods. Thus,
TruePositive+FalsePositive after considering overall accuracy, AUC and the perfor-
mancediagram,thebestperformingmodelwouldbeeither
TruePositive
POD= . (16) the logistic regression, support vector machines or naÃ¯ve
TruePositive+FalseNegative
Bayes. At this junction, the practitioner has the option
AlloftheMLmodels, exceptsupportvectormachines
(ascodedinsklearn),canprovideaprobabilisticestimation
5Noformalpeerreviewedjournalstatesthis,itismoreofaruleof
thumbinmachinelearningpractice
11
a) b)
Fig.9. Performancemetricsfromthesimpleclassification(onlyusingð‘‡ ð‘). (a)ReceiverOperatingCharacteristic(ROC)curvesforeachML
model(exceptsupportvectormachines), logisticregression(LgR;blue), naÃ¯veBayes(NB;red), decisiontree(DT;geen), randomforest(RF;
yellow)andgradientboostedtrees(GBT;lightgreen). TheareaundertheROCcurveisreportedinthelegend. (b)PerformanceDiagramforall
MLmodels(samecolorsasa). ColorfillisthecorrespondingCSIvalueforeachSuccessRatio-ProbabilityofDetection(SR,POD)pair. Dashed
contoursarethefrequencybias.
toconsideriftheywantaslightlyover-forecastingsystem anomaly)whicharedefinedmathematicallyas
o
st
r
or
a
m
s
,
li
n
g
o
h
-
t
t
l
h
y
u
u
n
n
d
d
e
e
rs
r
t
-
o
fo
rm
rec
t
a
a
s
s
t
k
in
,
g
th
s
e
y
re
ste
a
m
re
.
no
F
t
o
m
r
a
th
n
e
y
t
i
h
m
u
p
n
l
d
ic
e
a
r-
- minmax=
ð‘¥âˆ’ð‘¥
ð‘šð‘–ð‘› . (19)
ð‘¥ ð‘šð‘Žð‘¥âˆ’ð‘¥
ð‘šð‘–ð‘›
tions for over-forecasting or under-forecasting. However,
developersofatornadopredictionmodelmaypreferasys- and
tem that produces more false positives (over-forecasting; ð‘¥âˆ’ðœ‡
storm warned, no tornado) than false negatives (under- standardanom.= ðœŽ (20)
forecasting; storm not warned, tornado) as missed events
respectively. InEq. 19,ð‘¥ ð‘šð‘–ð‘›istheminimumvaluewithin
couldhavesignificantimpacttolifeandproperty. Itshould thetrainingdatasetforsomeinputfeatureð‘¥ whileð‘¥ ð‘šð‘Žð‘¥ is
be clear that without going beyond a single metric, this the maximum value in the training dataset. In Eq. 20, ðœ‡
differentiationbetweentheMLmethodswouldnotbepos- isthemeanoffeatureð‘¥inthetrainingdatasetandðœŽisthe
sible. standarddeviation. Forthispaper,thestandardanomalyis
While the previous example was simple by design, we used.
ashumanscouldhaveusedasimplethresholdattheinter- Usingallavailableinputfeaturesyieldsanaccuracyof
sectionofthetwohistogramsinFig. 8toachievesimilar 90%, 84%, 86%, 91%, 90%, 89%forlogisticregression,
accuracy (e.g., 81%; not shown). The next logical step naÃ¯veBayes,decisiontree,randomforest,gradientboosted
with the classification task would be to use all available treesandsupportvectormachinesrespectively. Beyondthe
relativelygoodaccuracy,theROCcurvesareshowninFig.
features. One important thing to mention at this step is
10a. Thistimetherearegenerallytwosetsofcurves,one
thatitisgoodpracticetonormalizeinputfeatures. Some
better performing group (logistic regression, random for-
of the ML methods (e.g., random forest) can handle in-
est, gradient boosted trees and support vector machines)
puts of different magnitudes (e.g., CAPE is on the order
with AUCs of 0.97 and a worse performing group (naÃ¯ve
of100sto1000s,butLiftedIndexisontheorderof1sto
Bayes and decision tree) AUCs around 0.87. This sep-
10s),butothers(e.g.,logisticregression)willbeuninten-
aration coincides with the flexibility of the classification
tionallybiasedtowardslargermagnitudefeaturesifyoudo methods. The better performing groups are better set to
not scale your input features. Common scaling methods dealwithmanyfeaturesandnon-linearinteractionsofthe
include min-max scaling and scaling your input features features, while the worse performing group is a bit more
tohavemean0andstandarddeviationof1(i.e.,standard restrictedinhowitcombinesmanyfeatures. Considering
12
a) b)
Fig.10.AsinFigure9,butnowtrainedwithallavailablepredictors.TheannotationsfromFig.9havebeenremoved.
the performance diagram (Fig. 10b), the same grouping sivelypermutesfeatures. Inotherwords,featuresaresuc-
of high AUC performing models have higher CSI scores cessivelypermutedintheorderthattheyweredetermined
(> 0.8) and have little to no frequency bias. Meanwhile as important (most important then second most impor-
thelowerAUCperformingmodelshavelowerCSI(0.75) tant etc) from the single pass but are now left shuffled.
andNBhasaslightoverforecastingbias. Overall,theML The specific name for the method we have been describ-
performanceonclassifyingifanimagehasathunderstorm ing is the backward multi-pass permutation importance.
isdoingwellwithallpredictors. Whileagoodperforming The backward name comes from the direction of shuf-
modelisadesiredoutcomeofML,atthispointwedonot fling, starting will all variables unshuffled and shuffling
knowhowtheMLismakingitspredictions. Thisispart more and more of them. There is the opposite direction,
oftheâ€™black-boxâ€™issueofMLanddoesnotlenditselfto namedforwardmulti-passpermutationimportance,where
beingconsistentwiththeMLuserâ€™spriorknowledge(see the starting point is that all features are shuffled to start.
noteinintroductiononconsistency;Murphy1993). Theneachfeatureisunshuffledinorderoftheirimportance
In order to alleviate some of opaqueness of the ML from the single-pass permutation importance. For visual
black-box, one can interrogate the trained ML models by learners, see the animations (for the backward direction;
asking: "What input features are most important to the Fig. ES4andFig. ES5)inthesupplementofMcGovern
decision?" and "Are the patterns the ML models learned etal.(2019). Thereasonfordoingmulti-passpermutation
physical(e.g., followmeteorologicalexpectation)?". The importance is because correlated features could result in
techniquesnamedpermutationimportance(Breiman2001; falselyidentifyingnon-importantvariablesusingthesingle
Lakshmanan et al. 2015) and accumulated local effects passpermutationimportance. Thebestanalysisoftheper-
(ALE; Apley and Zhu 2020) are used to answer these mutationtestistouseboththesinglepassandmulti-pass
two questions respectively. Permutation importance is a testsinconjunction.
method in which the relative importance of a input fea- Thetopfivemostimportantfeaturesforthebetterper-
tureisquantifiedbyconsideringthechangeinevaluation formingmodels(i.e.,logisticregression,randomforestand
metric (e.g., AUC) when that input variable is shuffled gradientboostedtrees)asdeterminedbypermutationim-
(i.e.,randomized). Theintuitionisthatthemostimportant portanceareshowninFig. 11. ForallMLmethodsboth
variables when shuffled will cause the largest change to thesingleandmulti-passtestshowthatthemaximumverti-
theevaluationmetric. Therearetwomainflavorsofper- callyintegratedliquidisthemostimportantfeature,while
mutation importance, named single-pass and multi-pass. the minimum brightness temperature from the clean in-
Single-passpermutationimportancegoesthrougheachin- fraredandmidtroposphericwatervaporchannelsarefound
putvariableandshufflesthemonebyone,calculatingthe within the top 5 predictors (except multi-pass test for lo-
changeintheevaluationmetrics. Multi-passpermutation gisticregression). Ingeneral,thewaytointerprettheseare
importanceusestheresultofthesingle-pass,butprogres- to take the consensus over all models which features are
13
Fig.11. BackwardpermutationimportancetestforthebestperformingclassificationMLmodels. Singlepassresultsareinthetoprow,while
multi-passforwardresultsareforthebottomrow.EachcolumncorrespondstoadifferentMLmethod:logisticregression(a,d),randomforest(b,e)
andgradientboostedtrees(c,f).Barsarecoloredbytheirsource,yellowfortheverticallyintegratedliquid(VIL),redfortheinfrared(IR),bluefor
watervapor(WV)andblackforvisible(VIS).Numbersubscriptscorrespondtothepercentileofthatvariable.Thedashedblacklineistheoriginal
AUCvaluewhenallfeaturesarenotshuffled.
important. Atthispointittimetoconsiderifthemostim- In the latter case, it is possible that your model might be
portant predictors make meteorological sense. Vertically gettingtherightanswerforthewrongreasons.
integratedliquidhasbeenshowntohavearelationshipto Meanwhileminimumbrightnesstemperatureatboththe
lightning (e.g., Watson et al. 1995) and is thus plausible watervaporandcleaninfraredchannelsalsomakephysi-
to be the most important predictor. Similarly, the mini- cal sense since lower temperatures are related with taller
storms. Wecouldalsoreconcilethemaxinfraredbright-
mumbrightnesstemperatureatthewatervaporandclean
ness temperature as a proxy for the surface temperature,
infraredchannelsalsomakesphysicalsensebecauselower
which correlates to buoyancy, but not that the relative
temperatures are generally associated with taller storms.
change in AUC with this feature is quite small. If any
Wecouldalsoreconcilethemaximuminfraredbrightness
thetoppredictorsdonâ€™tmakesensemeteorologically,then
temperature(Fig11a)asaproxyforthesurfacetempera-
yourmodelmightbegettingtherightanswerforthewrong
turewhichcorrelatestobuoyancy,butnotethattherelative
reasons.
changeinAUCwiththisfeatureisquitesmall. Conversely,
Accumulated local effects are where small changes to
any important predictors that donâ€™t align with traditional
inputfeaturesandtheirassociatedchangeontheoutputof
meteorologicalknowledgemayrequirefurtherexploration
the model are quantified. The goal behind ALE is to in-
todeterminewhythemodelisplacingsuchweightonthose vestigatetherelationshipbetweenaninputfeatureandthe
variables. Doesthepredictorhavesomestatisticalcorre- output. ALEisperformedbybinningthedatabasedonthe
lationwiththemeteorologicaleventthatisunexplainedby featureofinterest. Thenforeachexampleineachbin,the
past literature, or are there nonphysical characteristics of featurevalueisreplacedbytheedgesofthebin. Themean
thedatathatmaybeinfluencingthemodelduringtraining? difference in the model output from the replaced feature
14
Fig.12.Accumulatedlocaleffects(ALE)for(a)themaximumVerticallyIntegratedLiquid(VILmax),(b)theminimumbrightnesstemperature
frominfrared(IRmin)and(c)theminimumbrightnesstemperaturefromthewatervaporchannel(WVmin).LinescorrespondtoalltheMLmethods
trained(exceptsupportvectormachines)andcolorsmatchFig.9.Greyhistogramsinthebackgroundarethecountsofpointsineachbin.
valueisthenusedastheALEforthatbin. Thisprocessis
repeatedforallbinswhichresultinacurve. Forexample,
theALEforsomeofthetoppredictorsofthepermutation
testareshownininFig. 12. Atthisstep,theALEscanbe
mainlyused toseeif theMLmodels havelearnedphysi-
callyplausibletrendswithinputfeatures. Forthevertically
integratedliquid,allmodelsshowthatasthemaxvertically
integratedliquidincreasesfromabout2ð‘˜ð‘”ð‘š2to30ð‘˜ð‘”ð‘š2
theaverageoutputprobabilityofthemodelwillincrease,
butvalueslargerthan30ð‘˜ð‘”ð‘š2generallyallhavethesame
localeffectontheprediction(Fig. 12a). Asforthemini-
mumcleaninfraredbrightnesstemperature,themagnitude
oftheaveragechangeisconsiderablydifferentacrossthe
different models, but generally all have the same pattern.
As the minimum temperature increases from -88â—¦C to -
55â—¦C,themeanoutputprobabilitydecreases: temperatures
largerthan-17â—¦Chavenochange(Fig. 12b). Lastly, all
butthelogisticregressionshowsasimilarpatternwiththe
minimum water vapor brightness temperature, but notice
themagnitudeofthey-axis(Fig. 12c). Muchlesschange
occurswiththisfeature. Forinterestedreaders,additional Fig.13.Thetrainingdatarelationshipbetweentheminimumbright-
interpretation techniques and examples can be found in
nesstemperaturefrominfrared(ð‘‡ ð‘)andthenumberofflashesdetected
byGLM.Allnon-thunderstormimages(numberofflashesequalto0)
Molnar(2022).
areinblack.
2) Regression
AsstatedinSection3.a,task(2)istopredictthenumber
dataset; black points in Fig. 13), the linear methods will
oflightningflashesinsideanimage. Thus,theregression
likelystruggletocaptureaskillfulprediction. Onewayto
methods available to do this task are: linear regression,
improve performance would be to only predict the num-
decision tree, random forest, gradient boosted trees and
ber of flashes on images where there is non-zero flashes.
supportvectormachines. Similartotask(1)asimplesce-
nario is considered first, using ð‘‡ ð‘ as the lone predictor. Whilethismightnotseemlikeaviablewayforwardsince
Figure 13 shows the general relationship betweenð‘‡ ð‘ and non-lightningcaseswouldbeusefultopredict,inpractice
thenumberofflashesintheimage. Forð‘‡ ð‘ >-25â—¦ð¶,most wecouldleveragetheverygoodperformanceoftheclassi-
imagesdonothaveanylightning,whileð‘‡ ð‘<-25â—¦ð¶shows ficationmodelfromSection3.c.1,andthenusethetrained
ageneralincreaseoflightningflashes. Giventherearealot regressiononimagesthatareconfidenttohaveatleastone
of images with 0 flashes (approximately 50% of the total flashinthem. Anexampleofthisdoneintheliteratureis
15
Gagneetal.(2017)wherehailsizepredictionswereonly andthetruenumberofflashesintheone-to-oneplot(Fig.
madeiftheclassificationmodelsaidtherewashail. 16). Meanwhilethescatterforrandomforestandgradient
Asbefore,allmethodsarefitonthetrainingdatainitially boostedtreeshasreducedconsiderablywhencomparingto
using the default hyperparameters. A common way to thesingleinputmodels(Fig. 16c,d). Whilecomparingthe
compareregressionmodelperformanceistocreateaone- biasofthemodelstrainedwithallpredictorsisrelatively
to-oneplot,whichhasthepredictednumberofflasheson similar, the other metrics are much improved, showing
the x-axis and the true measured number of flashes on largereductionsinMAE,RMSEandincreasesinð‘…2(Fig.
the y-axis. A perfect model will show all points tightly 17)forallmethodsexceptdecisiontrees. Thisreinforces
centered along the diagonal of the plot. This is often that fact that similar to the classification example, it is
the quickest qualitative assessment of how a regression alwaysgoodtocomparemorethanonemetric.
model is performing. While ð‘‡ ð‘ was well suited for the SincetheinitialfittingoftheMLmodelsusedthedefault
classificationofthunderstorm/no-thunderstorm,itisclear parameters,theremightberoomfortuningthemodelsto
that fitting a linear model to the data in Fig. 13 did not have better performance. Here we will show an example
dowell(Fig. 14a,e),leadingtoastrongover-predictionof of some hyperparameter tuning of a random forest. The
thenumberoflightningflashesinanimageswithlessthan commonparametersthatcanbealteredinarandomforest
100flashes,whileunder-predictingthenumberofflashes include, the maximum depth of the trees (i.e., number of
for images with more than 100 flashes. The tree based decisions in a tree) and the number of trees in the forest.
methodstendtodobetter,butthereisstillalargeamount Theformalhyperparametersearchwillusethefulltraining
ofscatterandanoverestimationofstormswithlessthan dataset,andsystematicallyvarythedepthofthetreesfrom
100flashes. 1to10(inincrementsof1)aswellasthenumberoftrees
In order to tie quantitative metrics to the performance from1to100(1,5,10,25,50,100). Thisresultsin60total
of each model the following are common metrics calcu- modelsthataretrained.
lated: Mean Bias, Mean Absolute Error (MAE), Root Inordertoevaluatewhichisthebestconfiguration,the
Mean Squared Error (RMSE) and coefficient of determi- samemetricsasbeforeareshowninFig. 18asafunctionof
nation (ð‘…2). Their mathematical representations are the thedepthofthetrees. Therandomforestquicklygainsskill
following: withaddeddepthbeyondone,withallmetricsimproving
forboththetraining(dashedlines)andvalidationdatasets
ð‘
1 âˆ‘ï¸ (solidlines). Beyondadepthoffour, thebias, MAEand
Bias= ð‘ (ð‘¦ ð‘—âˆ’ð‘¦Ë†ð‘—), (21) RMSEallstagnate,buttheð‘…2valueincreasesuntiladepth
ð‘—=1
ofeightwherethetrainingdatacontinuetoincrease. There
ð‘ doesnotseemtobethatlargeofaneffectofincreasingthe
1 âˆ‘ï¸
MAE=
ð‘
|ð‘¦ ð‘—âˆ’ð‘¦Ë†ð‘—| (22) number of trees beyond 10 (color change of lines). The
ð‘—=1 characteristic of increasing training metric skills but no
increase(oradecrease)tovalidationdataskillistheover-
(cid:118)(cid:117)(cid:116)
ð‘
1 âˆ‘ï¸ fitting signal we discussed in Section 3.b. Thus, the best
RMSE=
ð‘
(ð‘¦ ð‘—âˆ’ð‘¦Ë†ð‘—)2 (23)
randomforestmodelchoiceforpredictinglightningflashes
ð‘—=1
is a random forest with a max depth of eight and a total
R2=1âˆ’
(cid:205)ð‘
ð‘—=1
(ð‘¦ ð‘—âˆ’ð‘¦Ë†ð‘—)2
(24)
o
ge
f
n
1
e
0
ra
t
l
re
c
e
h
s
o
.
o
T
si
h
n
e
g
r
a
ea
s
s
i
o
m
n
p
w
le
e
rm
ch
o
o
d
o
e
s
l
e
is
10
le
t
s
r
s
ee
c
s
o
,
m
is
pu
b
t
e
a
c
t
a
io
u
n
se
al
i
l
n
y
(cid:205)ð‘
ð‘—=1
(ð‘¦ ð‘—âˆ’ð‘¦Â¯)2
expensive to use as well as a more interpretable than a
modelwith1000trees.
All of these metrics are shown in Fig. 15. In general,
themetricsgiveamorequantitativeperspectivetotheone-
to-oneplots. Thepoorperformanceofthelinearmethods
d. Testing
shows,withthetwoworstperformancesbeingthesupport
vectormachinesandlinearregressionwithbiasesof71and Asmentionedbefore,thetestdatasetisthedatasetyou
6flashesrespectively. Whilenomethodprovidesremark- holdoutuntiltheendwhenallhyperparametertuninghas
ableperformance,therandomforestandgradientboosted finished so that there is no unintentional tuning of the
trees perform better with this single feature model (show finalmodelconfigurationtoadataset. Thus,nowthatwe
bettermetricsholistically). have evaluated the performance of all our models on the
As before, the next logical step is to use all available validationdatasetitistimetorunthesameevaluationsas
features to predict the number of flashes: those results in Section 3.c.1 and Section 3.c.2. These test results are
are found in Fig. 16 and 17. As expected, the model theendperformancemetricsthatshouldbeinterpretedas
performance increases. Now all models show a general the expected ML performance on new data (e.g., the ML
correspondence between the predicted number of flashes appliedinpractice). FortheMLmodelsherethemetrics
16
Fig.14.Theone-to-onerelationshipbetweenthepredictednumberoflightningflashesfromtheMLlearningmodelstrainedononlyð‘‡ ð‘(x-axis;
ð‘¦Ë†)andthenumberofmeasureflashesfromGLM(y-axis;ð‘¦).Eachmarkerisoneobservation.Meanwhileareaswithmorethan100pointsinclose
proximityareshowninthecoloredboxes. Thelightertheshadeofthecolor,thehigherdensityofpoints. (a)linearregression(LnR;reds),(b)
decisiontree(DT;blues),(c)randomforest(RF;oranges),(d)gradientboostedtrees(GBT;purples)and(e)linearsupportvectormachines(SVM;
greys).
areverysimilarasthevalidationset. Forbrevitytheextra Additionally,thismanuscriptprovidedatutorialexam-
figuresareincludedintheappendix(FigA1-A3). pleofhowtoapplyMLtoacouplemeteorologicaltasks
usingtheStormEVentImageRydataset(SEVIR;Veillette
etal.2020)dataset. We:
4. SummaryandFutureWork
1. Discussed the various steps of preparing data for
ThismanuscriptwasthefirstoftwoMachineLearning
ML (i.e., removing artifacts; engineering features,
(ML) tutorial papers designed for the operational meteo-
train/val/testsplits;Section3.b)
rologycommunity. Thispapersuppliedasurveyofsome
of the most common ML methods. All ML methods de- 2. Conducted a classification task to predict if satellite
scribedhereareconsideredsupervisedmethods,meaning images had lightning within them. This section in-
the data the models are trained from include pre-labeled cluded discussions of training, evaluation and inter-
truth data. The specific methods covered included lin- rogationofthetrainedMLmodels(Section3.c.1)
earregression,logisticregression,decisiontrees,random
forests, gradient boosted decision trees, naÃ¯ve Bayes and 3. Exhibitedaregressiontasktopredictthenumberof
supportvectormachines. Theoverarchinggoalofthepaper lightningflashesinasatelliteimage. Thissectionalso
wastointroducetheMLmethodsinsuchawaythatML containeddiscussionsoftraining/evaluationaswellas
methods are more familiar to readers as they encounter anexampleofhyperparametertuning(Section3.c.2)
them in the operational community and within the gen-
4. Releasedpythoncodetoconductallstepsandexam-
eral meteorological literature. Moreover, this manuscript
ples in this manuscript (see Data Availability State-
providedamplereferencesofpublishedmeteorologicalex-
ment)
amplesaswellasopen-sourcecodetoactascatalystsfor
readerstoadaptandtryMLontheirowndatasetsandin The follow on paper in this series will discuss a more
theirworkflows. complex, yet potentially more powerful, grouping of ML
17
Fig.15. ValidationdatasetmetricsforallMLmodels. Colorsare
thesameasinFig.14.Exactnumericalvalueisreportedontopofeach
bar.
methods: neuralnetworksanddeeplearning. Likealotof
theMLmethodsdescribedinthispaper, neuralnetworks
arenâ€™t necessarily new (Rumelhart et al. 1986) and were
firstappliedtometeorologytopicsdecadesago(e.g.,Key
et al. 1989; Lee et al. 1990). Although, given the expo-
nential growth of computing resources and dataset sizes,
researchusingneuralnetworksanddeeplearninginmete-
orology has been accelerating (e.g., Fig 1c; Gagne et al.
2019; Lagerquist et al. 2020; Cintineo et al. 2020; Chase
et al. 2021; Hilburn et al. 2021; Lagerquist et al. 2021;
Molina et al. 2021; Ravuri et al. 2021). Thus, it is im-
portantthatoperationalmeteorologistsalsounderstandthe
basicsofneuralnetworksanddeeplearning.
18
Fig.16.AsinFig.14,butnowthex-axisisprovidedfromtheMLmodelstrainedwithallavailableinputfeatures.
Fig.18.Hyperparametertuningofarandomforestforpredictingthe
numberoflightningflashes. Allinputfeaturesareused.Solidlinesare
thevalidationdatasetwhilethedashedlinesarethetrainingdata. The
Fig.17. AsinFig. 15,butforMLmodelstrainedwithallavailable verticaldottedlineisthedepthoftreeswhereover-fittingbegins.
inputfeatures.
19
Acknowledgments. This material is based upon work
supportedbytheNationalScienceFoundationunderGrant
No. ICER-2019758, supporting authors RJC, AM and
AB.AuthorDRHwasprovidedsupportbyNOAA/Office
of Oceanic and Atmospheric Research under NOAA-
UniversityofOklahomaCooperativeAgreementsnumber
NA16OAR4320115andnumberNA21OAR4320204,U.S.
DepartmentofCommerce. Thescientificresultsandcon-
clusions,aswellasanyviewsoropinionsexpressedherein,
arethoseoftheauthorsanddonotnecessarilyreflectthe
viewsofNOAAortheDepartmentofCommerce.
We want to acknowledge the work put forth by the
authors of the SEVIR dataset (Mark S. Veillette, Sid-
dharth Samsi and Christopher J. Mattioli) for making a
high-qualityfreedataset. Wewouldalsoliketoacknowl-
edgetheopen-sourcepythoncommunityforprovidingtheir
toolsforfree. Specifically,weacknowledgeGoogleColab
(Bisong 2019), Anaconda (Anaconda 2020), scikit-learn
(Pedregosa et al. 2011), Pandas (Wes McKinney 2010),
Numpy (Harris et al. 2020) and Jupyter (Kluyver et al.
2016).
Data availability statement. As an effort to catalyse
theuseandtrustofmachinelearningwithinmeteorology
wehavesuppliedagithubrepositorywithacodetutorialof
alotofthesamethingsdiscussedinthispaper. Thelatest
versionofgithubrepositorycanbelocatedhere: https:
//github.com/ai2es/WAF_ML_Tutorial_Part1. If
you are interested in the version of the repository that
was available at time of publication please see the zendo
archive of version 1 here: URL. The original github
repoforSEVIRislocatedhere: https://github.com/
MIT-AI-Accelerator/neurips-2020-sevir.
APPENDIX
Testingdatasetfigures
This appendix contains the test dataset evaluations for
both the classification task (Fig. A1) and the regression
task (Fig. A2-A3). Results are largely the same as the
validationset,sotosavespacetheywereincludedhere.
20
a) b)
Fig.A1.AsinFigure9,butnowforthetestdataset
21
Fig.A2.AsinFig.14,butforthetestdataset
22
Fig.A3.AsinFig.15,butforthetestdataset
23
References Cintineo, J., andCoauthors, 2018: Anempiricalmodelforassessing
thesevereweatherpotentialofdevelopingconvection.Weatherand
Anaconda,2020:Anacondasoftwaredistribution.AnacondaInc.,URL
Forecasting,33(1),331â€“345.
https://docs.anaconda.com/.
Cintineo,J.L.,M.J.Pavolonis,J.M.Sieglaff,L.Cronce,andJ.Brun-
Apley, D. W., and J. Zhu, 2020: Visualizing the effects of
ner,2020: Noaaprobseverev2.0â€”probhail,probwind,andprobtor.
predictor variables in black box supervised learning models.
Weather and Forecasting, 35 (4), 1523 â€“ 1543, https://doi.org/10.
Journal of the Royal Statistical Society: Series B (Statistical
1175/WAF-D-19-0242.1, URL https://journals.ametsoc.org/view/
Methodology),82(4),1059â€“1086,https://doi.org/https://doi.org/10.
journals/wefo/35/4/wafD190242.xml.
1111/rssb.12377, URL https://rss.onlinelibrary.wiley.com/doi/abs/
10.1111/rssb.12377, https://rss.onlinelibrary.wiley.com/doi/pdf/10. Conrick,R.,J.P.Zagrodnik,andC.F.Mass,2020: Dual-polarization
1111/rssb.12377. radarretrievalsofcoastalpacificnorthwestraindropsizedistribution
parametersusingrandomforestregression.JournalofAtmospheric
Bieli,M.,A.H.Sobel,S.J.Camargo,andM.K.Tippett,2020:Astatis-
and Oceanic Technology, 37 (2), 229 â€“ 242, https://doi.org/10.
ticalmodeltopredicttheextratropicaltransitionoftropicalcyclones.
1175/JTECH-D-19-0107.1,URLhttps://journals.ametsoc.org/view/
Weather and Forecasting, 35 (2), 451 â€“ 466, https://doi.org/10.
journals/atot/37/2/jtech-d-19-0107.1.xml.
1175/WAF-D-19-0045.1, URL https://journals.ametsoc.org/view/
journals/wefo/35/2/waf-d-19-0045.1.xml. Cui, W., X. Dong, B. Xi, and Z. Feng, 2021: Clima-
tology of linear mesoscale convective system morphology in
Billet, J., M. DeLisi, B. Smith, and C. Gates, 1997: Use of regres-
the united states based on the random-forests method. Jour-
siontechniquestopredicthailsizeandtheprobabilityoflargehail.
nal of Climate, 34 (17), 7257 â€“ 7276, https://doi.org/10.
WeatherandForecasting,12(1),154â€“164.
1175/JCLI-D-20-0862.1, URL https://journals.ametsoc.org/view/
Bischl, B., O. Mersmann, H. Trautmann, and C. Weihs, 2012: journals/clim/34/17/JCLI-D-20-0862.1.xml.
Resampling methods for meta-model validation with recommen-
Czernecki, B., M. Taszarek, M. Marosz, M. PÃ³Å‚rolniczak, L. Kolen-
dations for evolutionary computation. Evol. Comput., 20 (2),
dowicz,A.Wyszogrodzki,andJ.Szturc,2019: Applicationofma-
249â€“275,https://doi.org/10.1162/EVCO_a_00069,URLhttps://doi.
chinelearningtolargehailprediction-theimportanceofradarre-
org/10.1162/EVCO_a_00069.
flectivity, lightning occurrence and convective parameters derived
Bisong,E.,2019: GoogleColaboratory,59â€“64.Apress,Berkeley,CA, from era5. Atmospheric Research, 227, 249â€“262, https://doi.org/
https://doi.org/10.1007/978-1-4842-4470-8_7,URLhttps://doi.org/ https://doi.org/10.1016/j.atmosres.2019.05.010, URL https://www.
10.1007/978-1-4842-4470-8_7. sciencedirect.com/science/article/pii/S0169809519300900.
Bonavita, M., and Coauthors, 2021: Machine learning for earth Elmore, K. L., and H. Grams, 2016: Using mping data to generate
system observation and prediction. Bulletin of the American Me- randomforestsforprecipitationtypeforecasts.14thConferenceon
teorological Society, 102 (4), E710 â€“ E716, https://doi.org/10. ArtificialandComputationalIntelligenceanditsApplicationstothe
1175/BAMS-D-20-0307.1,URLhttps://journals.ametsoc.org/view/ EnvironmentalSciences,4.2.
journals/bams/102/4/BAMS-D-20-0307.1.xml.
Flora, M. L., C. K. Potvin, P. S. Skinner, S. Handler, and A. Mc-
Breiman,L.,1984:ClassificationandRegressionTrees.Routledge,New Govern, 2021: Using machine learning to generate storm-scale
York,NewYork. probabilistic guidance of severe weather hazards in the warn-on-
forecast system. Monthly Weather Review, 149 (5), 1535 â€“ 1557,
Breiman,L.,2001:Randomforests.MachineLearning,45,5â€“32,URL https://doi.org/10.1175/MWR-D-20-0194.1, URL https://journals.
https://doi.org/10.1023/A:1010933404324. ametsoc.org/view/journals/mwre/149/5/MWR-D-20-0194.1.xml.
Burke,A.,N.Snook,D.J.GagneII,S.McCorkle,andA.McGovern, Friedman,J.,2001:Greedyfunctionapproximation:agradientboosting
2020: Calibration of Machine Learningâ€“Based Probabilistic Hail machine.AnnalsofStatistics,29(5),1189â€“1232.
Predictions forOperational Forecasting. Weather andForecasting,
35(1),149â€“168,https://doi.org/10.1175/WAF-D-19-0105.1. Gagne,D.,H.Christensen,A.Subramanian,andA.Monahan,2019:
Machine learning for stochastic parameterization: Generative ad-
Cains, M. G., and Coauthors, 2022: Nws forecastersâ€™ perceptions versarial networks in the lorenz â€™96 model. Journal of Advances
and potential uses of trustworthy ai/ml for hazardous weather in Modeling Earth Systems, Conditionally Accepted, URL https:
risks, URL https://ams.confex.com/ams/102ANNUAL/meetingapp. //arxiv.org/abs/1909.04711.
cgi/Paper/393121, american meteorological society, 102 annual
meeting. Gagne,D.,A.McGovern,andJ.Brotzge,2009:Classificationofconvec-
tiveareasusingdecisiontrees.JournalofAtmosphericandOceanic
Chase, R. J., S. W. Nesbitt, and G. M. McFarquhar, 2021: A dual- Technology,26(7),1341â€“1353.
frequencyradarretrievaloftwoparametersofthesnowfallparticle
size distribution using a neural network. Journal of Applied Me- Gagne,D.,A.McGovern,J.Brotzge,andM.Xue,2013: Severehail
teorology and Climatology, 60 (3), 341 â€“ 359, https://doi.org/10. predictionwithinaspatiotemporalrelationaldataminingframework.
1175/JAMC-D-20-0177.1, URL https://journals.ametsoc.org/view/ InternationalConferenceonDataMining,Dallas,TX,Instituteof
journals/apme/60/3/JAMC-D-20-0177.1.xml. ElectricalandElectronicsEngineers.
Chisholm,D.,J.Ball,K.Veigas,andP.Luty,1968: Thediagnosisof Gagne,D.,A.McGovern,S.Haupt,R.Sobash,J.Williams,andM.Xue,
upper-levelhumidity.JournalofAppliedMeteorology,7(4),613â€“ 2017:Storm-basedprobabilistichailforecastingwithmachinelearn-
619. ing applied to convection-allowing ensembles. Weather and Fore-
casting,32(5),1819â€“1840.
Cintineo,J.,M.Pavolonis,J.Sieglaff,andD.Lindsey,2014:Anempir-
icalmodelforassessingthesevereweatherpotentialofdeveloping Gensini, V. A., C. Converse, W. S. Ashley, and M. Taszarek,
convection.WeatherandForecasting,29(3),639â€“653. 2021: Machine learning classification of significant tornadoes
24
and hail in the united states using era5 proximity soundings. https://doi.org/10.1175/WAF-D-19-0102.1, URL https://journals.
Weather and Forecasting, 36 (6), 2143 â€“ 2160, https://doi.org/10. ametsoc.org/view/journals/wefo/35/1/waf-d-19-0102.1.xml.
1175/WAF-D-21-0056.1, URL https://journals.ametsoc.org/view/
journals/wefo/36/6/WAF-D-21-0056.1.xml. Hubbert, J. C., M. Dixon, S. M. Ellis, and G. Meymaris,
2009: Weather radar ground clutter. part i: Identifica-
Glahn, H. R., and D. A. Lowry, 1972: The Use of Model Output tion, modeling, and simulation. Journal of Atmospheric and
Statistics(MOS)inObjectiveWeatherForecasting.J.Appl.Meteor., Oceanic Technology, 26 (7), 1165 â€“ 1180, https://doi.org/10.
11,1203â€“1211. 1175/2009JTECHA1159.1,URLhttps://journals.ametsoc.org/view/
journals/atot/26/7/2009jtecha1159_1.xml.
Goodfellow,I.,Y.Bengio,andA.Courville,2016:DeepLearning.MIT
Press,http://www.deeplearningbook.org. Jergensen, G. E., A. McGovern, R. Lagerquist, and T. Smith,
2020: Classifying convective storms using machine learning.
Grams, H. M., P.-E. Kirstetter, and J. J. Gourley, 2016: NaÃ¯ve Weather and Forecasting, 35 (2), 537 â€“ 559, https://doi.org/10.
bayesian precipitation type retrieval from satellite using a 1175/WAF-D-19-0170.1, URL https://journals.ametsoc.org/view/
cloud-top and ground-radar matched climatology. Journal of journals/wefo/35/2/waf-d-19-0170.1.xml.
Hydrometeorology, 17 (10), 2649 â€“ 2665, https://doi.org/10.
1175/JHM-D-16-0058.1, URL https://journals.ametsoc.org/view/ Kalnay, E., 2002: Atmospheric Modeling, Data Assimilation and
journals/hydr/17/10/jhm-d-16-0058_1.xml. Predictability.CambridgeUniversityPress,https://doi.org/10.1017/
CBO9780511802270.
Harris, C. R., and Coauthors, 2020: Array programming
with NumPy. Nature, 585 (7825), 357â€“362, https://doi.org/
Key,J.,J.Maslanik,andA.Schweiger,1989: Classificationofmerged
10.1038/s41586-020-2649-2, URL https://doi.org/10.1038/
AVHRRandSMMRArcticdatawithneuralnetworks.Photogram-
s41586-020-2649-2.
metricEngineeringandRemoteSensing,55(9),1331â€“1338.
Herman, G., and R. Schumacher, 2018a: Dendrology in numerical
Kluyver, T., and Coauthors, 2016: Jupyter notebooks â€“ a publish-
weatherprediction: Whatrandomforestsandlogisticregressiontell
ing format for reproducible computational workflows. Positioning
usaboutforecasting.Mon.Wea.Rev.,146,1785â€“1812,URLhttps:
andPowerinAcademicPublishing: Players,AgentsandAgendas,
//doi.org/10.1175/MWR-D-17-0307.1.
F.Loizides,andB.Schmidt,Eds.,IOSPress,87-90.
Herman,G.,andR.Schumacher,2018b:Moneydoesnâ€™tgrowontrees,
Kossin, J. P., and M. Sitkowski, 2009: An objective model for
but forecasts do: Forecasting extreme precipitation with random
identifying secondary eyewall formation in hurricanes. Monthly
forests. Mon. Wea. Rev., 146, 1571â€“1600, URL https://doi.org/10.
Weather Review, 137 (3), 876 â€“ 892, https://doi.org/10.1175/
1175/MWR-D-17-0250.1.
2008MWR2701.1,URLhttps://journals.ametsoc.org/view/journals/
mwre/137/3/2008mwr2701.1.xml.
Hilburn,K.A.,I.Ebert-Uphoff,andS.D.Miller,2021: Development
andinterpretationofaneural-network-basedsyntheticradarreflectiv-
Kuncheva, L. I., 2006: On the optimality of naÃ¯ve bayes
ityestimatorusinggoes-rsatelliteobservations.JournalofApplied
with dependent binary features. Pattern Recognition Letters,
Meteorology and Climatology, 60 (1), 3 â€“ 21, https://doi.org/10.
27 (7), 830â€“837, https://doi.org/https://doi.org/10.1016/j.patrec.
1175/JAMC-D-20-0084.1, URL https://journals.ametsoc.org/view/
2005.12.001, URL https://www.sciencedirect.com/science/article/
journals/apme/60/1/jamc-d-20-0084.1.xml.
pii/S0167865505003582.
Hill, A. J., G. R. Herman, and R. S. Schumacher, 2020: Forecast-
Kurdzo, J. M., E. F. Joback, P.-E. Kirstetter, and J. Y. N.
ingsevereweatherwithrandomforests.MonthlyWeatherReview,
Cho, 2020: Geospatial qpe accuracy dependence on weather
148 (5), 2135 â€“ 2161, https://doi.org/10.1175/MWR-D-19-0344.
radar network configurations. Journal of Applied Meteorology
1, URL https://journals.ametsoc.org/view/journals/mwre/148/5/
and Climatology, 59 (11), 1773 â€“ 1792, https://doi.org/10.
mwr-d-19-0344.1.xml.
1175/JAMC-D-19-0164.1, URL https://journals.ametsoc.org/view/
Hill,A.J.,andR.S.Schumacher,2021: Forecastingexcessiverainfall journals/apme/59/11/JAMC-D-19-0164.1.xml.
withrandomforestsandadeterministicconvection-allowingmodel.
Weather and Forecasting, 36 (5), 1693 â€“ 1711, https://doi.org/10. KÃ¼hnlein, M., T. Appelhans, B. Thies, and T. NauÃŸ, 2014: Pre-
1175/WAF-D-21-0026.1, URL https://journals.ametsoc.org/view/ cipitation estimates from msg seviri daytime, nighttime, and twi-
journals/wefo/36/5/WAF-D-21-0026.1.xml. light data with random forests. Journal of Applied Meteorol-
ogy and Climatology, 53 (11), 2457 â€“ 2480, https://doi.org/10.
Hoerl, A. E., and R. W. Kennard, 1970: Ridge regres- 1175/JAMC-D-14-0082.1, URL https://journals.ametsoc.org/view/
sion: Biased estimation for nonorthogonal problems. Techno- journals/apme/53/11/jamc-d-14-0082.1.xml.
metrics, 12 (1), 55â€“67, https://doi.org/10.1080/00401706.1970.
10488634, URL https://www.tandfonline.com/doi/abs/10.1080/ Lackmann,G.,2011:NumericalWeatherPrediction/DataAssimilation,
00401706.1970.10488634,https://www.tandfonline.com/doi/pdf/10. 274â€“287.AmericanMeteorologicalSociety.
1080/00401706.1970.10488634.
Lagerquist,R.,A.McGovern,C.R.Homeyer,D.J.G.II,andT.Smith,
Holte, R.C., 1993: Verysimpleclassificationrulesperformwellon 2020: Deeplearningonthree-dimensionalmultiscaledatafornext-
most commonly used datasets. Machine Learning, 11 (1), 63â€“90, hour tornado prediction. Monthly Weather Review, 148 (7), 2837
https://doi.org/10.1023/A:1022631118932, URL https://doi.org/10. â€“ 2861, https://doi.org/10.1175/MWR-D-19-0372.1, URL https://
1023/A:1022631118932. journals.ametsoc.org/view/journals/mwre/148/7/mwrD190372.xml.
Hu,L.,E.A.Ritchie,andJ.S.Tyo,2020: Short-termtropicalcyclone Lagerquist,R.,A.McGovern,andT.Smith,2017: Machinelearning
intensityforecastingfromsatelliteimagerybasedonthedeviationan- forreal-timepredictionofdamagingstraight-lineconvectivewind.
glevariancetechnique.WeatherandForecasting,35(1),285â€“298, WeatherandForecasting,32(6),2175â€“2193.
25
Lagerquist, R., J. Q. Stewart, I. Ebert-Uphoff, and C. Kumler, 2021: McGovern,A.,D.Gagne,J.Basara,T.Hamill,andD.Margolin,2015:
Using deep learning to nowcast the spatial coverage of convec- Solarenergyprediction:Aninternationalcontesttoinitiateinterdis-
tion from himawari-8 satellite data. Monthly Weather Review, ciplinaryresearchoncompellingmeteorologicalproblems.Bulletin
149 (12), 3897 â€“ 3921, https://doi.org/10.1175/MWR-D-21-0096. oftheAmericanMeteorologicalSociety,96(8),1388â€“1395.
1, URL https://journals.ametsoc.org/view/journals/mwre/149/12/
MWR-D-21-0096.1.xml. McGovern,A.,D.Gagne,J.Williams,R.Brown,andJ.Basara,2014:
Enhancingunderstandingandimprovingpredictionofsevereweather
Lakshmanan, V., C. Karstens, J. Krause, K. Elmore, A. Ryzhkov, throughspatiotemporalrelationallearning.MachineLearning,95(1),
and S. Berkseth, 2015: Which polarimetric variables are impor- 27â€“50.
tantforweather/no-weatherdiscrimination?JournalofAtmospheric
McGovern, A., R. Lagerquist, D. Gagne, G. Jergensen, K. Elmore,
and Oceanic Technology, 32 (6), 1209 â€“ 1223, https://doi.org/
C.Homeyer,andT.Smith,2019: Makingtheblackboxmoretrans-
10.1175/JTECH-D-13-00205.1, URL https://journals.ametsoc.org/
parent: Understandingthephysicalimplicationsofmachinelearn-
view/journals/atot/32/6/jtech-d-13-00205_1.xml.
ing.BulletinoftheAmericanMeteorologicalSociety,earlyonline
release.
Lee, C.-Y., S. J. Camargo, F. Vitart, A. H. Sobel, J. Camp,
S. Wang, M. K. Tippett, and Q. Yang, 2020: Subseasonal pre- Mecikalski, J., J. Williams, C. Jewett, D. AhÄ³evych, A. LeRoy, and
dictionsoftropicalcycloneoccurrenceandaceinthes2sdataset. J. Walker, 2015: Probabilistic 0â€“1-h convective initiation now-
Weather and Forecasting, 35 (3), 921 â€“ 938, https://doi.org/10. caststhatcombinegeostationarysatelliteobservationsandnumerical
1175/WAF-D-19-0217.1, URL https://journals.ametsoc.org/view/ weatherpredictionmodeldata.JournalofAppliedMeteorologyand
journals/wefo/35/3/waf-d-19-0217.1.xml. Climatology,54(5),1039â€“1059.
Lee,J.,R.Weger,S.Sengupta,andR.Welch,1990: Aneuralnetwork Molina, M. J., D. J. Gagne, and A. F. Prein, 2021: A bench-
approachtocloudclassification.IEEETransactionsonGeoscience mark to test generalization capabilities of deep learning
and Remote Sensing, 28 (5), 846â€“855, https://doi.org/10.1109/36. methods to classify severe convective storms in a changing
58972. climate. Earth and Space Science, 8 (9), e2020EA001490,
https://doi.org/https://doi.org/10.1029/2020EA001490, URL https:
Li,L.,andCoauthors,2020:Acausalinferencemodelbasedonrandom //agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020EA001490,
foreststoidentifytheeffectofsoilmoistureonprecipitation.Jour- e2020EA001490 2020EA001490, https://agupubs.onlinelibrary.
nal of Hydrometeorology, 21 (5), 1115 â€“ 1131, https://doi.org/10. wiley.com/doi/pdf/10.1029/2020EA001490.
1175/JHM-D-19-0209.1, URL https://journals.ametsoc.org/view/
Molnar,C.,2022:InterpretableMachineLearning.2nded.,URLhttps:
journals/hydr/21/5/jhm-d-19-0209.1.xml.
//christophm.github.io/interpretable-ml-book.
Loken, E. D., A. J. Clark, and C. D. Karstens, 2020: Gener-
Murphy, A. H., 1993: What is a good forecast? an es-
ating Probabilistic Next-Day Severe Weather Forecasts from
say on the nature of goodness in weather forecasting.
Convection-Allowing Ensembles Using Random Forests. Weather
Weather and Forecasting, 8 (2), 281 â€“ 293, https://doi.org/
and Forecasting, https://doi.org/10.1175/WAF-D-19-0258.1,
10.1175/1520-0434(1993)008<0281:WIAGFA>2.0.CO;2, URL
URL https://doi.org/10.1175/WAF-D-19-0258.1, https://journals.
https://journals.ametsoc.org/view/journals/wefo/8/2/1520-0434_
ametsoc.org/waf/article-pdf/doi/10.1175/WAF-D-19-0258.1/
1993_008_0281_wiagfa_2_0_co_2.xml.
4951271/wafd190258.pdf.
MuÃ±oz-Esparza, D., R. D. Sharman, and W. Deierling, 2020: Avia-
Loken, E. D., A. J. Clark, and A. McGovern, 2022: Com- tion turbulence forecasting at upper levels with machine learning
paring and interpreting differently-designed random forests techniques based on regression trees. Journal of Applied Meteo-
for next-day severe weather hazard prediction. Weather rology and Climatology, 59 (11), 1883 â€“ 1899, https://doi.org/10.
and Forecasting, https://doi.org/10.1175/WAF-D-21-0138.1, 1175/JAMC-D-20-0116.1, URL https://journals.ametsoc.org/view/
URL https://journals.ametsoc.org/view/journals/wefo/aop/ journals/apme/59/11/JAMC-D-20-0116.1.xml.
WAF-D-21-0138.1/WAF-D-21-0138.1.xml.
Neetu,S.,M.Lengaigne,J.Vialard,M.Mangeas,C.Menkes,I.Suresh,
Malone, T., 1955: Applicationofstatisticalmethodsinweatherpre- J.Leloup,andJ.Knaff,2020: Quantifyingthebenefitsofnonlinear
diction.ProceedingsoftheNationalAcademyofSciences,41(11), methods for global statistical hindcasts of tropical cyclones inten-
806â€“815. sity.WeatherandForecasting,35(3),807â€“820,https://doi.org/10.
1175/WAF-D-19-0163.1, URL https://journals.ametsoc.org/view/
Mao, Y., and A. Sorteberg, 2020: Improving radar-based precipita- journals/wefo/35/3/waf-d-19-0163.1.xml.
tion nowcasts with machine learning using an approach based on
Nowotarski, C. J., and A. A. Jensen, 2013: Classifying proxim-
random forest. Weather and Forecasting, 35 (6), 2461 â€“ 2478,
ity soundings with self-organizing maps toward improving super-
https://doi.org/10.1175/WAF-D-20-0080.1, URL https://journals.
cellandtornadoforecasting.Wea.Forecasting,28,783â€“801,URL
ametsoc.org/view/journals/wefo/35/6/waf-d-20-0080.1.xml.
https://doi.org/10.1175/WAF-D-12-00125.1.
McCorkel, J., J. Van Naarden, D. Lindsey, B. Efremova, M. Coak- Pedregosa,F.,andCoauthors,2011: Scikit-learn: Machinelearningin
ley,M.Black,andA.Krimchansky,2019: Goes-17advancedbase- Python.JournalofMachineLearningResearch,12,2825â€“2830.
lineimagerperformancerecoverysummary.IGARSS2019-2019
IEEEInternationalGeoscienceandRemoteSensingSymposium,1â€“ Peter, J. R., A. Seed, and P. J. Steinle, 2013: Application
4,https://doi.org/10.1109/IGARSS40859.2019.9044466. of a bayesian classifier of anomalous propagation to single-
polarization radar reflectivity data. Journal of Atmospheric and
McGovern,A.,I.Ebert-Uphoff,D.J.G.I.au2,andA.Bostrom,2021: Oceanic Technology, 30 (9), 1985 â€“ 2005, https://doi.org/
Theneedforethical, responsible, andtrustworthyartificialintelli- 10.1175/JTECH-D-12-00082.1, URL https://journals.ametsoc.org/
genceforenvironmentalsciences.2112.08453. view/journals/atot/30/9/jtech-d-12-00082_1.xml.
26
Quinlan, J., 1993: C4.5: Programs for Machine Learning. Morgan Watson, A. I., R. L. Holle, and R. E. LÃ³pez, 1995: Lightning
Kaufmann,SanMateo,California. from two national detection networks related to vertically in-
tegrated liquid and echo-top information from wsr-88d radar.
Ravuri, S., and Coauthors, 2021: Skilful precipitation nowcasting Weather and Forecasting, 10 (3), 592 â€“ 605, https://doi.org/
using deep generative models of radar. Nature, 597 (7878), 672â€“ 10.1175/1520-0434(1995)010<0592:LFTNDN>2.0.CO;2, URL
677,https://doi.org/10.1038/s41586-021-03854-z,URLhttps://doi. https://journals.ametsoc.org/view/journals/wefo/10/3/1520-0434_
org/10.1038/s41586-021-03854-z. 1995_010_0592_lftndn_2_0_co_2.xml.
Roebber, P., 2009: Visualizingmultiplemeasuresofforecastquality. Wes McKinney, 2010: Data Structures for Statistical Computing in
WeatherandForecasting,24(2),601â€“608. Python.Proceedingsofthe9thPythoninScienceConference,StÃ©fan
van der Walt, and Jarrod Millman, Eds., 56 â€“ 61, https://doi.org/
Rumelhart, D. E., G. E. Hinton, and R. J. Williams, 1986: Learn- 10.25080/Majora-92bf1922-00a.
ingrepresentationsbyback-propagatingerrors.Nature,323(6088),
533â€“536,https://doi.org/10.1038/323533a0,URLhttps://doi.org/10. Williams,J.,2014: Usingrandomforeststodiagnoseaviationturbu-
1038/323533a0. lence.MachineLearning,95(1),51â€“70.
Schumacher,R.S.,A.J.Hill,M.Klein,J.A.Nelson,M.J.Erickson, Williams,J.,D.AhÄ³evych,S.Dettling,andM.Steiner,2008a: Com-
S. M. Trojniak, and G. R. Herman, 2021: From random forests biningobservationsandmodeldataforshort-termstormforecasting.
tofloodforecasts: Aresearchtooperationssuccessstory.Bulletin Remote Sensing Applications for Aviation Weather Hazard Detec-
oftheAmericanMeteorologicalSociety, 102(9), E1742â€“E1755, tionandDecisionSupport,SanDiego,CA,InternationalSocietyfor
https://doi.org/10.1175/BAMS-D-20-0186.1, URL https://journals. OpticsandPhotonics.
ametsoc.org/view/journals/bams/102/9/BAMS-D-20-0186.1.xml.
Williams,J.,R.Sharman,J.Craig,andG.Blackburn,2008b: Remote
Sessa, M. F., and R. J. Trapp, 2020: Observed relationship be- detectionanddiagnosisofthunderstormturbulence.RemoteSensing
tweentornadointensityandpretornadicmesocyclonecharacteristics. ApplicationsforAviationWeatherHazardDetectionandDecision
Weather and Forecasting, 35 (4), 1243 â€“ 1261, https://doi.org/10. Support,SanDiego,CA,InternationalSocietyforOpticsandPho-
1175/WAF-D-19-0099.1, URL https://journals.ametsoc.org/view/ tonics.
journals/wefo/35/4/wafD190099.xml.
Yang, L., H. Xu, and S. Yu, 2021: Estimating pm2.5 con-
Shield, S. A., and A. L. Houston, 2022: Diagnosing super- centrations in contiguous eastern coastal zone of china us-
cell environments: A machine learning approach. Weather ing modis aod and a two-stage random forest model. Jour-
and Forecasting, https://doi.org/10.1175/WAF-D-21-0098.1, nal of Atmospheric and Oceanic Technology, https://doi.org/10.
URL https://journals.ametsoc.org/view/journals/wefo/aop/ 1175/JTECH-D-20-0214.1,URLhttps://journals.ametsoc.org/view/
WAF-D-21-0098.1/WAF-D-21-0098.1.xml. journals/atot/aop/JTECH-D-20-0214.1/JTECH-D-20-0214.1.xml.
Taillardat,M.,A.-L.FougÃ¨res,P.Naveau,andO.Mestre,2019:Forest- Yoshida, S., T. Morimoto, T. Ushio, and Z. Kawasaki, 2009: A
basedandsemiparametricmethodsforthepostprocessingofrainfall fifth-powerrelationshipforlightningactivityfromtropicalrainfall
ensembleforecasting.WeatherandForecasting,34(3),617â€“634, measuring mission satellite observations. Journal of Geophysical
https://doi.org/10.1175/WAF-D-18-0149.1, URL https://journals. Research: Atmospheres, 114 (D9), https://doi.org/https://doi.org/
ametsoc.org/view/journals/wefo/34/3/waf-d-18-0149_1.xml. 10.1029/2008JD010370, URL https://agupubs.onlinelibrary.wiley.
com/doi/abs/10.1029/2008JD010370, https://agupubs.onlinelibrary.
Tibshirani,R.,1996: Regressionshrinkageandselectionviathelasso. wiley.com/doi/pdf/10.1029/2008JD010370.
JournaloftheRoyalStatisticalSociety.SeriesB(Methodological),
58(1),267â€“288,URLhttp://www.jstor.org/stable/2346178. Zhang, Z., D. Wang, J. Qiu, J. Zhu, and T. Wang, 2021: Machine
learningapproachesforimprovingnear-real-timeimergrainfallesti-
Vapnik,V.,1963:Patternrecognitionusinggeneralizedportraitmethod. matesbyintegratingcloudpropertiesfromnoaacdrpatmos-x.Jour-
AutomationandRemoteControl,24,774â€“780,URLhttps://ci.nii.ac. nalofHydrometeorology,22(10),2767â€“2781,https://doi.org/10.
jp/naid/10020952249/en/. 1175/JHM-D-21-0019.1, URL https://journals.ametsoc.org/view/
journals/hydr/22/10/JHM-D-21-0019.1.xml.
Veillette, M., S. Samsi, and C. Mattioli, 2020: Sevir : A storm
event imagery dataset for deep learning applications in radar Zou, H., and T. Hastie, 2005: Regularization and variable selection
and satellite meteorology. Advances in Neural Information Pro- viatheelasticnet.JournaloftheRoyalStatisticalSociety.SeriesB
cessing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. F. (StatisticalMethodology), 67(2), 301â€“320, URLhttp://www.jstor.
Balcan, and H. Lin, Eds., Curran Associates, Inc., Vol. 33, org/stable/3647580.
22009â€“22019,URLhttps://proceedings.neurips.cc/paper/2020/file/
fa78a16157fed00d7a80515818432169-Paper.pdf.
Vigaud,N.,M.K.Tippett,J.Yuan,A.W.Robertson,andN.Acharya,
2019:Probabilisticskillofsubseasonalsurfacetemperatureforecasts
overnorthamerica.WeatherandForecasting,34(6),1789â€“1806,
https://doi.org/10.1175/WAF-D-19-0117.1, URL https://journals.
ametsoc.org/view/journals/wefo/34/6/waf-d-19-0117_1.xml.
Wang,C.,P.Wang,D.Wang,J.Hou,andB.Xue,2020: Nowcasting
multicell short-term intense precipitation using graph models and
randomforests.MonthlyWeatherReview, 148(11), 4453â€“4466,
https://doi.org/10.1175/MWR-D-20-0050.1, URL https://journals.
ametsoc.org/view/journals/mwre/148/11/MWR-D-20-0050.1.xml.
