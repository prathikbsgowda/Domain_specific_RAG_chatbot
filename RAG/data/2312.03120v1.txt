The Landscape of Modern Machine Learning: A Review of
Machine, Distributed and Federated Learning
Omer Subasi1*, Oceane Bel1, Joseph Manzano1, Kevin Barker1
1*High Performance Computing Group, Pacific Northwest National Laboratory, 902
Battelle Blvd, Richland, 99354, WA, USA.
*Corresponding author(s). E-mail(s): omer.subasi@pnnl.gov;
Contributing authors: obel@pnnl.gov; joseph.manzano@pnnl.gov; kevin.barker@pnnl.gov;
Abstract
Withtheadvanceofthepowerfulheterogeneous,parallelanddistributedcomputingsystemsandever
increasing immense amount of data, machine learning has become an indispensable part of cutting-
edge technology, scientific research and consumer products. In this study, we present a review of
modernmachineanddeeplearning.Weprovideahigh-leveloverviewforthelatestadvancedmachine
learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed
learning, deep learning as well as federated learning. As a result, our work serves as an introductory
text to the vast field of modern machine learning.
Keywords:MachineLearning,DistributedMachineLearning,DeepLearning,FederatedLearning,Parallel
andDistributedComputing.
1 Introduction
provide data privacy [9]. Ever since its inception
[9], FL has been studied extensively and adapted
Overthelastdecade,MachineLearning(ML)has
widely [10, 11, 12, 13].
been applied to ever increasing immense amount
In this study, we review the current landscape
of data that is becoming available as more people
ofmodernMLsystemsandapplications,andoffer
becomedailyusersofinternet,mobileandwireless
an overview as a self-contained text. While there
networks.Coupledwiththesignificantadvancesin
are many surveys on large-scale [6, 8], distributed
deep learning (DL), ML has found more complex
ML [7], DL [1, 2, 14], and FL [10, 11, 12, 13],
applications: from medical to machine transla-
we instead provide a high-level joint view of mod-
tion and speech recognition, to intelligent object
ern parallel and distributed ML and FL. In this
recognition, and to smart cities [1, 2]. Modern
way,ourworkdifferentiatesitselffromtheexisting
parallel and heterogeneous computing systems
literature. In brief, our study
[3, 4, 5] have enabled such applications by sup-
porting highly parallel training. These large-scale • presents the concepts and methods of ML and
anddistributedsystemsthereforehavebecomethe DL.
backbone of modern ML [6, 7, 8]. • discussestheparallelismandscalingapproaches
FederatedLearning(FL),asasub-fieldofDL, of large-scale distributed ML. Moreover, it
has emerged as a distributed learning solution to explores the communication aspects, such as
1
3202
ceD
5
]GL.sc[
1v02130.2132:viXra
costs, topologies, and networking, of parallel ML algorithms. Finally, we discuss the existing
and distributed training and inference. modern ML frameworks.
• introduces FL, its applications and aggregation
methods.Itthenelaboratesonthesecurityand
privacyaspectsaswellastheexistingplatforms
and datasets.
• summarizesopenresearchquestionsinthemod- 3.1 Introduction to ML
ern landscape of parallel and distributed ML, MListheprocessoflearningfromdatatoperform
DL and FL. complex tasks for which there is no known deter-
ministicandalgorithmicsolution,orbuildingsuch
Figure 1 outlines and summarizes our study.
asolutionisnotpractical.Forinstance,developing
Our study is organized as follows: Section 2
adeterministicalgorithmbasedonrulestodetect
overviews the related work on large-scale and
spam emails is highly impractical. It is not possi-
distributed ML. Section 3 provides the back-
bletoknowtheexactlistofthedetectionrules.In
ground on ML. Section 4 discusses distributed
addition, these rules most often change over time.
ML. Section 5 presents FL. Section 6 summarizes
Since the list of the rules may be ever-increasing
the existing open challenges. Finally, Section 7
and even contradictory, the maintenance of such
concludes our review.
algorithms would require constant labor.
The ML process is mainly two-fold: Training
2 Related Work
and prediction (inference). In the training phase,
the parameters of a learning model are optimized
Surveys pertaining to parallel, distributed and
basedondata.Inthepredictionphase,thetrained
largescaleMLhavebeenverynumerousinthelit-
model is deployed to perform predictions on new
erature [6, 7, 8]. Our work is different and unique
data.Whileinmostcasesthetrainingandpredic-
because it provides an introductory review of the
tionphasesaremutuallyexclusive,inincremental
latest joint landscape of ML, DL and FL.
learning cases, they are coupled together. The
Different than the general surveys such as
modelsinthesecasesarecontinuouslytrainedand
[15, 6, 7, 8], some surveys offer in depth cost
make predictions. Figure 2 visualizes the training
and comparisons of algorithms and methods both
and prediction phases.
theoretically and empirically [16, 17].
ThemaingoalofMListogeneralizesuchthat
Many studies focus on distributed DL. Some
it performs well with unseen data. However, this
of them are [1, 2, 18, 19, 14, 20]. Moreover, there
goalcontradictsitsoptimizationgoalinwhichML
existsasignificantnumberofsurveysthatfocuson
tries to minimize the training loss with the train-
specifictypesofmodelssuchas[21]forgraphneu-
ingdata.Asaresult,thewell-knownbias-variance
ral networks (GNNs), [22] for Internet-of-Things
problem emerges. If an ML model over-fits the
(IoTs), [23] for wireless networks, [24] for mobile
trainingdata,thatis,havinghighvariance,itper-
and 5G networks or for specific target environ-
forms poorly with the unseen data. On the other
ments such as [25] for unmanned aerial vehicles
hand, if the model under-fits, that is, having high
(UAV).
bias,itdoesnotlearnimportantpatternsorregu-
FL literature unsurprisingly offers many sur-
laritiesinthedata.Over-fittingtypicallyhappens
veys. Some of the latest surveys are [10, 11, 12].
when a model is too complex for the underly-
Among studies having specific topics, [26] sur-
ing problem. In contrast, under-fitting happens
veys privacy and security methods for FL, [27]
whenthemodelistoosimple.Figure3depictsthe
discusses block chain-based FL. [28] presents dif-
bias-variance trade-off.
ferential privacy for FL. [13] offers a survey of FL
In the following, we present different types of
for IoT.
ML tasks. After that, we look into different prob-
lems that ML can solve. Then, we review widely
3 Machine Learning (ML) used ML algorithms and methods. Finally, we
surveytheexistingMLplatformsthatarenotsup-
In this section, we first overview ML in terms ported with specialized hardware and not suited
of concepts and goals. Then we review various for DL or FL.
2
Our Review
§3: Machine Learning §4: Distributed Learning §5: Federated Learning §6: Open Questions &
Challenges
• §3.B: Algorithms • §4.B: Parallelism Types • §4.E: Sync Models • §5.B: Aggregation • §5.D: Frameworks • §6.A: Parallel &
• Feedback based • Data • Bulk Synchronous Algorithms • Tensorflow Distributed ML
• Target problem based • Model • Stale Synchronous • FedAvg Federated • §6.B: Federated Learning
• Algo. approach based • Pipeline • Approximate • FedProx, • IBM
• §3.C: Frameworks • §4.C: Vertical Optimization Synchronous • SCAFFOLD Federated
• • • • S W X S c h G e i o k B k g i o t a u - o L n s e t arn • • • M O A C O o p p p o m p t t d i i r m m m e o l x i i u z z S i a a m n im t t ic a i i o o p a t n n i t l o i i f o n ic n a tion • § Fr 4 a .F m • • • : e D w T P A is e y o s t T n y r r o s o k n r o s . c c r M h h fl r o L o w nous • § P 5 ri . v C a • • • : c S y F F … e e e c d d u S O r G i p ty D t , & • • • • N F F F P e L A y V A d S T I y R D M E f E I t L A
• LibSVM • §4.D: Comm. Topologies • … • Attacks • OpenFL
• Cloud based • Centralized • Defenses • §5.E: Datasets
• Hierarchical
• Fully Distributed
Fig. 1: The outline of our review.
Training Training data Hyperparameters
Phase
ML
Algorithm
Prediction Newdata
Phase Trained
Prediction
(Inference) Model
Fig.2:MLphases:Trainingandprediction(infer-
ence).
Model Complexity
3.2 ML Algorithms
ML algorithms can be categorized by the format
and requirements of data (external feedback), by
the type of problems they are designed for (tar-
get problem), and by the techniques they use
(algorithmic approaches).
It is worth noting that there is another way
of categorizing ML: online and offline. In offline
learning,theentiretrainingdataisavailableprior
to training. This is the most common application
of ML. In online learning [29], either the entire
data is not available beforehand or it is compu-
tationally infeasible to perform training over the
entire data at once. An example of the former
is sequential training such as time series analy-
sis in financial markets. An example of the latter
is learning with a very large dataset which does
notfitintothememoryandconsequently,training
becomes prohibitive.
ssoL
Fig. 3: Bias-variance trade-off. Model complexity
with respect to bias and variance.
3.2.1 External feedback
ML algorithms can be classified based on the
external feedback as follows:
Supervised Learning: Learning is performed
by feeding labeled input data so that a model’s
parameters are optimized. Labeled data can be
desired classes, categories, or numerical outputs
corresponding to the training instances. During
training,theoptimizationisachievedbyminimiz-
ing a predetermined cost function. After training,
the trained model is deployed to predict the out-
puts of new instances. An example supervised
learning is to classify newly seen handwritten
digits by training with the labeled digits.
Unsupervised Learning: The goal of unsuper-
vised learning is to find structures and patterns
3
Fig. 4: An artificial neural network example.
in unlabeled data. This means that in unsuper- Clustering finds the distinct groups of simi-
vised learning, the data does not possess desired lar data instances based on a selected similarity
outputs. As an example of unsupervised learning, metric.
clusteringaimstofindsimilargroups(clusters)in Anomaly and novelty detection is used to find
given data. Dimensionality reduction is another datainstancesthataresignificantlydifferentthan
example of unsupervised learning where the goal others. These instances are called outliers. In
is to find a subset of key features that describes anomaly detection, training data consists of both
the data well. outliers and regular (expected) data instances. In
Semi-supervised Learning: In semi-supervised novelty detection, on the other hand, the goal is
learning,theamountoflabeleddataissmallwhile used to detect unseen data where training data is
the amount of unlabeled data is large. Clustering free of outliers.
algorithms are typically used to propagate exist- Dimensionalityreductionisusedtoreducethe
ing labels to the unlabeled data. An assumption number features of the training data. In dimen-
of semi-supervised learning is that similar data sionality reduction, if a subset of the original set
shares the same label. ofthefeaturesisselected,itiscalledfeatureselec-
Reinforcement Learning:Reinforcementlearn- tion. In contrast, if features are combined into
ing is applied when an agent interacts with an new ones, it is called feature extraction. Dimen-
environment. Based on the observations it makes, sionality reduction can also be used to decrease
the agent takes actions. The actions are rewarded computational costs of training. Furthermore, it
or penalized according to a reward function. canalsobeusedtopreventover-fitting.Theprob-
Applications of reinforcement learning lie in the lem of over-fitting with high-dimensional data is
fieldssuchasgametheory,roboticsandindustrial famously known as the curse of dimensionality.
automation. The curse of dimensionality arises due to data
sparsity in high dimensional spaces.
3.2.3 Algorithmic Approaches
3.2.2 Target Problem
ML algorithms can be categorized based on algo-
Under this categorization, ML algorithms are rithmic approaches that they employ.
grouped according to the kind of problems they StochasticGradientDecent(SGD)basedalgo-
are designed to solve. rithms are optimized based on a loss function of
In classification problems, the aim is to cor- the outputs of the model parameters in the oppo-
rectly categorize data instances into the known site direction of the gradient. Because at each
classes. trainingsteparandomsubsetofdataisused,this
In regression problems, the goal is to estimate optimization method is called stochastic. Many
the value of a variable based on other variables common ML algorithms are optimized with SGD
(features). such as artificial neural networks.
4
SupportVectorMachines(SVMs)[30]aretyp- random initialization, neurons compete against
ically used when the input data is not linearly each other.
separable in its original space. They map the • BoltzmannMachines[37,38]arefullyconnected
input data to high dimensional spaces where it ANNs which, unlike other ANNs, have proba-
becomeslinearlyseparable.SVMscanbeusedfor bilistic activation functions. Neurons output 1
classification, regression, and novelty detection. or 0 based on Boltzmann distribution. Boltz-
Artificial Neural Networks (ANNs) are con- mann Machines can be used for classifying,
structed by multiple layers of nodes (neurons) denoising, or completing images.
that have inputs, outputs, corresponding feature • Deep Belief Networks [39] are stacked Boltz-
weights, and an activation function. Layers can mann Machines designed to tackle larger and
be input, hidden, and output layers. ANNs have more complex learning challenges. They are
recently been very successful in tasks such as used for semi-supervised learning.
image classification, object detection, and natural • Hopfield Networks [40, 41] are fully connected
language processing. Figure 4 depicts an exam- networks that are used for tasks such as char-
ple of an ANN. Some well-known types of ANNs acter recognition.
include:
Transformers [42] are a class of DL models
• ConvolutionalNeuralNetworks(CNNs)[31]are thathasshownextraordinarysuccessinmanyML
deep neural networks that incorporate convolu- fields including natural language processing and
tionsandpooling.Whileconvolutionshelpwith computer vision. Transformers were first intro-
learning local data, pooling help with learning duced by a landmark paper from Google [43]
abstract features. CNNs have been extremely which were based on a novel mechanism called
successful in tasks such as image classification, Attention.Atitscore,atransformerisanencoder-
object detection, and image segmentation. decoder model. The success of Transformers has
• Recurrent Neural Networks (RNNs) [32] main- become a regular news-headliner such as the
tains a temporal state of sequence data. The release of GPT-4 [44] and ChatGPT [45].
temporal state may hold short-term or long- Rule-basedalgorithms[46]useasetofrulesto
term memory. RNNs are used in tasks such as learnpatternsfromtheinputdata.Theyaretypi-
time series forecasting, natural language pro- callyeasiertointerpretthanotherMLalgorithms.
cessing, and anomaly detection. Decisiontreesarethemostwell-knownrule-based
• Autoencoders [33] are ANNs that learn latent algorithms.
representations of input data with no supervi- Evolutionary algorithms [47] use ideas from
sion.Theyareusedfordimensionalityreduction biological evolution. In evolutionary algorithms,
and visualization of high dimensional data. thetargetproblemisrepresentedbyasetofprop-
• Generative Adversarial Networks (GANs) [34] erties. The performance metric is called fitness
are (originally unsupervised) neural networks function.Basedonfitnessscores,thesetofproper-
usedtogeneratedatabasedonagamebetween tiesismutatedandcrossedover.Thesealgorithms
a generator and discriminator network. They iterateuntilaccurateestimatesareobtained.Evo-
havebeensuccessfullyappliedinsupervisedand lutionary algorithms can also be used to create
semi-supervised learning. other algorithms such as neural networks.
• GraphNeuralNetworks(GNNs)[35]areatype Semantic and Topic algorithms [48, 49] are
of ANNs designed to perform learning and pre- used to learn specific semantic patterns and dis-
diction on data described by graphs. GNNs tinct relationships in the input data. An example
provideaneasywaytodonode,edge,andgraph application of these algorithms is to find the top-
level ML tasks. ics and relate them to each other in a given set of
• Self-Organizing Maps (SOMs) [36] are neural documents.
networks which produce a low dimensional rep- Ensemble algorithms combine other algo-
resentationofhighdimensionaldata.SOMsare rithms to obtain a solution that performs better
usedforvisualization,clustering,andclassifica- than the individual algorithms. Different ways to
tion. The training is unsupervised where after build ensembles are:
5
• Bagging combines multiple classifiers and uses Google’s Cloud [55], Microsoft Azure [56], Ama-
voting to determine the final output. zon’s SageMaker [57] and the IBM Watson Cloud
• Boosting is a technique that trains the subse- [58].
quentmodelswiththedatainstancesmisclassi-
fied by the preceding models in the chain.
• Stacking is the process where a model trains 4 Distributed Machine
with the outputs of the preceding models in Learning
a chain of several models. Stacking typically
reduces the classification variance. In this section, we introduce large-scale dis-
• RandomForestscombinemultipledecisiontrees tributed ML. We then explore different types of
andoutputan(weighted)averageoftheoutputs parallelisms used in distributed training. Next,
of the individual trees. we dive into vertical scaling techniques. After
that, we present the optimizations for commu-
nications in distributed ML. Then we continue
with the communication topologies and synchro-
3.3 Existing ML Frameworks
nization models. Finally, we conclude this section
In this section, we present the existing ML plat- by the discussion of the existing distributed ML
forms that are not supported with specialized frameworks.
hardware and typically not suited for DL or FL. As a side note, we use client and participant
We then briefly mention the popular ML services interchangeably in the rest of the paper.
in the cloud.
Scikit-Learn [50] is the most popular open-
4.1 Introduction to Distributed ML
source Python library that offers an extensive
suite of ML algorithms. The library is very well Distributed ML is proposed to utilize distributed
maintained and provides a comprehensive set of and heterogeneous computing systems to solve
algorithms, methods, pre-processing, pipelining, large and complex problems where a solution
modelselectionandhyper-parametersearchcapa- cannot be obtained by a single standalone homo-
bilities.ItprovidesinterfacestoworkwithNumPy geneous computing device. Distributed ML offers
and SciPy packages. two different approaches. The first is to use het-
Weka [51] is a general-purpose and popular erogeneousresourcesavailableinasinglecomput-
Java ML library. It provides a large collection of ing system such as Graphical Processing Units
algorithmsandvisualizationtools.Wekasupports (GPUs).Thisiscalledverticalscaling.Thesecond
numerous tasks such as pre-processing, classifica- is to use multiple machines to solve larger prob-
tion, regression, clustering and visualization. lemsandtosupportfault-tolerance.Thisiscalled
XGBoost[52]isascalableanddistributedgra- horizontal scaling.
dient boosting library based on decision trees. It GPUs have been the most common mean of
implements parallel ML algorithms for classifica- vertical scaling. Given sufficient parallelism, it
tion, regression and ranking tasks. has been shown that GPUs significantly acceler-
Shogun [53] is a research-oriented open-source atetraining[59,60].Forinstance,NVIDIAGPUs
ML library. It offers a large number of ML algo- have been popular in accelerating ML [59, 61].
rithms and cross-platform support by providing Vendors such as Google have implemented their
bindings with other languages and environments own specific hardware accelerators. Tensor Pro-
such as Python, Octave, R, Java. Shogun’s core cessing Units (TPUs) [60] are designed specif-
library is implemented in C++. ically for this purpose. Others such as Graph-
LibSVM [54] is a specialized C/C++ library core [62] and SambaNova [63] have followed this
for SVMs. It provides interfaces for Python, R, trendwithsophisticateddataflow-basedhardware
MATLAB and many others. designs and powerful system software tool-chains.
Many companies offer standard ML and dis- In contrast to vertical scaling, horizontal scal-
tributed ML services. Moreover, these services ing corresponds to distributed training and infer-
oftenincludethesupportforGPUsandotherML ence across multiple machines. Horizontal scaling
specific hardware. Popular cloud ML services are enables ML solutions to handle applications and
6
data that do not fit in the resources of a sin- disadvantage is the potential intensive communi-
gle machine. Additionally, the usage of multiple cations among the resources.
machines typically accelerate training and infer-
ence. 4.2.3 Pipeline Parallelism
Pipelineparallelismcombinesmodelanddatapar-
4.2 Parallelisms in Distributed allelisms. It distributes the model and data in a
Training and Inference such a way that there is a pipeline among the
computing resources in which each resource has
There are three types of parallelisms used in
a different part of the model. Pipeline parallelism
distributed training. These are data, model and
maintains the advantages of model parallelism
pipeline parallelism.
while increasing the resource utilization. Figure 7
illustrates pipeline parallelism.
4.2.1 Data Parallelism
4.3 Vertical Optimization
Indataparallelism,thesameMLmodelistrained
Approaches
withdifferentsubsetsofthedatainparallelatdif-
ferent computing resources. Once all computing
We have discussed the types of parallelisms used
resources finish the assigned training, the mod-
in distributed ML above. Now, we explore three
els are accumulated and an average model is
vertical optimization approaches. They are model
obtained. Then, this average model is distributed
simplification, optimization approximation, and
back to each computing resource for the subse-
communication optimization approaches.
quent rounds of training. Figure 5 depicts data
parallelism with two parallel resources.
4.3.1 Model Simplification
Themainadvantageofdataparallelismisthat
itisapplicabletoanydistributedMLmodelwith- Model simplification refers to the reformulation
out requiring expert/domain knowledge. It is also of a target model to decrease its computational
very scalable for compute-intensive models, such complexityasawayofachievingefficiency.Model
as CNNs. One disadvantage of data parallelism is simplification can be further divided into cate-
that model synchronization may become a bot- goriesbasedonthetypeoftheMLmodels.These
tleneck. Another disadvantage occurs when the models can be based on kernels, trees, graphs and
model does not fit in the memory of a single deep neural networks. Table 1 summarizes the
device. model simplification techniques.
Simplifications for kernel-based models are
made by sampling-based or projection-based
4.2.2 Model Parallelism
approximations. While sampling-based methods
In model parallelism, the model is partitioned [64, 65] approximate kernel matrices by random
and distributed to different computing resources. samples, projection-based methods [66, 67] use
The data is distributed as well according to the Gaussianorsparserandomprojectionstomapthe
model distribution. When there is a dependency data features to low dimensional sub-spaces.
among the computing resources, synchronization Performance and scalability improvements for
is needed for the parameters (weights) to be tree-based models, such as decision trees and ran-
shared consistently. Figure 6 shows model paral- dom forests, are commonly based on rule [68] or
lelismwheretworesourcesareused.Animportant feature sampling [52] [69].
noteisthatinthefigure,everytimethatadashed Graph-based simplifications are developed for
line crosses a resource boundary, at least one syn- graph-based models where nodes represent the
chronization event must take place to ensure data data instances and edges represent the similarity
consistency. between the instances. In these models, the cost
The main advantage of model parallelism is of training comes from two main sources: graph
that models take less memory in each single construction and the label matrix inversion. For
resource (device). Its main disadvantage is that sparse graphs, graph construction constitutes the
themodelpartitioningisoftennontrivial.Another main cost of training. This is because when label
7
Fig. 5: Data parallelism for a deep neural network.
Fig. 6: Model parallelism for a deep neural network.
propagationisused,itlowersthecostoftheinver- Performance improvements for deep neural
sion of the label matrix and it becomes less costly networks can be achieved in two different ways.
than graph construction. As a result, graph con- First, activation functions, such as Rectified Lin-
structiondominatesthemaincomputationalcost. earUnit(ReLU)[75]anditsvariants[76][77],can
To construct sparse graphs [70], hashing meth- be employed instead of the expensive functions,
ods [71] [72] are often used. Different than sparse such as sigmoid and tanh, which use the expo-
graph models, there are also graph models that nentialfunction.Othertechniques,specificallyfor
are built by anchor graphs [73]. An anchor graph CNNs, involve depth-wise filter factorization [78]
is a hierarchical representation of a target graph. and group-wise convolutions [79].
It is built with a small subset of the instances.
This small subset is used to retain the similarities
4.3.2 Optimization Approximation
betweenallinstances.Insucharepresentation,the
label matrix inversion is the main cost of train- Optimization approximation is a family of tech-
ing.Toreducethecostofthematrixinversion,the niques that are used to reduce the cost of the
pruning of anchors’ adjacency [74] is a common optimization related computations, i.e., gradient
technique. computations, for training. It is generally realized
by computing the gradients with a small number
8
Fig. 7: Pipeline parallelism for a deep neural network.
Compute
ML Node Aggregate
Broadcast
ML Node ML Node
ML Node ML Node
ML Node Data Data ML Node
ML Node ML Node ML Node ML Node Data ML Node Data
Data Data Data Data ML Node Data ML Node
Data Data Data Data Data
(a)Centralized (b)Hierarchical (c)Distributed
Fig. 8: Different topologies of distributed ML.
of instances or parameters instead of all instances terms of achieving fast convergence [83]. Adap-
or parameters. Care has to be taken since such tivelearningratescanboostthespeedandquality
approximations can lead to longer convergence ofconvergence[84].Furtheradaptiveadjustments
times, local extrema, or even non-convergence. areshowntobeeffective[85][86].Complementary
Optimization approximation can be categorized to adaptive sampling or adaptive learning rates,
based on the specific optimization algorithm that reducing the variance of gradients and computing
is being used: Mini-batch gradient descent, coor- more accurate gradients are shown to be effec-
dinate descent, and numerical integration based tive and efficient in achieving fast convergence.
on Markov chain Monte Carlo. Table 2 shows the Suchmethodsuseaveragegradientsorlook-ahead
existing techniques. corrections of gradients [87] [88]. In addition to
Techniquesthatareusedformini-batchgradi- the accurate first-order gradients, higher-order
entdescentapproximationsareadaptivesampling gradients may be needed due to ill-conditioning
of mini-batches, adaptive learning rates, and the [89] [90]. Hessian matrices are estimated by the
improvements in gradient approximations. Adap- high-ordergradientstomakeconvergencepossible
tive sampling [80] [81] for mini-batches takes the [89].
data distribution and gradient contributions into Coordinate gradient descent are targeted at
account rather than just using random batches the problems where the instances are high dimen-
of samples or making a gradual increase in the sional, such as recommender systems [91] and
batch size [82]. Learning rates are also crucial in natural language processing [92]. To speed up the
9
Model Type Techniques Existing Work
Sampling-based
Kernel-based Models [64, 65, 66, 67]
Projection-based
Rule sampling
Tree-based Models [68, 69, 52]
Feature sampling
Sparse graph construction
Graph-based Models [70, 71, 72, 73, 74]
Anchor graph based optimization
Efficient activation functions
Deep Neural Network Models [75, 76, 77, 78, 79]
Filter factorization and grouping
Table 1: Model simplifications for different ML models.
Categories Techniques Existing Work
[80], [81], [82]
Adaptive sampling [83] [84]
Mini-batch gradient descent Adaptive learning rates [85] [86]
Gradient corrections [87], [88]
[89], [90]
[91] [92]
Rule sampling
Coordinate gradient descent [93] [94] [95]
Feature sampling
[96] [94] [97] [98]
Sparse graph construction
Bayesian optimization [99], [100], [101]
Anchor graph based optimization
Table 2: Optimization approximation based techniques.
optimizations performed by coordinate gradient Intheseoptimizations,compressionofgradientsis
descent, a small number of parameters can be one of the two main ideas. Some studies compress
selected at each iteration. Random selection of each gradient component to just 1 bit [102]. Oth-
parameters has shown to be effective [93] [94]. ers map gradients to a discrete set of values [103]
Parameter selection can also be based on the or sketch gradients into buckets and then encode
first and/or second-order gradients information them [104]. Some proposals only communicate
[95] [96]. Another approach for speedup is to use gradients that are bigger than a certain threshold
extrapolationstepsduringtheoptimizationphase [105]. A combination of gradient compression and
[94]. If the optimization problem is non-convex, low-precision learning has been shown to further
thenstudiessuchas[97][98]presentspecificsolu- reduce the communication costs [106]. The other
tions. For instance, Li and Lin [97] propose an main idea for the optimization of communication
extended variant of accelerated proximal gradient is gradient delaying [107]. Ho et. al. explore the
method. usageofgradientdelaysforstalesynchronouspar-
Finally, Bayesian optimization methods are allel communications. Zheng et. al. [108] on the
commonly based on Markov chain Monte Carlo other hand compute approximate second-order
[99] [100]. Such methods employ stochastic mini- gradientsandoverlapthesecomputationswiththe
batches due to the high cost of the acceptance delays to enhance the communication efficiency.
tests [101]. Zhang, Choromanska, and LeCun [109] define an
elastic relationship between the local and global
model to avoid local minima as gradient transfers
4.3.3 Communication Optimization
are delayed. Different than these studies, McMa-
Approaches han and Streeter [110] introduce communication
optimizations for online learning.
Optimizationstoreducecommunicationcostscon-
Table 3 summarizes these techniques.
stitute another option to those for computation.
10
Categories Techniques Existing Work
Gradient compression [102], [103], [104] [105], [106]
Communications
Gradient delay [107], [108], [109], [110]
Table 3: Communication optimization approaches.
4.4 Communication Topology 4.5 Synchronization Models
In a distributed ML system, the computing Synchronization models are techniques to guide
resources (clusters) can be structured in different and perform synchronization between parallel
ways. The types of topologies that the resources computations and communications. These models
use can be categorized into three: centralized, seek to establish the best trade-off between fast
hierarchical, and fully distributed (decentralized). updates and accurate models. To do fast updates,
Figure8depictsthesetopologies.Table4summa- lower levels of synchronization are required. In
rizes our discussion. comparison, to obtain accurate models, higher
levels of synchronization are needed.
As far as ML is concerned, stochastic gra-
4.4.1 Centralized Topology
dient descent is one of the most popular algo-
In this topology, the computation of the global rithms for the optimization during the training
model parameters, gradient averaging and com- phase. As discussed below, variants of stochas-
munications with the distributed nodes/clients tic gradient descent have been implemented in
are performed at a central server. Every dis- accordance with the underlying synchronization
tributed client directly communicates with the model. Therefore, those variants constitute prac-
central server and works with its local data only. tical examples for the corresponding synchroniza-
A major disadvantage of a centralized topology is tion model.
that the central server constitutes a single point
offailureandacomputationalbottleneck.Advan- 4.5.1 Bulk Synchronous Parallel
tages of a centralized topology are the ease of
It is a synchronization model [111] where syn-
its implementation and inspection. Figure 8 (a)
chronization happens between each computation
presents an example of this topology.
and communication phase. Since this model is
serializable by construction, the final output is
4.4.2 Hierarchical Topology
guaranteedtobecorrect.However,whenthereare
The computations and aggregation of the global discrepancybetweentheprogressofparallelwork-
model parameters are performed in a stage-wise ers, the faster workers have to wait for the slower
and hierarchical way. Each child node only com- ones.Thiscanresultinsignificantsynchronization
municates with its parent. These topologies offer overhead.
higher scalability than the centralized counter-
4.5.2 Stale Synchronous Parallel
parts and easier manageability than the dis-
tributed counterparts. Figure 8 (b) depicts a
Thissynchronizationmodel[107]allowsthefaster
hierarchical topology.
workers continue with their version of data for
an additional but limited number of iterations to
4.4.3 Fully Distributed Topology reduce the synchronization overheads due to the
wait on the slower workers. While this can help
Every participant maintains a local copy of the
reducetheoverheads,dataconsistencyandmodel
global model in a fully distributed topology. Par-
convergence may become difficult to establish.
ticipants directly communicate with each other.
Compared to the centralized and hierarchical
4.5.3 Approximate Synchronous
topologies, scalability is much higher and the sin-
Parallel
gle points of failure are eliminated. However, the
implementation of these topologies is relatively In this model, synchronization is sometimes omit-
more complex. Figure 8 (c) shows this topology. ted or delayed to reduce the overheads. However,
11
Topology Complexity Scalability Manageability Single Point Failures Latency
Centralized Low Low High Yes Low
Hierarchical Medium Medium Medium Yes Medium
Fully Distributed High High Low No High
Table 4: Comparison of different communication topologies.
the accuracy and consistency of a model may PyTorch [117] is another free and open-source
deteriorate if care is not taken. An advantage of framework based on the Torch Library developed
approximate synchronicity is that when a param- by Meta. It is a popular framework for scientific
eter update is insignificant, the server can delay research and provides automatic differentiation
synchronization as much as possible. A disadvan- and dynamic computation graphs. It supports
tageisthatselectingwhichupdatesaresignificant distributed learning mainly in two ways with
or not is typically difficult to do. As an example torch.distributedpackage[118].First,sameasthe
of the application of this model, Gaia [112] is an Tensorflow mirrored strategy, PyTorch offers dis-
approximate synchronous parallel ML system. tributed data-parallel training which is based on
the single-program and multiple-data paradigm.
4.5.4 Asynchronous Parallel Second, for the cases that do not fit into data
parallelism, PyTorch provides Remote Procedure
Thissynchronizationmodelomitsallsynchroniza-
Call (RPC) based distributed training. Examples
tions among the workers. While these omissions
of these types of distributed training are parame-
may significantly reduce the computation time
ter server, pipeline parallelism, and reinforcement
and communication overhead, asynchronous com-
learning with multiple agents and observers.
munications may cause ML models to produce
MXNet [119] is an open-source DL framework
incorrectoutputs.Togiveanexampleapplication,
for research prototyping and production. It offers
HOGWILD algorithms [113] are developed based
data-parallel distributed learning with parame-
on asynchronous communications.
ter servers. MXNet allows mixing both symbolic
and imperative programming for computational
4.6 Existing Distributed Learning
efficiency and scalability. MXNet supports many
Frameworks
programming languages such as C++, Python, R
There are many ML frameworks that provide dis- and Julia.
tributed ML algorithms and utilities. The most Horovod [121] is a distributed wrapper DL
popular distributed implementations are Tensor- framework for TensorFlow, Keras, PyTorch, and
flow [114, 115, 116], PyTorch [117, 118], MXNet Apache MXNet. Horovod is often easy to use
[119, 120], Horovod [121], Baidu [122], Dianne because it only requires an addition of a small
[123], CNTK [124] and Theano [125]. Table 5 number of library calls to the source code.
summarizes these frameworks. Other than the Horovodsupportsdata,modelandpipelineparal-
ML frameworks above, some general-purpose dis- lelisms.
tributed computing libraries, such as Apache Baidu [122] was started as an easy-to-use, effi-
Spark [126] and Hadoop [127], also support dis- cient distributed DL platform. It supports large-
tributed ML. scale ML and can train hundreds of machines in
Tensorflow [114] is a free and open-source parallel with GPUs. Baidu offers various com-
software library developed for ML and DL by mercial solutions, such as machine translation,
Google. In fact, Tensorflow is the most popular recommender systems, image classification and
library among the DL libraries. It supports dis- segmentation.
tributed learning with several distribution strate- Dianne [123] is a distributed and ANNs-
gies, such as mirrored, multi-worker and parame- focusedsoftwareframeworkbasedonOSGiwhich
ter server, that are either data or model parallel isadynamicmodulesystemforJava.Diannesup-
[115, 116]. The library provides efficient and scal- portsbothmodelanddataparallelismsandoffers
able ML implementations for CPUs, multi-GPUs UI-based functionality.
and mobile devices.
12
Frameworks Pros Cons Parallelism
Most popular.
Strong support by Google.
Efficient and scalable CPU,
Tensorflow multi-GPU, Difficult to use API Data, Model
mobile implementations.
Various training strategies:
Multi-worker, Parameter server...
Dynamic computation graph
Data, Model,
PyTorch Automatic differentiation No support for mobile
Pipeline
Support of remote procedure calls
High scalability
Support of many languages:
MXNet C++, Python, Julia, R Difficult to use API Data
Usage of symbolic
and imperative programming
Easy to use
Data Model
Horovod Supports Tensorflow, Keras, Lacks fault tolerance
Pipeline
PyTorch, and MXNet
Limited scalability
Baidu Commercial ML and DL solutions Data, Pipeline
No support for fault-tolerance
Dianne Java based development platform No other languages Data, Model
Open-source No longer actively developed
CNTK Data, Model
Efficient and high-performing Limited mobile support
Open-source and cross-platform
Theano Discontinued Data
Powerful numerical library
Table 5: Existing distributed learning platforms.
TheMicrosoftCognitiveToolkit(CNTK)[124] Hadoop [127], supports distributed ML algo-
is open-source software for commercial-grade DL. rithms, applications and utilities. Apache Spark
However, it is no longer actively developed. It is one of the most popular implementations of
supports distributed learning through parallel MapReduce. It includes MLlib [129] which is
Stochastic Gradient Descent (SGD) algorithms. an open-source scalable distributed ML library.
CNTK implements the following four parallel MLlib consists of widely-used ML algorithms and
SGDalgorithms:Data-parallel,blockmomentum, utilities for classification, regression, clustering,
model averaging, and asynchronous data-parallel and dimensionality reduction tasks.
SGD.
Theano [125] was a popular open-source 5 Federated Learning (FL)
Python library to define, optimize and evalu-
ate mathematical expressions. It has support for In this section, we first introduce FL. We then
efficient multi-dimensional arrays. Developed by present the existing aggregation algorithms in
UniversitedeMontreal,itisnolongerusedwidely. detail.Afterthat,wediscussthesecurityandpri-
Theano supports data-parallel distributed learn- vacy aspects of FL. We conclude this section by
ing by both synchronous and asynchronous train- the available FL platforms and datasets.
ing. It also supports multi-GPU multi-machine
distributed training. 5.1 Introduction to FL
General-purpose distributed frameworks that
are based on MapReduce programming model FL[9]isavariantofMLwheretrainingamodelis
[128], such as Apache Spark [126] and Apache done by distributed clients that individually train
local models. Once local models are trained, all
13
localmodelparametersaresenttoacentralserver Distributed machine learning and FL have
which then calculates the average of the parame- some fundamental differences. These are:
ters (weights) to compute an average model. This
• While distributed machine learning’s main goal
average model is then communicated back to the
is to minimize the computational costs and
clients for subsequent local training. FL performs
achieve high scalability, FL’s main goal is to
distributedtrainingwithoutsharingprivateclient
provide privacy and security for the user/client
data.
data. As a result, FL is designed such that
user/client data is never shared.
• Distributed learning assumes that the user
3-Global model aggregation and update data is independent and identically distributed
W, W, …, W -> W
1 2 n (i.i.d). On the other hand, FL assumes non-
W
i i.i.d because users typically have different data
distributions and types.
W W W 1-Model Initialization • Distributed learning is performed based on
1 2 n Wdownload
aggregating client data, which is then dis-
tributed to different clients for training and
inference. Contrarily, FL utilizes decentralized
data. The client data is never shared and is
2-Local model training never aggregated on a central server.
… and W i upload
Fig. 9: Federated Learning Overview.
Algorithm 1 Federated learning: client and
FL can be categorized based on how data par-
server functions
titioningisdone.HorizontalFL[130]referstothe
casewheretheclientssharethesamefeaturespace 1: i←isClient or isServer
buthavedifferentsamplespaces.Thisissimilarto 2: E ←totalEpochs
data parallelism. An example of horizontal FL is 3: B ←totalNumBatches
wake-up voice recognition on smartphones. Users 4: η ←learningRate
with different types of voices (different sample 5: w ←initialWeights
spaces) speak the same wake-up command (same 6: if i=isClient then
feature space). 7: function UpdateClientWeight(w,k)
Vertical FL [130] takes place where the clients 8: for epochs e from 1 to E do
share the same sample space but have different 9: for batchs b from 1 to B do
feature spaces. As an example, the common cus- 10: w ←w−η∇l(w,b)
tomers (same sample space) of a bank and an 11: end for
e-commerce company (different feature spaces) 12: end for
join the training of an FL model for optimizing 13: return w to the server
personal loans. 14: end function
Finally, Federated Transfer Learning [131] 15: else
refers to the case where both the sample and 16: function ServerUpdateWeight
thefeaturespacesaredifferent.Federatedtransfer 17: t←currentRoundID
learning transfers features from different feature 18: for k in sub batch of K clients do
spacestothesamerepresentationtotrainamodel 19: the following is done in parallel
with the data of different clients. An example 20: w t k +1 ←
UpdateClientWeight(k,wk)
is disease diagnosis by many different collabo- t
rating countries with multiple hospitals which 21: end for
(cid:80)K wk
have different patients (different sample spaces) 22: w t+1 ← k= K 1 t+1
with different medication tests (different feature 23: end function
spaces). 24: end if
14
We now discuss the very first FL algorithm, participants. FedAvg is a straightforward algo-
FedAvg, proposed by McMahan et. al. [9]. Algo- rithmhowever,itisbiasedtowardtheparticipants
rithm 1 describes FedAvg. It shows the action who have favorable network conditions.
taken by the server and clients during a round Aggregation algorithms have been studied
of FL. The clients train the model with their extensively for centralized topologies [132, 133].
data. Once trained, the weights are sent to the To decrease the communication overheads, Liu
server as described by the UpdateClientWeight et. al. propose the Federated Stochastic Block
function on line 7. Once the server receives the Coordinate Descent (FedBCD) algorithm [132]
weights from the clients, which is done in paral- in which each participant makes multiple local
lel, it averages out all the weights and sends the updates before synchronizing with other partic-
averageweightsbacktoeachclients,asseeninthe ipants. Differently, FedOpt [133] uses gradient
ServerUpdateWeight function on line 16. Train- compression to reduce communication overhead
ingisrepeatedifthedatachanges.Thisistokeep while sacrificing accuracy. Furthermore, for edge
the weights updated. devices where computational resources are lim-
ited, algorithms such as FedGKT [134] are devel-
oped.
5.2 FL Applications
A significant objective in FL is to provide
fairness. Fairness means that the clients equally
FL has a wide range of applications across differ-
contribute to the global model with respect to
ent domains and settings. Some of them are:
certain metrics. Researchers have proposed algo-
• Smartphones: FL has been used to develop ML rithms such as Stochastic Agnostic Federated
applicationsforsmartphonessuchasnext-word Learning (SAFL) [135] and FedMGDA+ [136] to
prediction, and face and voice recognition. achieve fairness.
• Healthcare:FLhasbeenappliedsuccessfullyfor Adaptive FL and its impact on convergence
research problems in medical studies such as and accuracy have been explored in various
drug discovery and brain tumor segmentation. recent works. ADAGRAD [137] offers an adap-
• The internet of things (IoT): IoT is a net- tive approach to ML optimization compared to
workofdigitalormechanicalcomputingobjects FedAvg. ADAGRAD and its variants dynami-
that have sensors, software, and other comput- cally choose server and client learning rates and
ingtechnologies.IoTexchangesdatawithother momentum parameters during training. Mime
devicesandsystemsovertheinternettoperform Lite [138] is a closely related study where adap-
specific learning tasks. Applications of FL in tive learning rates and momenta are reported to
IoT include autonomous driving and intrusion improve accuracy.
and anomaly detection. Some recent aggregation algorithms support
• Finance: FL has been adopted to detect/iden- heterogeneityofparticipantdata.FedProx[139]is
tify financial crimes such as fraudulent loans suchanalgorithmusedforFLoverheterogeneous
and money laundering. data and resources. SCAFFOLD [140] is another
algorithm that accounts for heterogeneous data
while reducing the number of rounds to converge.
5.3 FL Aggregation Algorithms
FedAtt [141] accounts for the client contributions
In FL, due to data parallelism and horizontal FL, by attending to the importance of their model
aggregation algorithms are needed to aggregate updates. The attention is quantified by the sim-
the models or gradients between the participants. ilarity between the server model and the client
As stated above, the very first aggregation algo- model in a layer-wise manner. FedNova [142] pro-
rithm, called Federated Averaging (FedAvg), was poses a normalized averaging method as a way to
introduced by McMahan et. al. [9] who essen- avoid objective inconsistencies and to achieve fast
tially kick-started FL itself. FedAvg computes convergence for highly heterogeneous clients.
the global model parameters by averaging the Personalizationisanotherimportantconsider-
parameter updates of the participants. Once the ationinFL.Therehasbeenextensiveresearchon
global parameters are computed and updated, personalized FL [143] [144]. Tan et. al. [145] offer
these parameters are communicated back to the a survey of the latest personalization techniques.
15
WhenthetopologyoftheclientsinFLishier- learnthemeta-characteristicsofthetrainingdata.
archical, an aggregation algorithm needs to take Classrepresentativeinferenceattacksaimtolearn
the hierarchy into account. Numerous hierarchi- representative samples of a target class.
cal aggregation algorithms have been proposed Generative Adversarial Networks (GANs)
for such settings [146, 147]. Among hierarchi- based attacks [166, 167, 168] are used to launch
cal solutions, SPAHM [146] and PFNM [147] are poisoning attacks where GANs generate the
BayesianFLmethods. Similarly, fordecentralized altered or false data and/or model parameters.
topologies, decentralized algorithms have been There are many other attack types [179, 180],
developed [148, 149]. such free-riders [26, 163] and Eavesdropping [26,
Considering fault-tolerance in FL, Krum [150] 163].
is an aggregation scheme that is reportedly
resilienttoByzantinefailures[151]wherecomput- 5.4.2 Defenses
ingprocessesfailarbitrarilyandfailuresymptoms
The most commonly used attack defense mecha-
aredifferentfordifferentobservers.Forthesetypes
nisms can be categorized by the usage of trusted
of failures, more fault-tolerant FL studies are
execution environments [169, 170], homomorphic
needed.
encryption[164,165],differentialprivacy[28,154,
155], and possibly some combinations of them.
5.4 Security and Privacy in FL
There are many other techniques which are based
The security of FL entails ensuring the triad of on GANs [181], anomaly detection [176], secure
confidentiality,integrityandavailabilityofitsdata multi-party computation [171], data anonymiza-
and models, and particularly, data privacy. Pri- tion [175], and blockchains [27, 172].
vacy is defined as the protection of the raw data A trusted execution environment [169, 170]
against the information leakage. In this section, is an (hardware/software) architecture where the
we first summarize the attack types and then the program execution is secured and information
defensive actions and methods existing in the FL leakageisnotpossible.Sucharchitecturesusespe-
literature [26]. cialized designs to prevent unauthorized accesses
as well as privacy violations in FL [169, 170].
5.4.1 Attacks Homomorphic encryption [182] is a certain
type of encryption in which the decryption of
TherearenumerousattacktypesinFL.Poisoning
the results of the computations performed on the
attacks aim to tamper with and/or alter the data
encrypted data is the same as the result of the
orthemodel.Datapoisoning[156,157,158]refers
samecomputationsperformedontheunencrypted
to altering the features in the training data or
data. Homomorphic encryption has various levels
generating false data to degrade the performance
depending on the whether addition and/or mul-
of a model on the unseen data. Model poison-
tiplication is supported. It has been adapted for
ing [159, 160, 161] refers to the modification of
data privacy in FL [164, 165].
the model parameters and/or the fabrication of
Differential privacy [152, 153] is a technique
false weights that are communicated between the
for achieving data privacy by adding noise to raw
participants and the servers.
data. It is commonly used in FL [28, 154, 155].
Backdoor attacks [177, 178] inject malicious
Table 6 reviews attacks and defenses in FL.
instructions into the models while not impact-
In addition, Table 7 compares the defense mech-
ing their expected performance. These attacks
anisms in terms of the strength of the protection,
are non-transparent and notoriously difficult to
the computational and communication efficiency,
detect.
robustness, scalability, and generalizability of a
Inferenceattacks[26,162,163]involvegaining
mechanism.
knowledge of the sensitive information of the par-
ticipants, the training data or the model through
5.5 Existing FL Frameworks
the communications occurring during training or
inference. Membership inference attacks aim to The most widely used FL frameworks are Ten-
learn if a sample has been used as a train- sorFlow Federated [183, 184], IBM Federated
ing instance. Property inference attacks aim to Learning [185], NVIDIA FLARE [186], FedML
16
Defense Type Addressed Attacks Potential Negative Effects
Data Poisoning [156, 157, 158]
Differential privacy
Model Poisoning [159, 160, 161] Decreased model utility
[28, 152, 153, 154, 155]
Inference attacks [26, 162, 163]
Homomorphic encryption Inference attacks [26, 162, 163]
High computational costs
[164, 165] GAN-based attacks [166, 167, 168]
Trusted execution environments Inference attacks [26, 162, 163]
Specialized hardware
[169, 170] Model Poisoning [159, 160, 161]
GAN-based attacks [166, 167, 168]
Secure Multi-party Computation
Inference attacks [26, 162, 163] High computational costs
[171]
Eavesdropping [26, 163]
Blockchain [27, 172] Blockchain attacks [173, 174] High resource costs
GAN-based attacks [166, 167, 168]
Data anonymization [175] Decreased data usability
Inference attacks [26, 162, 163]
Data Poisoning [156, 157, 158]
Anomaly detection [176] Model Poisoning [159, 160, 161] Detection latency
Free-riders [26, 163]
Table 6: Attacks and defenses in FL.
Defense Type Protection Efficiency Robustness Scalability Generalizability
Differential privacy
High High High High High
[28, 152, 153, 154, 155]
Homomorphic encryption
High Low High Low High
[182]
Trusted execution environments
Medium High Medium Low High
[169, 170]
Secure multi-party computations
High Medium High Low Medium
[171]
Blockchain [27, 172] High Low Medium High Medium
Data anonymization [175] Medium Medium Low High High
Anomaly detection [176] Medium High Medium High Low
Table 7: Comparison of the defense mechanisms in terms of the strength of the protection, the compu-
tational and communication efficiency, robustness, scalability, and generalizability of a mechanism.
[187], Federated AI Technology Enabler (FATE) lower-levelfunctionalitiesfornewalgorithmstobe
[188], PySyft [189], and Open Federated Learning implemented.
(OpenFL) [190]. Table 8 summarizes the existing IBM Federated Learning [185] provides sup-
FL frameworks. port for FL and DL models written in Keras,
TensorFlow Federated [183] (and Keras Fed- PyTorch and TensorFlow. FedAvg, SPAHM,
erated [184]) is an open-source framework for FL PFNM, and Krum are among the available aggre-
by Google. It enables researchers to simulate FL gation algorithms. IBM FL supports data and
algorithms.FedAvg,FedProx,FedSGD,andMime modelparallelism.Inaddition,differentialprivacy,
Lite are some of the FL aggregation algorithms secure multi-party computation and homomor-
that are readily available. TensorFlow Federated phicencryptionareavailabledefensesforensuring
supports data and model parallelisms. It provides privacy and security. IBM FL also offers the
differential privacy as a privacy measure. Tensor- implementationsofseveraltopologiesandcommu-
FlowFederatedhastwomainAPIs.FLAPIoffers nication protocols.
built-in algorithms. FL Core API offers a set of
17
Frameworks Aggregation Algorithm Parallelism Privacy and Security
TensorFlow Federated FedAvg, FedProx,
Data, Model Differential privacy
Keras Federated FedSGD, Mime Lite
Differential privacy
FedAvg, SPAHM,
IBM Federated Data, Model Secure multi-party computation
PFNM, Krum
Homomorphic encryptions
FedAvg, FedProx, Differential privacy
NVIDIA FLARE Data
SCAFFOLD Homomorphic encryption
Differential privacy
FedAvg, FedOpt,
FedML Data, Model Cryptography
FedNova, FedGKT
Coding approaches
Homomorphic encryption
FATE FedAvg Data, Pipeline
RSA
FedAvg, FedProx, Differential privacy
PySyft Data, Model
FedSGD Homomorphic encryption
Trusted execution environments
FedAvg,
OpenFL Data RSA
FedADAGRAD
Differential privacy
Table 8: Existing FL platforms.
NVIDIA FLARE (Federated Learning Appli- a module for cloud technologies. FATE provides
cation Runtime Environment) [186] is a modular homomorphic encryption and RSA for secure and
open-source software development kit (SDK) for privacy preserving training. FATE supports data
FL which offers secure and privacy-preserving and pipeline parallelisms.
distributed learning. FLARE provides FL algo- PySyft [189] is an open-source multi-language
rithmssuchasFedAvg,FedProxandSCAFFOLD. library that provides secure and private DL and
It offers differential privacy and homomorphic FL in Python for frameworks such as PyTorch,
encryption.FLARESDKhasseveralcomponents, Tensorflow and Keras. It supports differential
such as a simulator for prototyping, secure man- privacy and homomorphic encryption. FedAvg,
agement tools for provisioning and deployment FedProx and FedSGD are among the available
and an API for extensions. aggregation algorithms. Training can be data or
FedML [187] framework offers a wide-range model parallel.
of cross-platform FL capabilities including nat- OpenFL[190]isanopen-sourcePythonframe-
ural language processing, computer vision, and work originally developed by Intel Labs. It pro-
GNNs. FedAvg, FedOpt, FedNova and FedGKT vides a set of workflows for the researchers to
are the supported FL algorithms. FedML offers experiment with FL. FedAvg and ADAGRAD
defense mechanisms such as differential privacy, algorithms are built-in. OpenFL’s capabilities
cryptography routines, and several coding meth- includetrustedexecutionenvironments,RSA,dif-
ods. It supports data and model parallel dis- ferential privacy.
tributed learning. FedML models can be trained
and deployed at the edge or on the cloud. 5.6 FL Datasets
FATE [188] is an open-source platform ini-
tiated by WeBank, a bank based in Shenzhen, AsFLresearchprogresses,newdatasetsarebeing
China. It provides a diverse set of FL algorithms, built.Oneofthemostwell-knowndatasetsforFL
such as tree-based algorithms, DL, and trans- istheLEAF[191].Itisasuiteofopen-sourcefed-
fer learning. It offers a set of modules consisting erated datasets. There are a total of six different
of an ML algorithms library, a high-performance datasets. One of the datasets, called FEMNIST,
serving system, an end-to-end pipeline system, a is built for image classification. Sentiment140,
multi-party communication network system, and which consists of Tweets, is a dataset for senti-
ment analysis. Shakespeare is a text dataset of
18
ShakespeareDialogueswhichisusedfornextchar- Distributed and parallel ML platforms, espe-
acter prediction. Celeba is an image classification cially those executed on high-performance com-
dataset of celebrity images. There is a synthetic putingsystems,oftenconsiderfault-toleranceasa
classification dataset which is generated for the second-class concern. However, given the sizes of
FL models that are device-dependant. Lastly, the the latest large-scale computing systems, failures
Reddit comments dataset is used for next word are common; not rare [199]. As a result, efficient
prediction. checkpointing and/or replication solutions [199]
TensorFlow Federated [192] offers several are needed to recover from errors and to limit the
datasetstosupportFLsimulations.Whilesomeof amount of lost computation due to a failure.
its datasets are the same as those of LEAF, there Ensuring security and privacy for distributed
are also different datasets, such as the federated and parallel ML has consistently been a serious
CIFAR-100 dataset, the FLAIR dataset, and the concern [196]. While FL was devised for the pri-
federated Google Landmark v2 dataset. vacy of user data, there have been many novel
Street Dataset [193] is a real-world image types of attacks [179, 180]. These attacks include
dataset. It contains images generated from street adversarial [168], poisoning, evasion, backdoor,
cameras. A total of seven object categories anno- and integrity attacks [180, 197]. As such attacks
tated with bounding boxes. This dataset is built get sophisticated, so must their defenses. More-
for object detection tasks. over, the systematic deployment of the defenses
CC-19[194]isanewdatasetrelatedtothelat- to the physical systems as well as the evalua-
estfamilyofcoronavirus(COVID-19).Itcontains tion of these deployments have not studied well
theComputedTomography(CT)scanofsubjects [197]. Furthermore, there is a lack of the rigor-
and is built for image classification. ousefficiencyandefficacystudiesofattackdefense
FedTADBench [195] offers three different mechanisms. As a result of these issues, security
datasetstoevaluatetimeseriesanomalydetection and privacy for ML remain an open problem.
algorithms.
6.2 Challenges for FL
6 Open Questions and
Challenges The main challenges in FL are two-fold [200]:
explainability and interpretability, and federated
GNNs. Explainability and interpretability refer
In this section, we summarize the challenges that
to the understanding of the contributions of the
MLandFLface.Weonlypresentmajorproblems.
clients or the data features. For instance, Shapley
This is because there is a large number of open
values are proposed [201] to quantify the impact
problems, and we choose to keep our presentation
of the features on the model output. Zheng et.
concise and focused.
al. propose a quantified ranking of features [202].
6.1 Challenges for Parallel and Similarly, there are studies [203] targeting verti-
cal FL. Several works introduce tailored measures
Distributed ML
of interpretability such as [204] defining a mea-
Themajorchallengeswithparallelanddistributed sure based on the gradients. However, in general,
ML are related to performance, fault-tolerance, the problem of explainability and interpretability
security and privacy [7, 196, 197]. remains open because i) ensuring privacy while
Typically, in distributed and parallel training, building explainable models is not trivial, ii) the
additional resources are used to decrease wall- aggregationofthelocalparametersobscuresinter-
clock time [198]. Such additional resources can pretability, iii) there is a lack of datasets that are
be multiple machines, multiple GPUs and high- not composed of images or text, and iv) there
end communication networks. As a result, the is a lack of a general framework for explainable
decrease in wall-clock time may not compensate federated models.
for the additional resources or their energy con- ResearchforFLwithGNNs[205,206,207]has
sumption. Therefore, research studies, such as recently started. For instance, FedGraphNN [205]
[16], are needed to investigate this trade-off with provides an FL benchmark system to evaluate
different applications and system architectures. various graph models, algorithms and datasets.
19
Another example is GraphFL [207] which is Performance Extreme Computing Confer-
designed to classify nodes on graphs. However, ence (HPEC), pages 1–12, 2020.
manyquestionsarestillwaitingtobesolved,such [4] Albert Reuther, Peter Michaleas, Michael
as the protection against malicious attacks, inter- Jones, Vijay Gadepally, Siddharth Samsi,
pretability, lack of modern graph neural frame- and Jeremy K˜epner. Survey and bench-
works for FL [208]. marking of machine learning accelerators.
In 2019 IEEE High Performance Extreme
7 Conclusions Computing Conference (HPEC), pages 1–9,
2019.
In this work, we provided a review of mod- [5] Sathwika Bavikadi, Abhijitt Dhavlle,
ern large-scale, parallel and distributed ML: the Amlan Ganguly, Anand Haridass, Hagar
state-of-the-art algorithms, optimization meth- Hendy, Cory Merkel, Vijay Janapa Reddi,
ods, types of parallelisms, communication topolo- PurabRanjanSutradhar,ArunJoseph,and
gies, synchronization models, and the existing Sai Manoj Pudukotai Dinakarrao. A survey
frameworks. Moreover, we reviewed FL. We dis- on machine learning accelerators and evolu-
cussed various aggregation algorithms in FL. In tionary hardware platforms. IEEE Design
addition, we reviewed the security and privacy & Test, 39(3):91–116, 2022.
aspects including various types of attacks and [6] MengWang,WeijieFu,XiangnanHe,Shijie
defense mechanisms. Moreover, we explored the Hao, and Xindong Wu. A survey on large-
existing FL frameworks and datasets. We con- scale machine learning. IEEE Transac-
cludedourstudywiththeopenresearchproblems tions on Knowledge and Data Engineering,
and challenges in large-scale distributed ML and 34(6):2574–2594, 2022.
FL. The major challenges are typically related [7] Joost Verbraeken, Matthijs Wolting,
to performance, security, privacy, explainability, Jonathan Katzy, Jeroen Kloppenburg, Tim
portability, and fault-tolerance. Verbelen, and Jan S. Rellermeyer. A sur-
vey on distributed machine learning. ACM
Acknowledgment Comput. Surv., 53(2), mar 2020.
[8] Lucy Ellen Lwakatare, Aiswarya Raj, Ivica
This work was supported by the U.S. DOE Office Crnkovic, Jan Bosch, and Helena Holm-
of Science, Office of Advanced Scientific Com- str¨om Olsson. Large-scale machine learning
puting Research, under award 66150: ”CENATE systems in real-world industrial settings: A
- Center for Advanced Architecture Evaluation” reviewofchallengesandsolutions. Informa-
project. The Pacific Northwest National Lab- tion and Software Technology, 127:106368,
oratory is operated by Battelle for the U.S. 2020.
Department of Energy under contract DE-AC05- [9] Brendan McMahan, Eider Moore, Daniel
76RL01830. Ramage, Seth Hampson, and Blaise Agu¨era
y Arcas. Communication-efficient learn-
References ing of deep networks from decentralized
data. In Aarti Singh and Xiaojin (Jerry)
[1] Shi Dong, Ping Wang, and Khushnood Zhu, editors, Proceedings of the 20th Inter-
Abbas. A survey on deep learning and national Conference on Artificial Intelli-
its applications. Computer Science Review, gence and Statistics, AISTATS 2017, 20-22
40:100379, 2021. April 2017, Fort Lauderdale, FL, USA, vol-
[2] Iqbal H Sarker. Deep learning: a compre- ume 54 of Proceedings of Machine Learning
hensive overview on techniques, taxonomy, Research, pages 1273–1282. PMLR, 2017.
applications and research directions. SN [10] Ji Liu, Jizhou Huang, Yang Zhou, Xuhong
Computer Science, 2(6):420, 2021. Li, Shilei Ji, Haoyi Xiong, and Dejing Dou.
[3] Albert Reuther, Peter Michaleas, Michael From distributed machine learning to feder-
Jones, Vijay Gadepally, Siddharth Samsi, ated learning: A survey. Knowl. Inf. Syst.,
and Jeremy Kepner. Survey of machine 64(4):885–917, apr 2022.
learning accelerators. In 2020 IEEE High
20
[11] Chen Zhang, Yu Xie, Hang Bai, Bin Yu, desirable criteria and future directions in
WeihongLi,andYuanGao.Asurveyonfed- communication and networking systems.
erated learning. Knowledge-Based Systems, IEEECommunicationsSurveys&Tutorials,
216:106775, 2021. 23(2):1342–1397, 2021.
[12] Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu [21] Yingxia Shao, Hongzheng Li, Xizhi Gu,
Hu, Naibo Wang, Yuan Li, Xu Liu, and HongboYin,YawenLi,XupengMiao,Wen-
Bingsheng He. A survey on federated learn- tao Zhang, Bin Cui, and Lei Chen. Dis-
ing systems: Vision, hype and reality for tributed graph neural network training: A
dataprivacyandprotection. IEEETransac- survey, 2022.
tions on Knowledge and Data Engineering, [22] Haozhao Wang, Zhihao Qu, Qihua Zhou,
35(4):3347–3366, 2023. Haobo Zhang, Boyuan Luo, Wenchao Xu,
[13] Dinh C. Nguyen, Ming Ding, Pubudu N. Song Guo, and Ruixuan Li. A compre-
Pathirana, Aruna Seneviratne, Jun Li, and hensive survey on training acceleration for
H. Vincent Poor. Federated learning for large machine learning models in iot. IEEE
internet of things: A comprehensive survey. Internet of Things Journal, 9(2):939–963,
IEEECommunicationsSurveys&Tutorials, 2022.
23(3):1622–1658, 2021. [23] Shuyan Hu, Xiaojing Chen, Wei Ni, Ekram
[14] Shaveta Dargan, Munish Kumar, Hossain, and Xin Wang. Distributed
Maruthi Rohit Ayyagari, and Gulshan machine learning for wireless communica-
Kumar. A survey of deep learning and its tion networks: Techniques, architectures,
applications: a new paradigm to machine and applications. IEEE Communications
learning. Archives of Computational Surveys & Tutorials,23(3):1458–1493,2021.
Methods in Engineering, 27:1071–1092, [24] Omar Nassef, Wenting Sun, Hakimeh
2020. Purmehdi, Mallik Tatipamula, and Toktam
[15] Iqbal H Sarker. Machine learning: Algo- Mahmoodi. A survey: Distributed machine
rithms,real-worldapplications and research learningfor5gandbeyond. Comput. Netw.,
directions. SN computer science, 2(3):160, 207(C), apr 2022.
2021. [25] Yahao Ding, Zhaohui Yang, Quoc-Viet
[16] TalBen-NunandTorstenHoefler. Demysti- Pham, Zhaoyang Zhang, and Mohammad
fyingparallelanddistributeddeeplearning: Shikh-Bahaei. Distributedmachinelearning
An in-depth concurrency analysis. ACM for uav swarms: Computing, sensing, and
Comput. Surv., 52(4), aug 2019. semantics, 2023.
[17] Yuhao Zhang, Frank McQuillan, Nandish [26] Viraaji Mothukuri, Reza M. Parizi,
Jayaram, Nikhil Kak, Ekta Khanna, Orhan Seyedamin Pouriyeh, Yan Huang, Ali
Kislal, Domino Valdano, and Arun Kumar. Dehghantanha, and Gautam Srivastava. A
Distributed deep learning on data systems: survey on security and privacy of federated
Acomparativeanalysisofapproaches. Proc. learning. Future Generation Computer
VLDB Endow.,14(10):1769–1782,jun2021. Systems, 115:619–640, 2021.
[18] Matthias Langer, Zhen He, Wenny Rahayu, [27] Youyang Qu, Md Palash Uddin, Chenquan
and Yanbo Xue. Distributed training of Gan,YongXiang,LongxiangGao,andJohn
deep learning models: A taxonomic per- Yearwood. Blockchain-enabled federated
spective. IEEE Transactions on Parallel learning: A survey. ACM Computing Sur-
and Distributed Systems, 31(12):2802–2818, veys, 55(4):1–35, 2022.
2020. [28] AhmedElOuadrhiriandAhmedAbdelhadi.
[19] Ruben Mayer and Hans-Arno Jacobsen. Differential privacy for deep and federated
Scalabledeeplearningondistributedinfras- learning: A survey. IEEE Access, 10:22359–
tructures:Challenges,techniques,andtools. 22380, 2022.
ACM Comput. Surv., 53(1), feb 2020. [29] Steven CH Hoi, Doyen Sahoo, Jing Lu, and
[20] Omar Abdel Wahab, Azzam Mourad, Hadi Peilin Zhao. Online learning: A comprehen-
Otrok, and Tarik Taleb. Federated machine sive survey. Neurocomputing, 459:249–289,
learning: Survey, multi-level classification, 2021.
21
[30] Marti A. Hearst, Susan T Dumais, Edgar Geir Kjetil Sandve, et al. Hopfield net-
Osuna,JohnPlatt,andBernhardScholkopf. works is all you need. arXiv preprint
Support vector machines. IEEE Intelligent arXiv:2008.02217, 2020.
Systems and their applications,13(4):18–28, [42] Tianyang Lin, Yuxin Wang, Xiangyang Liu,
1998. and Xipeng Qiu. A survey of transformers.
[31] Asifullah Khan, Anabia Sohail, Umme AI Open, 2022.
Zahoora,andAqsaSaeedQureshi. Asurvey [43] Ashish Vaswani, Noam Shazeer, Niki Par-
of the recent architectures of deep convo- mar, Jakob Uszkoreit, Llion Jones, Aidan N
lutional neural networks. Artificial intelli- Gomez,L(cid:32) ukaszKaiser,andIlliaPolosukhin.
gence review, 53(8):5455–5516, 2020. Attentionisallyouneed.Advancesinneural
[32] Yong Yu, Xiaosheng Si, Changhua Hu, and information processing systems, 30, 2017.
JianxunZhang.Areviewofrecurrentneural [44] OpenAI. Gpt-4 technical report, Accessed
networks: Lstm cells and network architec- on 03-29-2023.
tures.Neuralcomputation,31(7):1235–1270, [45] OpenAI. Chatgpt, Accessed on 03-29-2023.
2019. [46] SholomMWeissandNitinIndurkhya.Rule-
[33] Dor Bank, Noam Koenigstein, and Raja based machine learning methods for func-
Giryes. Autoencoders. arXiv preprint tionalprediction. Journal of Artificial Intel-
arXiv:2003.05991, 2020. ligence Research, 3:383–403, 1995.
[34] IanGoodfellow,JeanPouget-Abadie,Mehdi [47] Akbar Telikani, Amirhessam Tahmassebi,
Mirza,BingXu,DavidWarde-Farley,Sherjil Wolfgang Banzhaf, and Amir H. Gandomi.
Ozair,AaronCourville,andYoshuaBengio. Evolutionary machine learning: A survey.
Generativeadversarialnetworks. Communi- ACM Comput. Surv., 54(8), oct 2021.
cations of the ACM, 63(11):139–144, 2020. [48] Rubayyi Alghamdi and Khalid Alfalqi. A
[35] Zonghan Wu, Shirui Pan, Fengwen Chen, surveyoftopicmodelingintextmining. Int.
Guodong Long, Chengqi Zhang, and S Yu J. Adv. Comput. Sci. Appl.(IJACSA), 6(1),
Philip. A comprehensive survey on graph 2015.
neuralnetworks. IEEEtransactionsonneu- [49] Pooja Kherwa and Poonam Bansal. Topic
ral networks and learning systems, 32(1):4– modeling: A comprehensive review. EAI
24, 2020. EndorsedTransactionsonScalableInforma-
[36] Marc M Van Hulle. Self-organizing maps. tion Systems, 7(24), 7 2019.
Handbook of natural computing, 1:585–622, [50] FabianPedregosa,Ga¨elVaroquaux,Alexan-
2012. dre Gramfort, Vincent Michel, Bertrand
[37] Geoffrey E Hinton, Terrence J Sejnowski, Thirion, Olivier Grisel, Mathieu Blondel,
and David H Ackley. Boltzmann machines: Peter Prettenhofer, Ron Weiss, Vincent
Constraint satisfaction networks that learn. Dubourg, Jake Vanderplas, Alexandre Pas-
Carnegie-Mellon University, Department of sos, David Cournapeau, Matthieu Brucher,
Computer Science Pittsburgh, PA, 1984. Matthieu Perrot, and E´douard Duchesnay.
[38] Geoffrey E Hinton, Terrence J Sejnowski, Scikit-learn: Machine learning in python.
et al. Learning and relearning in boltzmann Journal of Machine Learning Research,
machines. Parallel distributed processing: 12(85):2825–2830, 2011.
Explorations in the microstructure of cogni- [51] Ian H. Witten, Eibe Frank, Mark A. Hall,
tion, 1(282-317):2, 1986. and Christopher J. Pal. Data Mining,
[39] Geoffrey E Hinton. Deep belief networks. Fourth Edition: Practical Machine Learning
Scholarpedia, 4(5):5947, 2009. Tools and Techniques. Morgan Kaufmann
[40] John J Hopfield. Hopfield network. Schol- Publishers Inc., San Francisco, CA, USA,
arpedia, 2(5):1977, 2007. 4th edition, 2016.
[41] Hubert Ramsauer, Bernhard Sch¨afl, [52] TianqiChenandCarlosGuestrin. Xgboost:
Johannes Lehner, Philipp Seidl, Michael Ascalabletreeboostingsystem. InProceed-
Widrich, Thomas Adler, Lukas Gruber, ings of the 22nd acm sigkdd international
Markus Holzleitner, Milena Pavlovi´c, conference on knowledge discovery and data
mining, pages 785–794, 2016.
22
[53] S¨oren Sonnenburg, Gunnar Ratsch, Sebas- Walter Wang, Eric Wilcox, and Doe Hyun
tian Henschel, Christian Widmer, Jonas Yoon. In-datacenter performance analy-
Behr, Alexander Zien, Fabio de Bona, sis of a tensor processing unit. SIGARCH
Alexander Binder, Christian Gehl, and Comput.Archit.News,45(2):1–12,jun2017.
Vojtech Franc. The shogun machine learn- [61] Rengan Xu, Frank Han, and Quy Ta. Deep
ing toolbox. Journal of Machine Learning learningatscaleonnvidiav100accelerators.
Research, 11(60):1799–1802, 2010. In 2018 IEEE/ACM Performance Model-
[54] Chih-Chung Chang and Chih-Jen Lin. Lib- ing, Benchmarking and Simulation of High
svm: a library for support vector machines. Performance Computer Systems (PMBS),
ACM transactions on intelligent systems pages 23–32, 2018.
and technology (TIST), 2(3):1–27, 2011. [62] Zhe Jia, Blake Tillman, Marco Maggioni,
[55] Google. Google cloud, Accessed on 03-27- andDanielePaoloScarpazza. Dissectingthe
2023. graphcoreIPUarchitectureviamicrobench-
[56] Microsoft. Microsoft azure, Accessed on 03- marking. CoRR, abs/1912.03413, 2019.
27-2023. [63] Murali Emani, Venkatram Vishwanath,
[57] Amazon.Machinelearningonaws,Accessed Corey Adams, Michael E. Papka, Rick
on 03-27-2023. Stevens, Laura Florescu, Sumti Jairath,
[58] IBM Watson. Ibm watson assistant, William Liu, Tejas Nama, and Arvind
Accessed on 03-27-2023. Sujeeth. Accelerating scientific applications
[59] SharanChetlur,CliffWoolley,PhilippeVan- with sambanova reconfigurable dataflow
dermersch, Jonathan Cohen, John Tran, architecture. Computing in Science & Engi-
Bryan Catanzaro, and Evan Shelhamer. neering, 23(2):114–119, 2021.
cudnn:Efficientprimitivesfordeeplearning. [64] Sanjiv Kumar, Mehryar Mohri, and Ameet
CoRR, abs/1410.0759, 2014. Talwalkar. Sampling methods for the
[60] Norman P. Jouppi, Cliff Young, Nishant nystr¨om method. The Journal of Machine
Patil, David Patterson, Gaurav Agrawal, Learning Research, 13(1):981–1006, 2012.
Raminder Bajwa, Sarah Bates, Suresh [65] Djallel Bouneffouf and Inanc Birol. Sam-
Bhatia, Nan Boden, Al Borchers, Rick plingwithminimumsumofsquaredsimilar-
Boyle, Pierre-luc Cantin, Clifford Chao, ities for nystrom-based large scale spectral
Chris Clark, Jeremy Coriell, Mike Daley, clustering.InIJCAI,pages2313–2319,2015.
Matt Dau, Jeffrey Dean, Ben Gelb, [66] Per-Gunnar Martinsson, Vladimir Rokhlin,
Tara Vazir Ghaemmaghami, Rajendra Got- and Mark Tygert. A randomized algo-
tipati, William Gulland, Robert Hagmann, rithm for the decomposition of matrices.
C. Richard Ho, Doug Hogberg, John Hu, AppliedandComputationalHarmonicAnal-
Robert Hundt, Dan Hurt, Julian Ibarz, ysis, 30(1):47–68, 2011.
Aaron Jaffey, Alek Jaworski, Alexander [67] Ali Rahimi and Benjamin Recht. Ran-
Kaplan, Harshit Khaitan, Daniel Killebrew, domfeaturesforlarge-scalekernelmachines.
Andy Koch, Naveen Kumar, Steve Lacy, Advances in neural information processing
James Laudon, James Law, Diemthu Le, systems, 20, 2007.
ChrisLeary,ZhuyuanLiu,KyleLucke,Alan [68] Jia Deng, Sanjeev Satheesh, Alexander
Lundin, Gordon MacKean, Adriana Mag- Berg, and Fei Li. Fast and balanced: Effi-
giore, Maire Mahony, Kieran Miller, Rahul cientlabeltreelearningforlargescaleobject
Nagarajan, Ravi Narayanaswami, Ray Ni, recognition. Advances in Neural Informa-
Kathy Nix, Thomas Norrie, Mark Omer- tion Processing Systems, 24, 2011.
nick, Narayana Penukonda, Andy Phelps, [69] Yael Ben-Haim and Elad Tom-Tov. A
Jonathan Ross, Matt Ross, Amir Salek, streaming parallel decision tree algorithm.
Emad Samadiani, Chris Severn, Gregory Journal of Machine Learning Research,
Sizikov,MatthewSnelham,JedSouter,Dan 11(2), 2010.
Steinberg,AndySwing,MercedesTan,Gre- [70] Wei Liu, Junfeng He, and Shih-Fu Chang.
gory Thorson, Bo Tian, Horia Toma, Erick Large graph construction for scalable semi-
Tuttle, Vijay Vasudevan, Richard Walter, supervised learning. In Proceedings of the
23
27th international conference on machine arXiv:1704.04861, 2017.
learning (ICML-10), pages 679–686. Cite- [79] Alex Krizhevsky, Ilya Sutskever, and Geof-
seer, 2010. frey E Hinton. Imagenet classification with
[71] Yan-Ming Zhang, Kaizhu Huang, Guang- deep convolutional neural networks. Com-
gang Geng, and Cheng-Lin Liu. Fast k nn munications of the ACM,60(6):84–90,2017.
graph construction with locality sensitive [80] SiddharthGopal. Adaptivesamplingforsgd
hashing. In Machine Learning and Knowl- by exploiting side information. In Inter-
edgeDiscoveryinDatabases:EuropeanCon- national Conference on Machine Learning,
ference, ECML PKDD 2013, Prague, Czech pages 364–372. PMLR, 2016.
Republic, September 23-27, 2013, Proceed- [81] Guillaume Alain, Alex Lamb, Chinnadhu-
ings, Part II 13, pages 660–674. Springer, rai Sankar, Aaron Courville, and Yoshua
2013. Bengio. Variance reduction in sgd by
[72] Jingdong Wang, Ting Zhang, Nicu Sebe, distributed importance sampling. arXiv
Heng Tao Shen, et al. A survey on learning preprint arXiv:1511.06481, 2015.
tohash. IEEE transactions on pattern anal- [82] Priya Goyal, Piotr Doll´ar, Ross Girshick,
ysis and machine intelligence, 40(4):769– PieterNoordhuis,LukaszWesolowski,Aapo
790, 2017. Kyrola, Andrew Tulloch, Yangqing Jia, and
[73] Meng Wang, Weijie Fu, Shijie Hao, KaimingHe. Accurate,largeminibatchsgd:
Hengchang Liu, and Xindong Wu. Learn- Trainingimagenetin1hour. arXiv preprint
ing on big graph: Label inference and arXiv:1706.02677, 2017.
regularization with anchor hierarchy. [83] Yoshua Bengio. Practical recommendations
IEEE transactions on knowledge and data forgradient-basedtrainingofdeeparchitec-
engineering, 29(5):1101–1114, 2017. tures. NeuralNetworks:TricksoftheTrade:
[74] MengWang,WeijieFu,ShijieHao,Dacheng Second Edition, pages 437–478, 2012.
Tao, and Xindong Wu. Scalable semi- [84] Sashank J Reddi, Satyen Kale, and Sanjiv
supervised learning by efficient anchor Kumar. On the convergence of adam and
graph regularization. IEEE Transac- beyond. arXiv preprint arXiv:1904.09237,
tions on Knowledge and Data Engineering, 2019.
28(7):1864–1877, 2016. [85] JohnDuchi,EladHazan,andYoramSinger.
[75] VinodNairandGeoffreyEHinton.Rectified Adaptive subgradient methods for online
linear units improve restricted boltzmann learning and stochastic optimization. Jour-
machines. In Proceedings of the 27th inter- nal of machine learning research, 12(7),
national conference on machine learning 2011.
(ICML-10), pages 807–814, 2010. [86] Matthew D Zeiler. Adadelta: an adap-
[76] Andrew L Maas, Awni Y Hannun, tive learning rate method. arXiv preprint
Andrew Y Ng, et al. Rectifier nonlinearities arXiv:1212.5701, 2012.
improve neural network acoustic models. [87] Ning Qian. On the momentum term in gra-
In Proc. icml, volume 30, page 3. Atlanta, dient descent learning algorithms. Neural
Georgia, USA, 2013. networks, 12(1):145–151, 1999.
[77] Shan Sung Liew, Mohamed Khalil-Hani, [88] Yu Nesterov. Gradient methods for mini-
and Rabia Bakhteri. Bounded activation mizing composite functions. Mathematical
functions for enhanced training stability programming, 140(1):125–161, 2013.
of deep neural networks on visual pat- [89] Shiliang Sun, Zehui Cao, Han Zhu, and
ternrecognitionproblems. Neurocomputing, Jing Zhao. A survey of optimization
216:718–734, 2016. methods from a machine learning perspec-
[78] Andrew G Howard, Menglong Zhu, tive. IEEE transactions on cybernetics,
Bo Chen, Dmitry Kalenichenko, Weijun 50(8):3668–3681, 2019.
Wang, Tobias Weyand, Marco Andreetto, [90] Quoc V Le, Jiquan Ngiam, Adam Coates,
and Hartwig Adam. Mobilenets: Effi- Abhik Lahiri, Bobby Prochnow, and
cient convolutional neural networks for Andrew Y Ng. On optimization methods
mobile vision applications. arXiv preprint fordeeplearning. InProceedings of the 28th
24
International Conference on International 101(suppl 1):5228–5235, 2004.
Conference on Machine Learning, pages [101] Sungjin Ahn, Anoop Korattikara, and Max
265–272, 2011. Welling. Bayesian posterior sampling via
[91] Immanuel Bayer, Xiangnan He, Bhargav stochastic gradient fisher scoring. arXiv
Kanagal, and Steffen Rendle. A generic preprint arXiv:1206.6380, 2012.
coordinate descent framework for learning [102] Frank Seide, Hao Fu, Jasha Droppo, Gang
from implicit feedback. In Proceedings of Li, and Dong Yu. 1-bit stochastic gradient
the 26th International Conference on World descent and its application to data-parallel
Wide Web, pages 1341–1350, 2017. distributed training of speech dnns. In
[92] Guo-XunYuan,Chia-HuaHo,andChih-Jen Fifteenth annual conference of the inter-
Lin. Recent advances of large-scale lin- national speech communication association,
ear classification. Proceedings of the IEEE, 2014.
100(9):2584–2603, 2012. [103] Dan Alistarh, Demjan Grubic, Jerry Li,
[93] Julie Nutini, Mark Schmidt, Issam Laradji, Ryota Tomioka, and Milan Vojnovic. Qsgd:
Michael Friedlander, and Hoyt Koepke. Communication-efficient sgd via gradient
Coordinate descent converges faster with quantization and encoding. Advances in
the gauss-southwell rule than random neural information processing systems, 30,
selection. In International Conference 2017.
on Machine Learning, pages 1632–1641. [104] Wei Wen, Cong Xu, Feng Yan, Chunpeng
PMLR, 2015. Wu, Yandan Wang, Yiran Chen, and Hai
[94] Yu Nesterov. Efficiency of coordinate Li. Terngrad: Ternary gradients to reduce
descentmethodsonhuge-scaleoptimization communicationindistributeddeeplearning.
problems. SIAM Journal on Optimization, Advances in neural information processing
22(2):341–362, 2012. systems, 30, 2017.
[95] Hao-Jun Michael Shi, Shenyinying Tu, [105] YujunLin,SongHan,HuiziMao,YuWang,
Yangyang Xu, and Wotao Yin. A primer and William J Dally. Deep gradient
on coordinate descent algorithms. arXiv compression: Reducing the communication
preprint arXiv:1610.00040, 2016. bandwidth for distributed training. arXiv
[96] Sangwoon Yun and Kim-Chuan Toh. A preprint arXiv:1712.01887, 2017.
coordinate gradient descent method for l1- [106] Hantian Zhang, Jerry Li, Kaan Kara, Dan
regularized convex minimization. Com- Alistarh, Ji Liu, and Ce Zhang. Zipml:
putational Optimization and Applications, Training linear models with end-to-end low
48(2):273–307, 2011. precision, and a little bit of deep learning.
[97] Huan Li and Zhouchen Lin. Accelerated In International Conference on Machine
proximal gradient methods for nonconvex Learning, pages 4035–4043. PMLR, 2017.
programming. Advances in neural informa- [107] Qirong Ho, James Cipar, Henggang Cui,
tion processing systems, 28, 2015. Seunghak Lee, Jin Kyu Kim, Phillip B Gib-
[98] StephenBoyd,NealParikh,EricChu,Borja bons, Garth A Gibson, Greg Ganger, and
Peleato, Jonathan Eckstein, et al. Dis- Eric P Xing. More effective distributed
tributed optimization and statistical learn- ml via a stale synchronous parallel parame-
ing via the alternating direction method of ter server. Advances in neural information
multipliers. Foundations and Trends® in processing systems, 26, 2013.
Machine learning, 3(1):1–122, 2011. [108] ShuxinZheng,QiMeng,TaifengWang,Wei
[99] Siddhartha Chib and Edward Green- Chen, Nenghai Yu, Zhi-Ming Ma, and Tie-
berg. Understanding the metropolis- Yan Liu. Asynchronous stochastic gradient
hastings algorithm. The american statisti- descent with delay compensation. In Inter-
cian, 49(4):327–335, 1995. national Conference on Machine Learning,
[100] Thomas L Griffiths and Mark Steyvers. pages 4120–4129. PMLR, 2017.
Finding scientific topics. Proceedings [109] Sixin Zhang, Anna E Choromanska, and
of the National academy of Sciences, Yann LeCun. Deep learning with elastic
25
averaging sgd. Advances in neural informa- Desmaison, Andreas Kopf, Edward Yang,
tion processing systems, 28, 2015. Zachary DeVito, Martin Raison, Alykhan
[110] Brendan McMahan and Matthew Streeter. Tejani, Sasank Chilamkurthy, Benoit
Delay-tolerant algorithms for asynchronous Steiner, Lu Fang, Junjie Bai, and Soumith
distributed online learning. Advances in Chintala. Pytorch: An imperative style,
Neural Information Processing Systems, 27, high-performance deep learning library. In
2014. Advances in Neural Information Process-
[111] Leslie G Valiant. A bridging model for par- ing Systems 32, pages 8024–8035. Curran
allel computation. Communications of the Associates, Inc., 2019.
ACM, 33(8):103–111, 1990. [118] PyTorch. Pytorch distributed overview.
[112] Kevin Hsieh, Aaron Harlap, Nandita Available at https://pytorch.org/tutorials/
Vijaykumar, Dimitris Konomis, Gregory R beginner/dist overview.html, Last accessed
Ganger, Phillip B Gibbons, and Onur 09.01.2022.
Mutlu. Gaia: Geo-distributed machine [119] Tianqi Chen, Mu Li, Yutian Li, Min
learning approaching lan speeds. In NSDI, Lin, Naiyan Wang, Minjie Wang, Tian-
pages 629–647, 2017. jun Xiao, Bing Xu, Chiyuan Zhang, and
[113] ChristopherDeSa,CeZhang,KunleOluko- Zheng Zhang. Mxnet: A flexible and
tun, and Christopher R´e. Taming the wild: efficient machine learning library for het-
A unified analysis of hogwild!-style algo- erogeneous distributed systems. CoRR,
rithms, 2015. abs/1512.01274, 2015.
[114] Mart´ın Abadi, Ashish Agarwal, Paul [120] AmithRMamidala,GeorgiosKollias,Chris
Barham, Eugene Brevdo, Zhifeng Chen, Ward, and Fausto Artico. Mxnet-mpi:
Craig Citro, Greg S. Corrado, Andy Davis, Embedding mpi parallelism in parameter
Jeffrey Dean, Matthieu Devin, Sanjay Ghe- server task model for scaling deep learning,
mawat, Ian Goodfellow, Andrew Harp, 2018.
Geoffrey Irving, Michael Isard, Yangqing [121] Alexander Sergeev and Mike Del Balso.
Jia, Rafal Jozefowicz, Lukasz Kaiser, Horovod: fast and easy distributed
Manjunath Kudlur, Josh Levenberg, Dan- deep learning in tensorflow. CoRR,
delion Man´e, Rajat Monga, Sherry Moore, abs/1802.05799, 2018.
Derek Murray, Chris Olah, Mike Schus- [122] AndrewGibiansky. Bringinghpctechniques
ter, Jonathon Shlens, Benoit Steiner, Ilya to deep learning. Baidu Research, Tech.
Sutskever, Kunal Talwar, Paul Tucker, Rep., 2017.
Vincent Vanhoucke, Vijay Vasudevan, Fer- [123] Elias De Coninck, Steven Bohez, Sam Ler-
nanda Vi´egas, Oriol Vinyals, Pete Warden, oux, Tim Verbelen, Bert Vankeirsbilck,
Martin Wattenberg, Martin Wicke, Yuan Pieter Simoens, and Bart Dhoedt. Dianne:
Yu, and Xiaoqiang Zheng. TensorFlow: a modular framework for designing, train-
Large-scale machine learning on heteroge- ing and deploying deep neural networks
neous systems, 2015. Software available onheterogeneousdistributedinfrastructure.
from tensorflow.org. JournalofSystemsandSoftware,141:52–65,
[115] Tensorflow. Distributed training with 2018.
tensorflow. Available at https://www. [124] Frank Seide and Amit Agarwal. Cntk:
tensorflow.org/guide/distributed training, Microsoft’s open-source deep-learning
Last accessed 09.01.2022. toolkit. In Proceedings of the 22nd ACM
[116] Keras. Distributed training with keras. SIGKDD International Conference on
Available at https://www.tensorflow.org/ Knowledge Discovery and Data Mining,
tutorials/distribute/keras, Last accessed KDD ’16, page 2135, New York, NY,
09.01.2022. USA, 2016. Association for Computing
[117] Adam Paszke, Sam Gross, Francisco Massa, Machinery.
Adam Lerer, James Bradbury, Gregory [125] James Bergstra, Olivier Breuleux, Fr´ed´eric
Chanan, Trevor Killeen, Zeming Lin, Bastien, Pascal Lamblin, Razvan Pascanu,
Natalia Gimelshein, Luca Antiga, Alban GuillaumeDesjardins,JosephTurian,David
26
Warde-farley, and Yoshua Bengio. Theano: [135] Mehryar Mohri, Gary Sivek, and
A cpu and gpu math compiler in python. Ananda Theertha Suresh. Agnostic
In Proceedings of the 9th Python in Science federated learning, 2019.
Conference, pages 3–10, 2010. [136] Zeou Hu, Kiarash Shaloudegi, Guojun
[126] Matei Zaharia, Reynold S Xin, Patrick Zhang,andYaoliangYu. Federatedlearning
Wendell, Tathagata Das, Michael Arm- meets multi-objective optimization, 2020.
brust, Ankur Dave, Xiangrui Meng, Josh [137] Sashank Reddi, Zachary Charles, Manzil
Rosen, Shivaram Venkataraman, Michael J Zaheer,ZacharyGarrett,KeithRush,Jakub
Franklin, et al. Apache spark: a unified Koneˇcny`, Sanjiv Kumar, and H Brendan
engineforbigdataprocessing. Communica- McMahan. Adaptive federated optimiza-
tions of the ACM, 59(11):56–65, 2016. tion. arXiv preprint arXiv:2003.00295,
[127] Apache Software Foundation. Hadoop. 2020.
[128] JeffreyDeanandSanjayGhemawat.Mapre- [138] Sai Praneeth Karimireddy, Martin
duce: simplified data processing on large Jaggi, Satyen Kale, Mehryar Mohri,
clusters. Communications of the ACM, Sashank Reddi, Sebastian U Stich, and
51(1):107–113, 2008. AnandaTheerthaSuresh. Breakingthecen-
[129] Xiangrui Meng, Joseph Bradley, Burak tralized barrier for cross-device federated
Yavuz, Evan Sparks, Shivaram Venkatara- learning. Advances in Neural Information
man,DaviesLiu,JeremyFreeman,DBTsai, Processing Systems, 34:28663–28676, 2021.
Manish Amde, Sean Owen, Doris Xin, [139] Tian Li, Anit Kumar Sahu, Manzil Zaheer,
Reynold Xin, Michael J. Franklin, Reza Maziar Sanjabi, Ameet Talwalkar, and Vir-
Zadeh, Matei Zaharia, and Ameet Tal- ginia Smith. Federated optimization in
walkar. Mllib: Machine learning in apache heterogeneous networks, 2018.
spark. Journal of Machine Learning [140] Sai Praneeth Karimireddy, Satyen Kale,
Research, 17(34):1–7, 2016. Mehryar Mohri, Sashank Reddi, Sebastian
[130] Qiang Yang, Yang Liu, Tianjian Chen, and Stich,andAnandaTheerthaSuresh. SCAF-
Yongxin Tong. Federated machine learning: FOLD: Stochastic controlled averaging for
Concept and applications. ACM Transac- federated learning. In Hal Daum´e III
tions on Intelligent Systems and Technology and Aarti Singh, editors, Proceedings of the
(TIST), 10(2):1–19, 2019. 37th International Conference on Machine
[131] Yang Liu, Yan Kang, Chaoping Xing, Tian- Learning, volume 119 of Proceedings of
jian Chen, and Qiang Yang. A secure fed- Machine Learning Research, pages 5132–
erated transfer learning framework. IEEE 5143. PMLR, 13–18 Jul 2020.
Intelligent Systems, 35(4):70–82, 2020. [141] Shaoxiong Ji, Shirui Pan, Guodong Long,
[132] Yang Liu, Xinwei Zhang, Yan Kang, Liping Xue Li, Jing Jiang, and Zi Huang. Learn-
Li,TianjianChen,MingyiHong,andQiang ing private neural language modeling with
Yang. Fedbcd: A communication-efficient attentive aggregation. In 2019 Interna-
collaborative learning framework for dis- tional Joint Conference on Neural Networks
tributed features. IEEE Transactions on (IJCNN), pages 1–8, 2019.
Signal Processing, 70:4277–4290, 2022. [142] Jianyu Wang, Qinghua Liu, Hao Liang,
[133] Muhammad Asad, Ahmed Moustafa, and Gauri Joshi, and H Vincent Poor. Tack-
Takayuki Ito. Fedopt: Towards communi- ling the objective inconsistency problem
cation efficiency and privacy preservation in heterogeneous federated optimization.
in federated learning. Applied Sciences, Advances in neural information processing
10(8):2864, 2020. systems, 33:7611–7623, 2020.
[134] Chaoyang He, Murali Annavaram, and [143] Yuyang Deng, Mohammad Mahdi Kamani,
Salman Avestimehr. Group knowledge and Mehrdad Mahdavi. Adaptive person-
transfer: Federated learning of large cnns at alized federated learning. arXiv preprint
the edge. Advances in Neural Information arXiv:2003.13461, 2020.
Processing Systems, 33:14068–14080, 2020. [144] Aviv Shamsian, Aviv Navon, Ethan Fetaya,
and Gal Chechik. Personalized federated
27
learning using hypernetworks. In Inter- [154] Kang Wei, Jun Li, Ming Ding, Chuan Ma,
national Conference on Machine Learning, Howard H Yang, Farhad Farokhi, Shi Jin,
pages 9489–9502. PMLR, 2021. Tony QS Quek, and H Vincent Poor. Fed-
[145] Alysa Ziying Tan, Han Yu, Lizhen Cui, and erated learning with differential privacy:
Qiang Yang. Towards personalized feder- Algorithmsandperformanceanalysis. IEEE
atedlearning. IEEETransactionsonNeural Transactions on Information Forensics and
Networks and Learning Systems, 2022. Security, 15:3454–3469, 2020.
[146] Mikhail Yurochkin, Mayank Agarwal, [155] Stacey Truex, Ling Liu, Ka-Ho Chow,
Soumya Ghosh, Kristjan Greenewald, and MehmetEmreGursoy,andWenqiWei.Ldp-
Nghia Hoang. Statistical model aggregation fed: Federated learning with local differen-
viaparametermatching. Advancesinneural tial privacy. In Proceedings of the Third
information processing systems, 32, 2019. ACM International Workshop on Edge Sys-
[147] Mikhail Yurochkin, Mayank Agarwal, tems, Analytics and Networking, pages 61–
SoumyaGhosh,KristjanGreenewald,Nghia 66, 2020.
Hoang, and Yasaman Khazaeni. Bayesian [156] ValeTolpegin,StaceyTruex,MehmetEmre
nonparametric federated learning of neural Gursoy, and Ling Liu. Data poisoning
networks. In International conference on attacks against federated learning systems.
machine learning, pages 7252–7261. PMLR, InComputer Security–ESORICS 2020: 25th
2019. European Symposium on Research in Com-
[148] Istv´an Hegedu˝s, G´abor Danner, and M´ark puter Security, ESORICS 2020, Guildford,
Jelasity. Decentralized learning works: An UK, September 14–18, 2020, Proceedings,
empirical comparison of gossip learning and Part I 25, pages 480–501. Springer, 2020.
federated learning. Journal of Parallel and [157] Florian Nuding and Rudolf Mayer. Data
Distributed Computing, 148:109–124, 2021. poisoning in sequential and parallel feder-
[149] Hao Ye, Le Liang, and Geoffrey Ye Li. ated learning. In Proceedings of the 2022
Decentralized federated learning with unre- ACM on International Workshop on Secu-
liable communications. IEEE Journal rityandPrivacyAnalytics,IWSPA’22,page
of Selected Topics in Signal Processing, 24–34, 2022.
16(3):487–500, 2022. [158] Gan Sun, Yang Cong, Jiahua Dong, Qiang
[150] Peva Blanchard, El Mahdi El Mhamdi, Wang, Lingjuan Lyu, and Ji Liu. Data poi-
Rachid Guerraoui, and Julien Stainer. soning attacks on federated machine learn-
Machine learning with adversaries: Byzan- ing. IEEE Internet of Things Journal,
tine tolerant gradient descent. Advances in 9(13):11365–11375, 2022.
neural information processing systems, 30, [159] Ashwinee Panda, Saeed Mahloujifar,
2017. Arjun Nitin Bhagoji, Supriyo Chakraborty,
[151] Leslie Lamport, Robert Shostak, and Mar- and Prateek Mittal. Sparsefed: Mitigating
shall Pease. The byzantine generals prob- model poisoning attacks in federated learn-
lem. In Concurrency: the works of leslie ing with sparsification. In International
lamport, pages 203–226. ACM New York, Conference on Artificial Intelligence and
NY, USA, 2019. Statistics, pages 7587–7624. PMLR, 2022.
[152] Cynthia Dwork. Differential privacy. In [160] Xiaoyu Cao and Neil Zhenqiang Gong.
Automata, Languages and Programming: Mpaf: Model poisoning attacks to federated
33rd International Colloquium, ICALP learning based on fake clients. In Pro-
2006, Venice, Italy, July 10-14, 2006, Pro- ceedings of the IEEE/CVF Conference on
ceedings, Part II 33, pages 1–12. Springer, Computer Vision and Pattern Recognition,
2006. pages 3396–3404, 2022.
[153] Cynthia Dwork, Aaron Roth, et al. The [161] Zhuoran Ma, Jianfeng Ma, Yinbin Miao,
algorithmic foundations of differential pri- Yingjiu Li, and Robert H. Deng. Shieldfl:
vacy. Foundations and Trends® in The- Mitigating model poisoning attacks in
oretical Computer Science, 9(3–4):211–407, privacy-preservingfederatedlearning. IEEE
2014. Transactions on Information Forensics and
28
Security, 17:1639–1654, 2022. systems, applications, and services, pages
[162] Milad Nasr, Reza Shokri, and Amir 94–108, 2021.
Houmansadr. Comprehensive privacy anal- [170] Yu Chen, Fang Luo, Tong Li, Tao Xiang,
ysis of deep learning: Passive and active Zheli Liu, and Jin Li. A training-
white-box inference attacks against central- integrityprivacy-preservingfederatedlearn-
ized and federated learning. In 2019 IEEE ing scheme with trusted execution envi-
symposium on security and privacy (SP), ronment. Information Sciences, 522:69–79,
pages 739–753. IEEE, 2019. 2020.
[163] Pengrui Liu, Xiangrui Xu, and Wei Wang. [171] Vaikkunth Mugunthan, Antigoni Polychro-
Threats, attacks and defenses to federated niadou, David Byrd, and Tucker Hybinette
learning:issues,taxonomyandperspectives. Balch. Smpai: Secure multi-party computa-
Cybersecurity, 5(1):1–19, 2022. tionforfederatedlearning. InProceedingsof
[164] ChengliangZhang,SuyiLi,JunzheXia,Wei the NeurIPS 2019 Workshop on Robust AI
Wang,FengYan,andYangLiu.Batchcrypt: in Financial Services, 2019.
Efficient homomorphic encryption for cross- [172] Yunlong Lu, Xiaohong Huang, Yueyue
silofederatedlearning. InProceedings of the Dai, Sabita Maharjan, and Yan Zhang.
2020USENIXAnnualTechnicalConference Blockchain and federated learning for
(USENIX ATC 2020), 2020. privacy-preserved data sharing in industrial
[165] Stephen Hardy, Wilko Henecka, Hamish iot. IEEE Transactions on Industrial Infor-
Ivey-Law, Richard Nock, Giorgio Patrini, matics, 16(6):4177–4186, 2019.
Guillaume Smith, and Brian Thorne. Pri- [173] Muhammad Saad, Jeffrey Spaulding, Lau-
vate federated learning on vertically parti- rent Njilla, Charles Kamhoua, Sachin
tioned data via entity resolution and addi- Shetty, DaeHun Nyang, and David
tively homomorphic encryption. arXiv Mohaisen. Exploring the attack surface of
preprint arXiv:1711.10677, 2017. blockchain: A comprehensive survey. IEEE
[166] Poongodi Manoharan, Ranjan Walia, Communications Surveys & Tutorials,
Celestine Iwendi, Tariq Ahamed Ahanger, 22(3):1977–2008, 2020.
ST Suganthi, MM Kamruzzaman, Sami [174] Yourong Chen, Hao Chen, Yang Zhang,
Bourouis, Wajdi Alhakami, and Mounir Meng Han, Madhuri Siddula, and Zhipeng
Hamdi. Svm-based generative adverserial Cai. A survey on blockchain systems:
networks for federated learning and edge Attacks, defenses, and privacy preservation.
computing attack model and outpoising. High-Confidence Computing, 2(2):100048,
Expert Systems, page e13072, 2022. 2022.
[167] Jiale Zhang, Bing Chen, Xiang Cheng, [175] Olivia Choudhury, Aris Gkoulalas-Divanis,
Huynh Thi Thanh Binh, and Shui Yu. Theodoros Salonidis, Issa Sylla, Yoonyoung
Poisongan: Generative poisoning attacks Park,GraceHsu,andAmarDas.Anonymiz-
against federated learning in edge comput- ing data for privacy-preserving federated
ing systems. IEEE Internet of Things learning. arXiv preprint arXiv:2002.09096,
Journal, 8(5):3310–3322, 2020. 2020.
[168] Ishai Rosenberg, Asaf Shabtai, Yuval [176] Suyi Li, Yong Cheng, Yang Liu, Wei Wang,
Elovici, and Lior Rokach. Adversarial andTianjianChen. Abnormalclientbehav-
machine learning attacks and defense meth- ior detection in federated learning. arXiv
ods in the cyber security domain. ACM preprint arXiv:1910.09933, 2019.
Computing Surveys (CSUR), 54(5):1–36, [177] Eugene Bagdasaryan, Andreas Veit,
2021. Yiqing Hua, Deborah Estrin, and Vitaly
[169] Fan Mo, Hamed Haddadi, Kleomenis Kat- Shmatikov. How to backdoor federated
evas, Eduard Marin, Diego Perino, and learning. In International Conference on
Nicolas Kourtellis. Ppfl: privacy-preserving Artificial Intelligence and Statistics, pages
federated learning with trusted execution 2938–2948. PMLR, 2020.
environments. In Proceedings of the 19th [178] Xueluan Gong, Yanjiao Chen, Qian Wang,
annual international conference on mobile and Weihan Kong. Backdoor attacks and
29
defenses in federated learning: State-of-the- Li, Andriy Myronenko, Dong Yang, Sean
art, taxonomy, and future directions. IEEE Yang,NicolaRieke,AboodQuraini,Chester
Wireless Communications, 2022. Chen, Daguang Xu, Nic Ma, Prerna Dogra,
[179] Maria Rigaki and Sebastian Garcia. A sur- Mona Flores, and Andrew Feng. Nvidia
vey of privacy attacks in machine learning. flare: Federated learning from simulation to
arXiv preprint arXiv:2007.07646, 2020. real-world, 2022.
[180] Micah Goldblum, Dimitris Tsipras, Chulin [187] Chaoyang He, Songze Li, Jinhyun So,
Xie,XinyunChen,AviSchwarzschild,Dawn Mi Zhang, Hongyi Wang, Xiaoyang Wang,
Song, Aleksander Madry, Bo Li, and Tom Praneeth Vepakomma, Abhishek Singh,
Goldstein. Dataset security for machine Hang Qiu, Li Shen, Peilin Zhao, Yan Kang,
learning: Data poisoning, backdoor attacks, Yang Liu, Ramesh Raskar, Qiang Yang,
and defenses. IEEE Transactions on Pat- Murali Annavaram, and Salman Aves-
tern Analysis and Machine Intelligence, timehr. Fedml: A research library and
45(2):1563–1580, 2022. benchmark for federated machine learning.
[181] Ying Zhao, Junjun Chen, Jiale Zhang, Advances in Neural Information Process-
Di Wu, Jian Teng, and Shui Yu. Pdgan: ing Systems, Best Paper Award at Federate
A novel poisoning defense method in fed- Learning Workshop, 2020.
erated learning using generative adversarial [188] YangLiu,TaoFan,TianjianChen,QianXu,
network. In Algorithms and Architectures and Qiang Yang. Fate: An industrial grade
for Parallel Processing: 19th International platformforcollaborativelearningwithdata
Conference, ICA3PP, page 595–609, 2019. protection. J. Mach. Learn. Res., 22(1), jul
[182] Caroline Fontaine and Fabien Galand. A 2022.
survey of homomorphic encryption for non- [189] Alexander Ziller, Andrew Trask, Antonio
specialists. EURASIP Journal on Informa- Lopardo, Benjamin Szymkow, Bobby Wag-
tion Security, 2007:1–10, 2007. ner, Emma Bluemke, Jean-Mickael Nouna-
[183] Tensorflow. Federated learning. Avail- hon, Jonathan Passerat-Palmbach, Kri-
able at https://www.tensorflow.org/ tika Prakash, Nick Rose, Th´eo Ryffel,
federated/federated learning, Last accessed ZarreenNaowalReza,andGeorgiosKaissis.
09.01.2022. PySyft:ALibraryforEasyFederatedLearn-
[184] Keras. Federated learning for image ing, pages 111–139. Springer International
classification. Available at https: Publishing, Cham, 2021.
//www.tensorflow.org/federated/tutorials/ [190] Patrick Foley, Micah J Sheller, Brandon
federated learning for image classification, Edwards, Sarthak Pati, Walter Riviera,
Last accessed 09.01.2022. Mansi Sharma, Prakash Narayana Moor-
[185] Heiko Ludwig, Nathalie Baracaldo, Gegi thy, Shi-han Wang, Jason Martin, Parsa
Thomas, Yi Zhou, Ali Anwar, Shashank Mirhaji, Prashant Shah, and Spyridon
Rajamoni, Yuya Jeremy Ong, Jayaram Bakas. Openfl: the open federated learn-
Radhakrishnan, Ashish Verma, Math- ing library. Physics in Medicine & Biology,
ieu Sinn, Mark Purcell, Ambrish Rawat, 2022.
Tran Ngoc Minh, Naoise Holohan, Supriyo [191] Sebastian Caldas, Sai Meher Karthik
Chakraborty, Shalisha Witherspoon, Dean Duddu,PeterWu,TianLi,JakubKoneˇcny´,
Steuer, Laura Wynter, Hifaz Hassan, Sean H. Brendan McMahan, Virginia Smith, and
Laguna, Mikhail Yurochkin, Mayank Agar- Ameet Talwalkar. Leaf: A benchmark for
wal, Ebube Chuba, and Annie Abay. IBM federated settings, 2018.
federated learning: an enterprise framework [192] Tensorflow. Datasets for running ten-
white paper V0.1. CoRR, abs/2007.10987, sorflow federated simulations. Available
2020. at https://www.tensorflow.org/federated/
[186] Holger R. Roth, Yan Cheng, Yuhong Wen, api docs/python/tff/simulation/datasets,
Isaac Yang, Ziyue Xu, Yuan-Ting Hsieh, Last accessed 02.21.2022.
Kristopher Kersten, Ahmed Harouni, Can [193] Jiahuan Luo, Xueyang Wu, Yun Luo, Anbu
Zhao, Kevin Lu, Zhihong Zhang, Wenqi Huang, Yunfeng Huang, Yang Liu, and
30
Qiang Yang. Real-world image datasets for Song, Weikang Song, Sebastian U. Stich,
federated learning, 2019. Ziteng Sun, Ananda Theertha Suresh, Flo-
[194] Rajesh Kumar, Abdullah Aman Khan, Jay rian Tram`er, Praneeth Vepakomma, Jianyu
Kumar, Noorbakhsh Amiri Golilarz, Simin Wang, Li Xiong, Zheng Xu, Qiang Yang,
Zhang, Yang Ting, Chengyu Zheng, Weny- Felix X. Yu, Han Yu, and Sen Zhao.
ong Wang, et al. Blockchain-federated- Advances and open problems in federated
learninganddeeplearningmodelsforcovid- learning. CoRR, abs/1912.04977, 2019.
19detectionusingctimaging.IEEESensors [201] Guan Wang. Interpret federated learn-
Journal, 21(14):16301–16314, 2021. ing with shapley values. arXiv preprint
[195] FanxingLiu,ChengZeng,LeZhang,Yingjie arXiv:1905.04519, 2019.
Zhou, Qing Mu, Yanru Zhang, Ling Zhang, [202] FanglanZheng,KunLi,JiangTian,Xiaojia
and Ce Zhu. Fedtadbench: Federated time- Xiang, et al. A vertical federated learning
series anomaly detection benchmark, 2022. method for interpretable scorecard and its
[196] Nikolaos Pitropakis, Emmanouil Panaousis, application in credit scoring. arXiv preprint
Thanassis Giannetsos, Eleftherios Anas- arXiv:2009.06218, 2020.
tasiadis, and George Loukas. A taxonomy [203] Xiaolin Chen, Shuai Zhou, Bei Guan, Kai
andsurveyofattacksagainstmachinelearn- Yang,HaoFao,HuWang,andYongjiWang.
ing. Computer Science Review, 34:100199, Fed-eini:Anefficientandinterpretableinfer-
2019. ence framework for decision tree ensembles
[197] Mingfu Xue, Chengxiang Yuan, Heyi Wu, inverticalfederatedlearning. In2021 IEEE
Yushu Zhang, and Weiqiang Liu. Machine international conference on big data (big
learningsecurity:Threats,countermeasures, data), pages 1242–1248. IEEE, 2021.
and evaluations. IEEE Access, 8:74720– [204] Zhe Li, Honglong Chen, Zhichen Ni, and
74742, 2020. Huajie Shao. Balancing privacy protection
[198] SalemAlqahtaniandMuratDemirbas. Per- and interpretability in federated learning.
formance analysis and comparison of dis- arXiv preprint arXiv:2302.08044, 2023.
tributed machine learning systems. arXiv [205] Chaoyang He, Keshav Balasubramanian,
preprint arXiv:1909.02061, 2019. Emir Ceyani, Carl Yang, Han Xie, Lichao
[199] Aurick Qiao, Bryon Aragam, Bingjing Sun, Lifang He, Liangwei Yang, Philip S
Zhang, and Eric Xing. Fault tolerance Yu, Yu Rong, et al. Fedgraphnn: A fed-
in iterative-convergent machine learning. erated learning system and benchmark for
In International Conference on Machine graph neural networks. arXiv preprint
Learning, pages 5220–5230. PMLR, 2019. arXiv:2104.07145, 2021.
[200] Peter Kairouz, H. Brendan McMahan, [206] Chaoyang He, Emir Ceyani, Keshav Bal-
Brendan Avent, Aur´elien Bellet, Mehdi asubramanian, Murali Annavaram, and
Bennis, Arjun Nitin Bhagoji, Kallista A. Salman Avestimehr. Spreadgnn: Server-
Bonawitz, Zachary Charles, Graham Cor- less multi-task federated learning for
mode, Rachel Cummings, Rafael G. L. graph neural networks. arXiv preprint
D’Oliveira, Salim El Rouayheb, David arXiv:2106.02743, 2021.
Evans, Josh Gardner, Zachary Garrett, [207] Binghui Wang, Ang Li, Meng Pang, Hai
Adri`a Gasc´on, Badih Ghazi, Phillip B. Li, and Yiran Chen. Graphfl: A feder-
Gibbons, Marco Gruteser, Za¨ıd Harchaoui, atedlearningframeworkforsemi-supervised
Chaoyang He, Lie He, Zhouyuan Huo, nodeclassificationongraphs. In2022 IEEE
Ben Hutchinson, Justin Hsu, Martin Jaggi, International Conference on Data Mining
Tara Javidi, Gauri Joshi, Mikhail Khodak, (ICDM), pages 498–507. IEEE, 2022.
JakubKoneˇcny´,AleksandraKorolova,Fari- [208] RuiLiuandHanYu.Federatedgraphneural
naz Koushanfar, Sanmi Koyejo, Tancr`ede networks: Overview, techniques and chal-
Lepoint,YangLiu,PrateekMittal,Mehryar lenges. arXiv preprint arXiv:2202.07256,
Mohri, Richard Nock, Ayfer O¨zgu¨r, Ras- 2022.
mus Pagh, Mariana Raykova, Hang Qi,
Daniel Ramage, Ramesh Raskar, Dawn
31
