TOWARDS MODULAR MACHINE LEARNING SOLUTION DEVELOPMENT:
BENEFITS AND TRADE-OFFS
SamiyuruMenik1 LakshmishRamaswamy1
ABSTRACT
Machinelearningtechnologieshavedemonstratedimmensecapabilitiesinvariousdomains. Theyplayakey
role in the success of modern businesses. However, adoption of machine learning technologies has a lot of
untouchedpotential. Costofdevelopingcustommachinelearningsolutionsthatsolveuniquebusinessproblems
isamajorinhibitortofar-reachingadoptionofmachinelearningtechnologies. Werecognizethatthemonolithic
nature prevalent in today’s machine learning applications stands in the way of efficient and cost effective
customizedmachinelearningsolutiondevelopment. Inthisworkweexplorethebenefitsofmodularmachine
learningsolutionsanddiscusshowmodularmachinelearningsolutionscanovercomesomeofthemajorsolution
engineeringlimitationsofmonolithicmachinelearningsolutions. Weanalyzethetrade-offsbetweenmodularand
monolithicmachinelearningsolutionsthroughthreedeeplearningproblems;onetextbasedandthetwoimage
based. Ourexperimentalresultsshowthatmodularmachinelearningsolutionshaveapromisingpotentialtoreap
thesolutionengineeringadvantagesofmodularitywhilegainingperformanceanddataadvantagesinawaythe
monolithicmachinelearningsolutionsdonotpermit.
1 INTRODUCTION nologiesaccessibletoawiderrangeoforganizationsand
differentorganizationallevelsandindividualsthatoperate
Machinelearning(ML)hasgainedalotofattentionoverthe
in a wide range of domains is one of the big challenges
pastyears. Machinelearningtechnologieshavebecomea
thatMLasafieldfacetoday. Enablingwidescaleadoption
partofmanyorganizationalworkflowsanddaytodaytasks
ofMLtechnologieshasthepotentialofachievingthenext
ofindividualsknowinglyorunknowingly. Bigtechcompa-
levelofbusinessprocessoptimizations,servicequalityim-
niesandacademicentitiesaretakingtheleadindeveloping
provementsanduserexperiences. Asanexample,today’s
cuttingedgeMLtechnologiesthatpushtheboundariesof
highleveldecisionmakersoflargeorganizationsalready
whatMLcanaccomplish.Cuttingedgecomputervisionand
usemachinelearningtechnologiesintensivelyintheirdeci-
languagemodelingtechnologiesprovideagoodexample
sionmakingprocesses. Buthowmuchofthesetechnologies
for this. Beyond the heightened attention, existing appli-
are practical and cost efficient to be implemented for the
cationsandlargescaleorganizationandacademicdriven
use of lower levels of the organizational hierarchy? As
developments,thereisalotofuntouchedpotentialtoML.
anothermoreconcreteexample,alargescaleorganization
Webelievethatthispotentialliesingroundlevelapplica-
mayimplementacuttingedgemachinelearningsolutionto
tion domains that are usually away from the mainstream
filterresumesintheirhiringprocess. Howpracticalisitto
attention. Thinkoforganizationsthatoperateinnon-techfo-
accesssuchtechnologiestoimplementasimilarsystemfor
cusedbusinessdomains. Thesecanincludeeducationaland
asmallerscaleorganizationthatmatchestheircustomized
researchinstitutes,healthcarefacilities,transportationunits
businessrequirements? Recurringthemehereisthat,when
andgovernmentorganizations. Theseorganizationshavea
makingmachinelearningtechnologiesmoreaccessible,1/
numberofworkflowsthatcanbeimprovedusingMLtech-
cuttingedgetechnologiesshouldbeaccessibletoawider
nologies. Dependingonthescale,theseorganizationsare
audiencethroughmeansthatareeasytograsp2/itshould
morelikelytohaveatraditionalinformationtechnologyand
bepossibletoimplementcustomizedmachinelearningsolu-
softwareengineeringstafftofulfillthetechrequirements.
tionsthatmatchbusinessrequirementsatalowercost. We
However, these organizations may not have the budgets,
believethatfurtherimprovingsoftwareengineeringprac-
resourcesandexpertisetofocusonproducingcustomML
ticesinmachinelearningsolutiondevelopment,especially
solutions for their workflows. Making cutting edge tech-
focusingondeeplearningapproaches, canprovideanef-
1SchoolofComputing,UniversityofGeorgia,Athens,Geor- fectivesolutiontotheaforementionedchallenge. Today’s
gia, United States. Correspondence to: Samiyuru Menik prevalent deep learning solutions are monolithic. These
<sm19812@uga.edu> solutionsaremostlylargeblackboxesthatproducetheright
3202
naJ
32
]GL.sc[
1v35790.1032:viXra
TowardsModularMachineLearningSolutionDevelopment:BenefitsandTrade-offs
answertoagivenproblemwhenfedwithalargeamountof affectedbydatasetavailabilityandcostofdataacquisition.
relevantdata. Majorinfluenceforthistrendiscomingfrom 2/Studythedifferencesofsolutiondevelopmenteffortbe-
endtoenddeeplearningtechnologies. Unliketraditional tweenmonolithicapproachandthemodularapproach. 3/
MLmethodsthatinvolvetimeconsumingandlaborinten- Tradeoffsbetweenmodelperformanceintermsofaccuracy
sivefeatureengineeringsteps,endtoendmachinelearning andefficiency. 4/Maintainabilityofthesolutionsdeveloped
attemptsusealargersinglemodelthataimstolearnallinter- withthetwoapproaches.
mediatestepsrequiredtosolvetheproblemathandwithout
needingmuchhandengineeringduringthelearningprocess.
2 MODULARITY IN MACHINE LEARNING
Thisisusuallyachievedbyemployinglargeparameterized
modelsthatcantakeadvantageoflargeamountsoftraining Modularity is a familiar approach when solving complex
data. ThiswayofdevelopingMLsolutionssignificantlycut problemsinmanydomains. Itsimplifiestheproblemsolv-
downthehumaneffortneededtoimplementMLsolutions ingprocessbybreakingdownacomplexproblemintomore
thatproduceimpressiveresultsforavarietyofMLproblems. manageablesubparts. Afterbreakingtheproblemintosub-
However,thisapproachofdevelopingMLsolutionshasa problems the subproblems can be solved independently.
setofkeydisadvantagesaswell. Theybecameespecially This process brings a number of advantages to the solu-
apparentwhendevelopingMLsolutionsoutsideofcutting tionengineeringprocess. ThegoalofmakingMLsolutions
edgetechfocusedorganizations. modularistoenabletheseadvantagestoMLsolutionengi-
neering.
Oneofthekeylimitationsofprevalentendtoendmachine
learning solutions is the lack of modularity. The lack of Combinatorial generalization is one of the main advan-
modularityresultsinanumberofothercriticalengineering tages of modular ML models. In general an ML model
challenges. Usuallyonemonolithicsolutionisonlyapplica- thatistrainedforaspecificproblemisonlyusefulforthat
bletoasingleproblem.Sincethesolutioncannotbebroken specificproblem. Thereforethemonolithicmodelsthatcan
downintosemanticallymeaningfulandmoregeneralsub- notbebrokendownintosubmodulesarenotusableoutside
modules, the effort that went into the system can not be oftheproblemthatitintendstosolve. Ontheotherhand
easilyreusedinothercontexts. Also,itisnotpossibletore- modularMLmodelsarecomposedofmorethanoneML
placepartsofthesystemasnewtechnologiesareintroduced modelthateachmodelsolvesasubproblemoftheoriginal
with improved performance characteristics. Further, the problem. ThisallowspartsofthemodularMLmodelsto
solutionshavetorelyonspecificdatasetsthatarefocused bereusedindifferentcontextsbeyondtheoriginalproblem.
onthespecificproblem. Traditionallyengineeringpractices Thisenablesmixingandmatchingmodulestocreatenew
encourage modular solutions in general to overcome the uniquemodelstosolvedifferentproblems. Thishelpsto
aforementionedchallengesanddevelopmoremaintainable minimize the chances of having to start from the scratch
and cost efficient solutions. We believe that encouraging whendevelopinguniquesolutions. Ontheotherhand,when
modularityindeeplearningsolutionscanbringanumberof usingendtoendlearningmethodsthatlearnamonolithic
advantagestodeeplearningsolutiondevelopmentthatother model,veryoften,eachnewproblemisauniqueproblem
moretraditionalengineeringdomainsalreadyexperience. thathastobesolvedstartingfromfundamentaltechnolo-
Fig. 1showsavisualrepresentationofmonolithicMLso- gies.
lutiondevelopmentincomparisontomodularMLsolution
ThesameistruefordatasetsthatareusedtotrainMLmod-
development.Asafirststeptowardsthisend,weexplorethe
els. Trainingmonolithicmodelsrequireadatasetspecific
costbenefittrade-offsofsolvingmachinelearningproblems
fortheintendedproblemthateachdatapointmapsaspecific
inamultistagemodularwayincomparisontosolvingthem
inputtypetoaspecifictargettypewithotherrequiredchar-
withamonolithicdeeplearningsolution. Inthiswork,we
acteristics. Sucha datasetisonly usefulto train amodel
performthisanalysisthroughthreeexampleproblems. For
similartotheoriginalproblem. Incomparison,modularML
eachexampleproblem, wewillimplementdeeplearning
modelsareacompositionofsubmoduleswhereeachmodel
solutionsintwoways. Oneofthesolutionswillbedevel-
istrainedwithadatasetthatmatchesthesubproblem.Atthe
opedtakingamonolithicapproach. Theothersolutionwill
sametime,theprocessofbreakingdownalargerproblem
bedevelopedtakingamodularapproach. Then,wewillbe
into subproblems usually results in subproblems that are
evaluating the two approaches quantitatively and qualita-
simplerandmoregeneral. Therefore,withinadevelopment
tivelytoanalyzethetrade-offsofeachapproach. Oneofthe
ecosystem,modularMLsolutionsincreasethechancesof
objectivesofourexperimentsistostudyifMLmodulescan
reusingdatasetstosolvedifferentproblems.
demonstratemodularitycharacteristicssimilartotraditional
non-MLsoftwaremodules. Duringtheevaluationwewill Domainexpertinterventiontosimplifythelearningpro-
focusonseveralaspectsofmonolithicandmodularsolution cessbyincorporatingdomainknowledgewhendeveloping
development. 1/Exploringhowthetwoapproacheswillbe solutions. Oneofthewaystodothisisbydecomposingthe
TowardsModularMachineLearningSolutionDevelopment:BenefitsandTrade-offs
ML ML ML ML ML
Module Module Module Module Module
≈
Monolithic ML ML ML ML ML ML
Model Module Module Module Module Module
ML ML ML ML ML
Module Module Module Module Module
Developing end to end Developing solutions using ML Modules
monolithic models from an eco-system
Figure1.VisualizationofmonolithicMLsolutiondevelopmentincomparisontomodularMLsolutiondevelopmentinanecho-system.In
themodularsolutiondevelopmentparadigm,MLmodulesarereusedtodevelopdifferentsolutionsbydifferentusersenablingenhanced
accessibilitytotechnologies.
probleminawaythattheresultingsubproblemsaremore accuracyforfasterperformanceatonesubproblemofthe
data efficient and simpler to learn. As a simple example, modularsolutiontomeetabusinessrequirementathand.
thinkaboutamulti-digitclassificationproblem. Inthiscase, Withmonolithicsolutions,makingperformancetrade-offs
adomainexpertmaybreakdowntheproblemtodetecteach usuallyrequirereplacingthewholemodelwithaonethat
digit individually using a simpler model and later aggre- hastherequiredcharacteristics.
gate the classification results to find the classification for
MaintainabilityofmodularMLmodelsishighercompared
theoriginalmultidigitinput. Thissimpledecomposition
tomonolithicMLmodels.Sincethesubmodulesofmodular
madethesubproblemsimpleraswellasmoredataefficient
ML models can be replaced with different models with
while increasing the amount of data points per classifica-
the same functionality, newer or improved technologies
tionclass. Intraditionalsoftwareengineeringmodularity
emerge, submodules can be upgraded without requiring
allowsengineeringteamstodivideworkacrossindividuals
major changes to the entire solution. In addition to that,
orspecializedteams. Thisminimizesthedevelopmentover-
since modular models are a composition of semantically
headbyassigningunitsofworkwithcross-cuttingconcerns
meaningfulmodels,theyaremorehumanunderstandable.
to one individual or one team. In this process a module
This opens up more opportunities to verify and monitor
boundary can define a unit of work that can be assigned
modular models. This simplifies the process of isolating
toanindividualorateam. Inadditiontothatthishelpsto
issuesandtroubleshooting.
reducedependenciesamongunitsofworkandparallelize
thedevelopmentprocess. Economiesofscaleeffectcanbeharnessedbetterwithmod-
ularMLmodelsfromadevelopmentecosystemperspective.
Performance tuning ability is higher with modular ML
Since modular ML models can take better advantage of
models. Modularityenablesbreakingdownaprobleminto
existing ML modules by reusing them to create different
subproblems and addressing them separately. Since sub-
higherlevelsolutions,themodulesthataremorecommonly
problemstendtobemoregeneralthantheoverallhighlevel
reused in many problems get a higher demand from the
problem, finding technologies and existing solutions for
developmentcommunity. Asaresultofthis,moredemand-
thesubproblemsislikelytobeeasier. Thisenablesmore
ing modules are likely to be further improved within the
optionsforsolutiondeveloperstomakeperformancetrade-
ecosystemduetotheeconomiesofscaleeffectandthese
offsatthesubproblemlevel. Further,modularitymakesit
improvementscanbeexploitedbythedownstreamsolutions.
easiertodoincrementalimprovementstosolutions. Indi-
Ontheotherhand,inendtoendML,thiseffectisrelatively
vidual modules in modular systems can be replaced with
lessprominentsincethelearnedsolutionsaremonolithic
differentmoduleswiththesamefunctionalitybutwithdif-
and specialized to individual problems making them less
ferent characteristics. As an example, one may trade off
usableinothercontexts.
TowardsModularMachineLearningSolutionDevelopment:BenefitsandTrade-offs
3 CASE STUDIES isusedinthemonolithicversion. Further,themonolithic
solution and the modular solution were distilled (Hinton
Asdiscussedbefore,wewillbeusingthreeexampleprob-
et al., 2015) into a smaller model to see the performance
lemsinordertoempiricallyhighlightthebenefitsandtrade-
characteristicsofthedistilledsolutionsincomparison. Ata
offsofmodularandmonolithicmachinelearningsolutions.
highlevel,thedistillationprocessisanalogoustoprogram
First example problem is a text based sentiment analysis.
compilationinsoftwaredevelopment. Insoftwaredevelop-
Theothertwoproblemsareasatelliteimageclassification
ment,theresultofcompilationisanobjectthatrunsmuch
problemandanearinfrared(NIR)fieldpredictionproblem.
fasterduringdeployment. However,thecompiledobjectis
lessmeaningfultohumanscomparedtotheprogramwritten
3.1 TextbasedSentimentAnalysis
usingahighlevelprogramminglanguage. Similarly,inthe
caseofamodularMLsolution,thedistillationresultsina
Textbasedsentimentanalysisisusefulinanumberofbusi-
faster solution but compromises the explainability that is
nesscontexts. Forinstance,businessesusesentimentanaly-
presentinthemodularsolution. Distillationisperformed
sistounderstandconsumersentimenttowardstheirbrand
withasmallerconvolutionalarchitectureasthestudentnet-
andtheproducts. Intoday’sinternetbasedglobalmarket
workinbothmonolithicandmodularcases. Inputtothis
setting,analyzingthesentimentofatextinagivenlanguage
modelisanintegersequencethatwascreatedusingaword
isimportantformanybusinessorganizations. Inthissection
dictionary that contains a unique index for each word in
weareusingthisproblemasaproxytostudythetrade-offs
thecorpus. Inputstothestudentmodelsaretruncatedtoa
ofmonolithicandmodularmachinelearningsolutions.
maximumlengthof500wordsandpaddedappropriatelyifa
Deeplearningtechnologieshavedemonstratedstateofthe sequenceisshort.Thearchitectureofthedistillationstudent
artperformanceinsentimentanalysistasks. Currentpreva- networkisshowninfig. 2. Thedistillationisdonebycon-
lent deeplearning basedmodels aremonolithic innature
(Tan&Le,2019;Devlinetal.,2019).Monolithicdeeplearn-
ingsolutionsforsentimentanalysistrainmachinelearning
modelsendtoendtopredictthesentimentofatextfrom
agivensourcelanguage. Moremodularsolutionforsenti-
mentanalysisistodevelopthesolutionusingtwomodules
thatsolvetheproblemintwointuitivestages. Thefirststage
istotranslatethesourcelanguagetexttoasuitabletarget
language. Thesecondstageistoanalyzethesentimentof
thetranslatedtext. Thisapproachenablestheopportunity
totrainasentimentanalysismodelforalanguagewitha
largerandmorerepresentativesentimentanalysisdataset
or to find a sentiment analysis model already trained for
aspecificlanguagethatdemonstrateshigherperformance
characteristics. In this section we will be implementing
Figure2.Architectureofthesentimentanalysisstudentmodel.
two sentiment analysis solutions one monolithic and one
modularasdescribedbeforeandcomparetheadvantages
sideringtheteachermodelasablackboxtokeeptheprocess
anddisadvantagesofthetwoapproaches.
more generally applicable. In the distillation process the
This experiment is performed with a Spanish sentiment studentistrainedtoimitatetheteacherbyminimizingthe
analysisproblem. Offtheshelfpretrainedlanguagemod- crossentropylossbetweentheteachermodel’soutputand
elsareusedtoimplementthemonolithicsolutionandthe thestudentmodel’soutput. Thisdistillationprocessdoes
two stage modular solution for this problem. The mono- notrequireanylabeleddatapoints. Itonlyneedsunlabeled
lithicversionoftheexperimentisconductedusinganexist- textfromtheinputlanguage.
ingBERTbasedpretrainedmultilingualsentimentanalysis
Fig. 3 shows a diagram of the monolithic and the mod-
model (Wolf et al., 2019). The model is already trained
ular solutions used in the experiment with their distilled
with150kEnglish,80kDutch,137kGerman,140kFrench,
counterparts.
72kItalianand50kSpanishsentimentanalysisdatapoints.
The model predicts the sentiment of the input text in a 1 Performancecharacteristicsofthesemodelsarecompared
to 5 scale where 1 is the least positive and 5 is the most usingthetestsetofthespanishportionoftheamazonmul-
positivesentiment. Thefirststageofthemodularversionis tilingual product review sentiment dataset (Keung et al.,
implementedwithanexistingpretrainedspanishtoenglish 2020). Thiscontains30000sentimentanalysisdatapoints.
translationmodel(Wolfetal.,2019;Tiedemann,2020).The Eachdatapointhasspanishproductreviewtextanda1to5
secondstageusesthesamesentimentanalysismodelthat starratingthatcorrespondtotheproductreviewtext.
TowardsModularMachineLearningSolutionDevelopment:BenefitsandTrade-offs
ingforpredictiontaskscomparedtotheoriginalEuroSAT
dataset. Since we want to represent the low labeled data
regimethatiscommonlyseeninrealworldindustrialappli-
cationdomains,weuseonlya20%oftheEuroSATlabeled
data to train, validate and test the models for the classifi-
cation and NIR band prediction tasks. In the case of the
Figure3.Monolithic(left)vsmodular(right)sentimentanalysis classificationtaskweusethecloudyRGBimageasthein-
modelsandthemodelsdistilledfromthem. putandthecorrespondingclassificationlabelasthetarget.
In the case of NIR prediction, the cloudy RGB image is
usedastheinputandthecorrespondingNIRbandisusedas
3.2 SatelliteImageClassificationandNIRprediction
thetarget. Therestofthedatathatisnotusedfortheclas-
Satelliteimagebasedremotesensingisusefulinanumber sificationandNIRpredictionisusedtotrain,validateand
ofrealworldapplicationssuchaslandsurvey,surveillance, testthecloudremovalmodulethatisusedinthemodular
trafficmonitoringetc. Inthissectionwearestudyingthe models. Itshouldbenotedthatthistrainingstepdoesnot
trade-offsbetweenmodularvsmonolithicMLsolutionsus- utilizethelabelsfromtheoriginalEuroSATdataset. This
ingasatelliteimageclassificationproblem. Inthisproblem cloudremovaldatasethastheRGBimageswiththecloud
weareclassifyingcloudysatelliteimagesbasedontheEu- overlayastheinputandthecorrespondingcloudfreeRGB
roSATdataset(Helberetal.,2019)into10classesofland imagesasthetarget. Suchanunlabeleddatasetisrelatively
use and land cover. We will be implementing one mono- easy to acquire in larger quantities in practice in the real
lithicmodelandtwomodularmodelsforthisclassification worldaswellsinceitdoesnotinvolvemanualdatalabeling.
problem. Performanceofthesethreemodelswillthenbe
Ourmonolithicclassificationmodelistrainedendtoend,
evaluatedbasedonaccuracyandlatency. Theaccuracyof
validated and tested using the classification split of the
the solutions are also compared with a large percentage
cloudysatelliteimages. Thenetworkarchitectureusedin
ofweightpruning. Further, weevaluatetheperformance
themonolithicmodelisshowninfig. 5. Thisarchitecture
ofmodelsdistilledfromthemonolithicandtwomodular
downsamplesthefeaturemapswhileincreasingthenumber
models. Next,wereuseamodulefromthemodularclassifi-
ofchannelsasthelayersprogressfrominputtotheoutput.
cationsolutionstopredictthenearinfrared(NIR)bandof
Inthefinallayerstheoutputoftheconvolutionallayersare
theEuroSATbasedcloudysatelliteimages. Thismodular
flattenedandsentthroughdenselayerstodotheclassifica-
NIRbandpredictionmodeliscomparedwithamonolithic
tion. Dropoutsareusedbeforethefeaturesarefedtothe
modelforthesametask. Unlikeinthepreviousexample,
denseunits.Themodularversionsperformtheclassification
inthisexamplewearetrainingcustomMLmodelsforthe
problem.
EuroSATdatasethasaredgreenblue(RGB)versionand
a13bandversion. Forourexperiments,weusetheRGB
andNIRbandsintheEuroSATdataset. EuroSATdataset
only contains clear satellite images without obstructions.
WealterthisdatasetbyaddingacloudoverlaytotheRGB
bandsusingtheapproachproposedbyKenjietal.(Enomoto
etal.,2017). Fig. 4showsasampleofthealteredEuroSAT
dataset. Added clouds makes the dataset more challeng-
Figure5.Architectureofthesatelliteimageclassificationmodel.
intwostages.Thefirststageperformscloudremoval.Cloud
removalisperformedusingtheencoderdecodernetwork
architecturethatisshowninfig. 6. Firstpartofthisarchi-
tecture, the encoder part, downsamples the feature maps
using6downsamplingblocks. Eachdownsamplingblock
hasaconvolutionallayer,batchnormalizationlayeranda
leakyreluactivation. Thefirstdownsamplingblockdoes
nothavebatchnormalization. Eachdownsamplingblock
from input to output reduces the feature map size while
Figure4.EuroSATdatapointsbefore(toprow)andafter(bottom increasingthenumberofchannels. Thesecondpartofthe
row)addingcloudlayers. architecture,thedecoderpart,upsamplesthefeaturemaps
TowardsModularMachineLearningSolutionDevelopment:BenefitsandTrade-offs
that are downsampled by the decoder using four upsam-
plingblocks. Eachupsamplingblockhasaconvolutional
transpose layer, dropout layer and a relu activation layer.
Thefirstupsamplingblockdoesnothaveadropoutlayer.
Eachupsamplinglayerfrominputtooutputincreasesthe
width and the height of feature maps while reducing the
numberofchannels. Asshowninthearchitecturefigure,
skipconnectionsareusedfromthedownsamplingblocks
to upsampling blocks with a mirror-like correspondence
to incorporate low level features to the latter upsampling
layers. Finallytheoutputoftheupsamplinglayersaresent
Figure7.Output(bottomrow)ofthecloudremovalmodulewhen
through another convolutional transpose layer with three
itisfedwithcloudydata(toprow).
channelsandasigmoidactivationfunction. Thisarchitec-
tureisadaptedfromtheU-net(Ronnebergeretal.,2015)
andpix2pix(Isolaetal.,2017)architectures. Thesecond sionisnotpretrainedthiswaybecauseitrepresentsthecase
wherecloudylabeledclassificationdataisnotavailable.
4 TRADE-OFFS OF MODULARITY
Inthissectionwewillusethesolutionswedevelopedfor
the three example problems to compare and contrast the
trade-offsbetweenmonolithicandmodularsolutions.
4.1 Accuracy
Thissectioncomparestheaccuracyofthemonolithicand
modularsolutionsusingsolutionsdevelopedforthesenti-
mentanalysisandsatelliteimageclassificationproblems.
Figure6.Encoder decoder architecture of the cloud removal
model.
4.1.1 SentimentAnalysis
stageperformsclassificationonthecloudremovedimages. Forthesentimentanalysiscase,thespanishproductreview
Inthefirstmodularsolutiontheclassifieristrainedusingthe testsetfromtheamazonmultilingualproductreviewsen-
outputofthecloudremovalmodule. Thismodularversion timentdatasetisusedtocomparetheaccuracyofthetwo
representsthecasewheretheclassificationdataavailablefor originalmonolithicandmodularsolutionsandthetwodis-
trainingiscloudy. Wewillbecallingthissolutionmodular tilled versions of the solution. In this experiment we use
(I).Inthesecondmodularversiontheclassifieristrained a one-off accuracy measure to quantify the performance.
usingcloudfreesatelliteimages. Thismodularversionrep- Hereweconsiderapredictionofthemodelascorrectifthe
resentsthecasewheretheclassificationdataavailablefor predictedstarratingisexactoroffbyonlyone. Ifnot,we
trainingarecloudfree. Wewillbecallingthissolutionmod- consider the prediction as incorrect. We believe that this
ular(II).Thiscorrespondstomakingabarebonepretrained measure is more realistic because there is no universally
satellite image classification module to be published in a agreeablestarratingforagivenreviewtext. Theresultsof
modelrepositorytobeusedbyothers. Allclassifiershave thisexperimentareshowninfig. 8.
thesamenetworkarchitectureshowninfig. 5andtheyare
trainedusingtheclassificationdatasplitthatwasdiscussed
before. Thecloudremovalmodule istrainedwith there-
mainingdatasplitthatwasdescribedbefore. Theoutputof
thecloudremovalmoduleisshowninfig. 7. Further,three
distilledmodelsaretrainedfromeachmonolithic,modular
(I)andmodular(II)solutions. Distillationisdonebyusing
thesameapproachthatwasusedinthesentimentanalysis
case. However,beforethedistillation,studentmodelsthat
correspondtothemonolithicsolutionandthemodular(I)
version are pre-trained with the available labeled cloudy Figure8.Sentimentanalysisaccuracyofmonolithicandmodular
classificationdata. Studentmodelofthemodular(II)ver- solutionsincludingtheirdistilledversions.
TowardsModularMachineLearningSolutionDevelopment:BenefitsandTrade-offs
The results in fig. 8 shows that the original monolithic 4.2.1 SentimentAnalysis
solutionhaslessthan1percentagepointhigheraccuracy
Thespanishproductreviewtestsetoftheamazonreview
overtheoriginalmodularsolution. Thesolutiondistilled
dataset is used to compare the latency of the two origi-
fromthemonolithicsolutionislessthan2percentagepoints
nalmonolithicandmodularsolutionsandthetwodistilled
lowerinaccuracycomparedtotheoriginalmonolithicso-
versions of the solution. In this experiment we measure
lution. The model distilled from the modular solution is
thetimeittakesforeachmodeltoprocessthe30000test
lessthan1percentagepointlowercomparedtotheoriginal
datapoints. Theresultsareshowninfig. 10. Theexperi-
modular solution. The model distilled from the modular
mentsareconductedonamachinewithIntel(R)Xeon(R)
versionhas0.3%higheraccuracycomparedtothemodel
CPU@2.20GHz,13298580kBofRAMandaTeslaP100
distilledfromthemonolithicsolution. Theresultsshowthat
16280MiBGPU.
modularsolutionscanbecomparableintermsofaccuracy
tomonolithicsolutions.
4.1.2 SatelliteImageClassification
ThetestsplitoftheEuroSATdatasetisusedtomeasurethe
accuracyofthemodelsandtheresultsareshowninfig. 9.
Figure10.Sentimentanalysislatencyofmonolithicandmodular
solutionsincludingtheirdistilledversions.
Theresultsinfig. 10showsthattheoriginalmodularso-
lutionismuchslowercomparedtotheoriginalmonolithic
solution. Thisperformancedropismainlyduetothetrans-
lationmodelusedinthefirststageofthemodularsolution.
However,thesolutionsdistilledfromeachoftheoriginal
Figure9.Cloudysatelliteimageclassificationaccuracyofmono-
solutionsaremuchfasteraswecanexpect. Theseresults
lithicandmodularsolutionsincludingtheirdistilledversions.
suggest that developing modular solutions and distilling
them into smaller models can result in efficient solutions
withminoraccuracytrade-offs. Theadditionalbenefitof
Theresultsinfig. 9showsthatthemodular(I)solutionthat
thisapproachisthatthemodularsolutiondevelopmentpro-
wastrainedwithcloudylabeleddatawiththeaddedcloud
videsanumberofengineeringadvantagesasdiscussedin
removalmoduleperformsthebestoutofthethreesolutions.
theintroductionsection.
Ithasbeenabletoachieve11.34%accuracyimprovement
overthemonolithicsolutionwhencomparingtheoriginal
4.2.2 SatelliteImageClassification
non-distilled solutions. When comparing the distilled so-
lutions,themodular(I)solutionwasabletoachieve10% Tocomparethelatencyofthemonolithicandmodularso-
accuracyimprovementoverthecorrespondingmonolithic lutions,thetimethateachsolutiontakestoprocess56700
solution. Thisaccuracyimprovementcanbeattributedto datapointsismeasured. Thesameisdonewiththedistilled
thecloudremovalmoduleinthemodularversionthatwas solutions. Thelatencymeasurementsforeachsolutionis
capableofutilizinglowcostunlabeleddata. Modular(II) takenonamachinewithIntel(R)Xeon(R)CPU@2.20GHz,
solutioninwhichtheclassificationmoduleistrainedwith 13298580kBofRAMandaTeslaP10016280MiBGPU.
cleandatahasthelowestaccuracy.However,thissolutionat- Theresultsareshowninfig. 11.
tainsitsaccuracylevelwithoutusinganylabeleddatapoints
Thelatencyresultsinfig. 11showsthatthenon-distilled
fromitstargetproblem,cloudysatelliteimageclassification.
monolithicsolutiontook63.83%lesstimecomparedtothe
fastestmodularsolutiontoprocessthedataload. However,
4.2 Latency
distilled versions have been able to address this issue by
This section compares the latency of the monolithic and achievinglowlatenciescomparabletothemonolithicver-
modularsolutionsusingsolutionsdevelopedforthesenti- sion,stillhavingbetteraccuracythanthemonolithicversion
mentanalysisandsatelliteimageclassificationproblems. inthecaseofmodular(II)solution.
TowardsModularMachineLearningSolutionDevelopment:BenefitsandTrade-offs
13298580kBofRAMandaTeslaP10016280MiBGPU.
Thelatencyresultsareshowninfig. 13.
Figure11.Cloudysatelliteimageclassificationlatency(lowerthe Figure12.Meansquarederror(lowerthebetter)ofmonolithicand
better)ofmonolithicandmodularsolutionswiththerespective modularNIRpredictionsolutionsincludingtheirdistilledversions.
distilledversions.
4.3 Reusability
InthissectionwearetestingwhetheranMLmodulethat
wasusedinoneproblemcanbeusedinanotherproblem.
Thisisdifferentfromgeneraltransferlearningwherebase
layers of a larger model are transferred to a similar task.
HerewearereusingasemanticallymeaningfulMLmodule
inadifferentproblem.
Aftertheclassificationcomparison,thecloudremovalmod- Figure13.Latency(lowerthebetter)ofmonolithicandmodular
uleisreusedinadifferenttasktoevaluatethereusability NIRpredictionsolutionswiththerespectivedistilledversions.
of the module. In this task the RGB bands of the cloudy
EuroSATdatasetareusedtopredicttheNIRbandforthe Theresultsinfig. 12and13resemblesthepatternweob-
image. Twomonolithicandmodularsolutionsareimple- servedintheclassificationcasewithmonolithicandmodular
mentedforthistask. Themonolithicsolutionistrainedend (I)solutions. Themodularsolutionhashigherperformance
toendonthecloudyRGBimages. Monolithicmodeluses intermsofaccuracy/error. However,ithashigherlatency
anencoderdecodernetworkarchitecturesimilartotheone compared to the monolithic version as one would expect.
usedinthecloudremovalmodulebutwithasingleoutput The solution distilled from the modular model has lower
channel. ThemodularversionpredictstheNIRbandintwo errorcomparedtothebothoriginalmonolithicsolutionand
stages. Thefirststageremovesthecloudsfromtrainingdata the solution distilled from the monolithic solution while
byreusingthecloudremovalmodulethatwastraineddur- havingcomparablelatencyvalues.
ingthepreviousclassificationtask. Thesecondstageuses
theoutputofthecloudremovalmoduletopredicttheNIR 4.4 Maintainability
band.Modelusedinthesecondstageusesthesamenetwork
In this section, we empirically highlight the trade-offs of
architectureusedinthemonolithicversion. Twodistilled
the monolithic and the modular solutions with respect to
modelsarecreatedinthiscaseaswellusingthemonolithic
maintainability as the requirements change. We will be
and modular versions of the solution. Both modular and
considering a case where the requirements of the model
monolithicversionsofthesolutionsaretrainedusingthe
changetohandlenoisyimagesinthecloudysatelliteimage
NIRpredictiontrainingdatasplitthatwasexplainedbefore.
classificationproblem.
Thedistillationwasperformedfollowingthesameprocess
usedfordistillingthemonolithicandmodular(I)classifica- Inthisexperiment,wemodifytheclassificationdatasetthat
tionmodels. However,inthiscasethedistilledmodelsare weusedbeforebyaddinggaussiannoisetotheRGBchan-
pretrainedwiththeNIRpredictiondatasetbeforethedistil- nels to represent noisy satellite images with cloud cover.
lation. Afterthetraininganddistillationsteps,testsplitof Fig. 14showstheimagesafteraddingnoise. Thenthenoisy
theNIRpredictiondatasetisusedtomeasuretheaccuracy dataisfedtothepreviouslytrainedmonolithicclassifica-
ofeachsolution. Theresultsofthisexperimentareshown tionmodelandthemodular(I)classificationmodelandthe
in fig. 12. The latency of each solution is measured for classificationaccuracyismeasuredusingaheldouttestset.
processing56700datapointsusingeachversionofthesolu- Additionally,wecreateanimprovedmodularsolutionbyup-
tiononamachinewithIntel(R)Xeon(R)CPU@2.20GHz, datingthecloudremovalmoduleofthemodular(I)solution.
TowardsModularMachineLearningSolutionDevelopment:BenefitsandTrade-offs
subproblems,findingdatasetsforthemcansometimesbe
challenging. Thisissuecanbemitigatedbyperformingthe
problemdecompositiontomatchdatathatisavailable. In
thiscase,thetrade-offsofdifferentproblemdecompositions
should be further studied. Further, synthetic data can be
helpfultofillsomeofthedatagaps.
Figure14.Cloudy EuroSAT data points after adding gaussian
noise. In some cases, module compositions may not perform in
synergy as expected. Sometimes even if the individual
modelsperformwell,whentheyarecomposedtogetherto
The cloud removal module is updated by training it with
solvealargerproblem,theperformancecanbeunexpectedly
unlabeled noisy cloud images. This improvement makes
low.Thiscanhappenduetovariousincompatibilitiesamong
thecloudremovalmodulerobusttonoise. Intheimproved
submodules. StudiesonMLadversarialattacksmaybeable
modularsolution,theclassificationmoduleremainstobe
toshedsomelightinthisregard. Furtherunderstandingthe
thesamemoduleusedintheoriginalmodular(I)solution.
reasonsforsuchfailuresandmethodsisimportanttomake
Theaccuracyoftheimprovedmodularsolutionismeasured
modularMLapplicableinawiderarrayofproblems.
withthesamenoisyheldouttestdata. Accuracyofeach
modelisshowninfig. 15. Asshownintheresultsinfig. LackoffeaturerichrepositoriestopublishandsearchML
models and datasets is another challenge to using modu-
lar ML in practice. Existing systems for publishing and
searching ML models do not provide sufficient metadata
andsemanticallymeaningfulsearchcapabilitiestolookfor
modelsanddatasetsthatcansatisfyspecificrequirements.
MoreadvancedMLmodelsanddatarepositorieswithse-
manticsearchcapabilitiesandmetadatasupport(Menik&
Figure15.Classificationaccuracyfornoisysatelliteimageswith
Ramaswamy,2021)shouldbedevelopedtomakemodular
cloudcover.
MLpractical.
15,bothmonolithicandmodular(I)solutionssignificantly Addressingthesechallengesthroughfutureworkiskeyto
dropinaccuracywhenfedwithnoisyimages. However,the reapingthebenefitsofmodularmachinelearning.
modular (I) solution has the capacity to replace modules
withimprovedones. Byreplacingthecloudremovalmod-
6 RELATED WORK
ulewithanupdatednoiserobustcloudremovalmodule,the
improvedmodularsolutioncouldachievearound2times Thereareseveralexistinglinesofworkthatarerelatedto
higheraccuracycomparedtotheoriginalmodular(I)solu- modularityinmachinelearning. Therehavebeenattempts
tion. Itshouldbenotedthatthisaccuracygainwasachieved tobreakpre-trainedneuralnetworksintoasetofmodules
without making any changes to the classification module basedonthelearnedweights. Ultimategoalofthisistofind
and by only using noisy unlabeled data. The monolithic semanticmoduleswithinalearnedneuralnetwork. Csordas
solutioncannotbeimprovedthiswayusingunlabeleddata. et. al.(Csorda´setal.,2021)haveattemptedtodothisby
Improvingthemonolithicsolutionneedslabeleddatathat learning weight masks to identify subnetworks for target
areusuallylaborintensivetoproduce. tasks. Theywereabletofindsomespecializedsubnetworks
intrainedneuralnetworkswithsomeotherinterestingin-
5 OPEN CHALLENGES sights. However,tothebestofourknowledge,theseworks
havesofarnotbeenabletofindstrongsemanticallymean-
Modularity in ML has a number of advantages. We dis- ingfulmoduleswithinlearnedmonolithicneuralnetworks
cussedthemindetailqualitativelyandquantitatively. How- that can be reused in other contexts. Ensemble learning
ever,modularMLhasseveralmainopenchallengeswhenit methods(Sagi&Rokach,2018)hasasenseofmodularity.
comestodevelopingeffectivemodularMLmodels. Anensemblemodelisnotasinglemonolithicunit. Ensem-
blemethodslikebagging,stackingandboostingcombine
Breakingdowntheoriginalproblemintoasetofsubprob-
several machine learning models to create a more robust
lems is not always feasible. Some problems do not have
singlesolution. Inthesemethodseachmodelintheensem-
semanticallymeaningfulsubproblems. Inthesecaseswe
blewillbespecializedinoneaspectofthewholeproblem.
havetoconsidertheproblemasanatomicunitandutilize
Thesespecializedpartstogethercreateabetterfinalsolu-
endtoendlearningmethods. Afterthatthetrainedmodel
tiontotheoverallmachinelearningproblem. However,the
hasthepotentialtobeusedinafutureModularMLmodel.
modules in ensemble models are usually not reusable in
Insituationswherenewmodelshavetobetrainedtosolve
TowardsModularMachineLearningSolutionDevelopment:BenefitsandTrade-offs
othersystemssincetheyarespecializedtoagivenspecific box knowledge distillation to overcome the performance
ensemble that solves one problem. Another line of work challengesthatmodularsolutionscanhaveandshowedthe
thatattemptstoincorporatemodularityindeeplearningis impactofaccuracyandlatencyincomparisontooriginal
routing networks (Cases et al., 2019). Modular question monolithicandmodularsolutions. Theexperimentalresults
answering approach proposed by Jacobs (Andreas et al., inthisworksuggestthatitisveryinterestingtofurtherinves-
2016) is another example for similar work. This line of tigatethepotentialofmultistagemodularmachinelearning
workmainlytriestolearnasetofmodulesandacontroller solutiondevelopmentincontrasttowidespreadmonolithic
tocomposethesemodulestosolveproblems. Thislearn- endtoendmachinelearningsolutiondevelopment.
ingprocessisusuallydoneinanendtoendfashion. Even
thoughtheseapproacheshaveshowngoodperformancein
REFERENCES
the problems that the model was trained to, the modules
havenotshowntobemuchusefuloutsideoftheproblem Andreas,J.,Rohrbach,M.,Darrell,T.,andKlein,D. Learn-
in concern. Therefore, routing networks, in their current ingtocomposeneuralnetworksforquestionanswering.
state, arenotyetcapableofaddressingtheproblemsthat arXivpreprintarXiv:1601.01705,2016.
wediscussedearlier. Transferlearningtechniques(Zhuang
Cases,I.,Rosenbaum,C.,Riemer,M.,Geiger,A.,Klinger,
etal.,2020)allowmachinelearningmodeldevelopersto
T.,Tamkin,A.,Li,O.,Agarwal,S.,Greene,J.D.,Juraf-
traincustomizedsolutionswithlessamountoftrainingdata
sky,D.,etal. Recursiveroutingnetworks: Learningto
byfinetuninganalreadyexistingmachinelearningmodel
composemodulesforlanguageunderstanding. InPro-
that is usually pre-trained with a large dataset. Transfer
ceedingsofthe2019ConferenceoftheNorthAmerican
learning has shown to be very effective when an already
Chapter of the Association for Computational Linguis-
existingmodelissimilartothetargettask. Therearefew
tics: Human Language Technologies, Volume 1 (Long
downsidestotransferlearningapproaches. First,themodel
andShortPapers),pp.3631–3648,2019.
developersusuallyhavetobeknowledgeableaboutthepre-
existingmodelinternalsinordertobeabletomodifyitto
Csorda´s,R.,vanSteenkiste,S.,andSchmidhuber,J. Are
matchthetargetmachinelearningproblem. Themodified
neural nets modular? inspectingfunctional modularity
modelhastoberetrainedwithnewtrainingdatathatbetter
throughdifferentiableweightmasks. In9thInternational
represent thenewmachine learningproblem. Thisis dif-
Conference on Learning Representations, ICLR 2021,
ferentfromtraditionalsoftwareengineeringmodulesthat
VirtualEvent,Austria,May3-7,2021.OpenReview.net,
attempttoexposeacleaninterfacetotheusersbyhiding 2021. URLhttps://openreview.net/forum?
themoduleinternals. Whenitcomestomakingmachine id=7uVcpu-gMD.
learningtechnologiesmoreaccessibletoawideraudience,
transferlearningtechniquesshouldbefurtherimprovedto Devlin,J.,Chang,M.,Lee,K.,andToutanova,K. BERT:
enableamorereadytousemodularitythatminimizesthe pre-training of deep bidirectional transformers for lan-
chancesofdevelopershavingtoworkwiththeinternalsof guage understanding. In Burstein, J., Doran, C., and
complexnetworkarchitecture. Solorio,T.(eds.),Proceedingsofthe2019Conferenceof
theNorthAmericanChapteroftheAssociationforCom-
putationalLinguistics: HumanLanguageTechnologies,
7 SUMMARY
NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,
Inthiswork,whileacknowledgingtheimmensepotential 2019,Volume1(LongandShortPapers),pp.4171–4186.
andpositiveimpactofcurrentdeeplearningtechnologies, Association for Computational Linguistics, 2019. doi:
wediscussedthechallengesandlimitationsofwidespread 10.18653/v1/n19-1423. URL https://doi.org/
monolithicdeeplearningtechnologieswithrespecttosys- 10.18653/v1/n19-1423.
tems engineering concerns especially when it comes to
Enomoto, K., Sakurada, K., Wang, W., Fukui, H., Mat-
wider adoption of these technologies in diverse organiza-
suoka, M., Nakamura, R., and Kawaguchi, N. Filmy
tions. Wepointedoutsemanticmodularityinmachinelearn-
cloud removal on satellite imagery with multispectral
ingasaninterestingavenuetoaddressanumberofproblems
conditional generative adversarial nets. In 2017 IEEE
inthisregard. Next,asafirststep,weusedthreeexample
ConferenceonComputerVisionandPatternRecognition
problemstoexplorethebenefitsandtrade-offsbetweende-
Workshops,CVPRWorkshops2017,Honolulu,HI,USA,
veloping machine learning solutions in a monolithic way
July21-26,2017,pp.1533–1541.IEEEComputerSoci-
and developing them in a multi-stage modular way. The
ety,2017.doi:10.1109/CVPRW.2017.197.URLhttps:
experimentsshowedhowmodularsolutionscanreuseex-
//doi.org/10.1109/CVPRW.2017.197.
istingpretrainedmodelsandexploitmoredatatoachieve
higheraccuracyandovercomedatalimitationsinwaysthat
Helber,P.,Bischke,B.,Dengel,A.,andBorth,D. Eurosat:
monolithicsolutionsdonotpermit. Further,weusedblack-
A novel dataset and deep learning benchmark for land
TowardsModularMachineLearningSolutionDevelopment:BenefitsandTrade-offs
useandlandcoverclassification. IEEEJ.Sel.Top.Appl. Research,pp.6105–6114.PMLR,2019. URLhttp://
EarthObs.Remote.Sens.,12(7):2217–2226,2019. doi: proceedings.mlr.press/v97/tan19a.html.
10.1109/JSTARS.2019.2918242. URLhttps://doi.
Tiedemann,J. TheTatoebaTranslationChallenge–Realis-
org/10.1109/JSTARS.2019.2918242.
ticdatasetsforlowresourceandmultilingualMT.InPro-
Hinton,G.E.,Vinyals,O.,andDean,J.Distillingtheknowl- ceedingsoftheFifthConferenceonMachineTranslation,
edgeinaneuralnetwork. CoRR,abs/1503.02531,2015. pp. 1174–1182, Online, November 2020. Association
URLhttp://arxiv.org/abs/1503.02531. for Computational Linguistics. URL https://www.
aclweb.org/anthology/2020.wmt-1.139.
Isola, P., Zhu, J., Zhou, T., and Efros, A. A. Image-
to-image translation with conditional adversarial net- Wolf,T.,Debut,L.,Sanh,V.,Chaumond,J.,Delangue,C.,
works. In 2017 IEEE Conference on Computer Vision Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,
and Pattern Recognition, CVPR 2017, Honolulu, HI, andBrew,J. Huggingface’stransformers: State-of-the-
USA,July21-26,2017,pp.5967–5976.IEEEComputer artnaturallanguageprocessing. ArXiv,abs/1910.03771,
Society, 2017. doi: 10.1109/CVPR.2017.632. URL 2019.
https://doi.org/10.1109/CVPR.2017.632.
Zhuang,F.,Qi,Z.,Duan,K.,Xi,D.,Zhu,Y.,Zhu,H.,Xiong,
H., and He, Q. A comprehensive survey on transfer
Keung, P., Lu, Y., Szarvas, G., and Smith, N. A. The
learning. ProceedingsoftheIEEE,109(1):43–76,2020.
multilingual amazon reviews corpus. In Webber, B.,
Cohn,T.,He,Y.,andLiu,Y.(eds.),Proceedingsofthe
2020ConferenceonEmpiricalMethodsinNaturalLan-
guageProcessing,EMNLP2020,Online,November16-
20,2020,pp.4563–4568.AssociationforComputational
Linguistics,2020. doi: 10.18653/v1/2020.emnlp-main.
369. URL https://doi.org/10.18653/v1/
2020.emnlp-main.369.
Menik, S. and Ramaswamy, L. Towards a robust knowl-
edgegraph-enabledmachinelearningservicedescription
framework.In15thIEEEInternationalConferenceonSe-
manticComputing,ICSC2021,LagunaHills,CA,USA,
January 27-29, 2021, pp. 104–107. IEEE, 2021. doi:
10.1109/ICSC50631.2021.00026. URLhttps://doi.
org/10.1109/ICSC50631.2021.00026.
Ronneberger, O., Fischer, P., and Brox, T. U-net: Con-
volutionalnetworksforbiomedicalimagesegmentation.
InNavab,N.,Hornegger,J.,III,W.M.W.,andFrangi,
A.F.(eds.), MedicalImageComputingandComputer-
Assisted Intervention - MICCAI 2015 - 18th Interna-
tional Conference Munich, Germany, October 5 - 9,
2015, Proceedings, Part III, volume 9351 of Lecture
NotesinComputerScience,pp.234–241.Springer,2015.
doi: 10.1007/978-3-319-24574-4\ 28. URLhttps://
doi.org/10.1007/978-3-319-24574-4_28.
Sagi,O.andRokach,L.Ensemblelearning:Asurvey.Wiley
InterdisciplinaryReviews: DataMiningandKnowledge
Discovery,8(4):e1249,2018.
Tan, M. and Le, Q. V. Efficientnet: Rethinking model
scaling for convolutional neural networks. In Chaud-
huri, K. and Salakhutdinov, R. (eds.), Proceedings of
the36thInternationalConferenceonMachineLearning,
ICML 2019, 9-15 June 2019, Long Beach, California,
USA, volume 97 of Proceedings of Machine Learning
